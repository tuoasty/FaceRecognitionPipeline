{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddce237",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "def find_project_root(start: Path, target_folder=\"RIG\"):\n",
    "  for parent in [start] + list(start.parents):\n",
    "    if parent.name == target_folder:\n",
    "      return parent\n",
    "  raise RuntimeError(f\"Could not find project root '{target_folder}'\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(NOTEBOOK_DIR)\n",
    "OUTPUT_ROOT = PROJECT_ROOT / \"output\" / \"v2\"\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Output Root: {OUTPUT_ROOT}\")\n",
    "\n",
    "output_root = Path(OUTPUT_ROOT)\n",
    "embeddings_root = output_root / 'embeddings'\n",
    "results_dir = output_root / 'evaluation_results'\n",
    "plots_dir = results_dir / 'plots'\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "models = [\n",
    "  'adaface_ir_50',\n",
    "  'adaface_ir_101',\n",
    "  'arcface_ir_50',\n",
    "  'arcface_ir_101'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model_name: str) -> Dict:\n",
    "  model_dir = embeddings_root / model_name\n",
    "  \n",
    "  if not model_dir.exists():\n",
    "    raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n",
    "  \n",
    "  embeddings = {}\n",
    "\n",
    "  embedding_files = {\n",
    "    'gallery_oneshot_base': 'gallery_one-shot_base.pkl',\n",
    "    'gallery_oneshot_augmented': 'gallery_one-shot_augmented.pkl',\n",
    "    'gallery_fewshot_base': 'gallery_few-shot_base.pkl',\n",
    "    'gallery_fewshot_augmented': 'gallery_few-shot_augmented.pkl',\n",
    "    'probe_positive_unsegmented': 'probe_positive_unsegmented.pkl',\n",
    "    'probe_positive_segmented': 'probe_positive_segmented.pkl',\n",
    "    'probe_negative': 'probe_negative.pkl'\n",
    "  }\n",
    "  \n",
    "  for key, filename in embedding_files.items():\n",
    "    file_path = model_dir / filename\n",
    "    if file_path.exists():\n",
    "      with open(file_path, 'rb') as f:\n",
    "        embeddings[key] = pickle.load(f)\n",
    "    else:\n",
    "        embeddings[key] = None\n",
    "  \n",
    "  return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2dc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "    norm1 = np.linalg.norm(emb1)\n",
    "    norm2 = np.linalg.norm(emb2)\n",
    "    if abs(norm1 - 1.0) < 0.01 and abs(norm2 - 1.0) < 0.01:\n",
    "        return np.dot(emb1, emb2)\n",
    "    return np.dot(emb1, emb2) / (norm1 * norm2)\n",
    "\n",
    "def compute_all_similarities(probe_emb: np.ndarray, \n",
    "                            gallery_embeddings: Dict[str, Dict]) -> List[Tuple[str, float]]:\n",
    "    similarities = []\n",
    "    for name, data in gallery_embeddings.items():\n",
    "        gallery_embs = data['embeddings']\n",
    "        for gallery_emb in gallery_embs:\n",
    "            sim = cosine_similarity(probe_emb, gallery_emb)\n",
    "            similarities.append((name, sim))\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_max(similarities: List[float]) -> float:\n",
    "    return max(similarities) if similarities else -1\n",
    "\n",
    "def aggregate_mean(similarities: List[float]) -> float:\n",
    "    return np.mean(similarities) if similarities else -1\n",
    "\n",
    "def aggregate_topk(similarities: List[float], k: int = 3) -> float:\n",
    "    if not similarities:\n",
    "        return -1\n",
    "    sorted_sims = sorted(similarities, reverse=True)\n",
    "    return np.mean(sorted_sims[:min(k, len(sorted_sims))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e39f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_probe(probe_embedding: np.ndarray, \n",
    "                   gallery_embeddings: Dict[str, Dict],\n",
    "                   threshold: float,\n",
    "                   aggregation: str = 'mean',\n",
    "                   k: int = 3) -> Tuple[Optional[str], float, Dict[str, float]]:\n",
    "    identity_scores = {}\n",
    "    \n",
    "    for name, data in gallery_embeddings.items():\n",
    "        gallery_embs = data['embeddings']\n",
    "        similarities = [cosine_similarity(probe_embedding, g_emb) for g_emb in gallery_embs]\n",
    "        \n",
    "        if aggregation == 'max':\n",
    "            score = aggregate_max(similarities)\n",
    "        elif aggregation == 'mean':\n",
    "            score = aggregate_mean(similarities)\n",
    "        elif aggregation == 'topk':\n",
    "            score = aggregate_topk(similarities, k)\n",
    "        else:\n",
    "            score = aggregate_max(similarities)\n",
    "        \n",
    "        identity_scores[name] = score\n",
    "    \n",
    "    if not identity_scores:\n",
    "        return None, -1, {}\n",
    "    \n",
    "    sorted_identities = sorted(identity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    best_name, best_score = sorted_identities[0]\n",
    "    \n",
    "    if best_score < threshold:\n",
    "        return None, best_score, identity_scores\n",
    "    \n",
    "    return best_name, best_score, identity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rank_metrics(identity_scores: Dict[str, float], \n",
    "                        true_identity: str,\n",
    "                        ranks: List[int] = [1, 5, 10]) -> Dict[str, bool]:\n",
    "    sorted_identities = sorted(identity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    rank_results = {}\n",
    "    for k in ranks:\n",
    "        top_k = [name for name, _ in sorted_identities[:k]]\n",
    "        rank_results[f'rank{k}'] = true_identity in top_k\n",
    "\n",
    "    try:\n",
    "        true_rank = [name for name, _ in sorted_identities].index(true_identity) + 1\n",
    "        rank_results['reciprocal_rank'] = 1.0 / true_rank\n",
    "    except ValueError:\n",
    "        rank_results['reciprocal_rank'] = 0.0\n",
    "    \n",
    "    return rank_results\n",
    "\n",
    "def compute_dprime(genuine_scores: List[float], impostor_scores: List[float]) -> float:\n",
    "    if not genuine_scores or not impostor_scores:\n",
    "        return 0.0\n",
    "    \n",
    "    mean_genuine = np.mean(genuine_scores)\n",
    "    mean_impostor = np.mean(impostor_scores)\n",
    "    std_genuine = np.std(genuine_scores)\n",
    "    std_impostor = np.std(impostor_scores)\n",
    "    \n",
    "    pooled_std = np.sqrt((std_genuine**2 + std_impostor**2) / 2)\n",
    "    \n",
    "    if pooled_std == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (mean_genuine - mean_impostor) / pooled_std\n",
    "\n",
    "def bootstrap_confidence_interval(data: List[float], \n",
    "                                 n_bootstrap: int = 1000, \n",
    "                                 confidence: float = 0.95) -> Tuple[float, float]:\n",
    "    if not data:\n",
    "        return (0.0, 0.0)\n",
    "    \n",
    "    bootstrap_means = []\n",
    "    n = len(data)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = np.random.choice(data, size=n, replace=True)\n",
    "        bootstrap_means.append(np.mean(sample))\n",
    "    \n",
    "    alpha = 1 - confidence\n",
    "    lower = np.percentile(bootstrap_means, alpha/2 * 100)\n",
    "    upper = np.percentile(bootstrap_means, (1 - alpha/2) * 100)\n",
    "    \n",
    "    return (lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fafb611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_verification_comprehensive(gallery_embeddings: Dict[str, Dict],\n",
    "                                       probe_positive: Dict[str, Dict],\n",
    "                                       probe_negative: Dict[str, Dict],\n",
    "                                       thresholds: List[float],\n",
    "                                       aggregation: str = 'mean',\n",
    "                                       k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Proper verification evaluation using:\n",
    "    - probe_positive: For genuine scores (probe vs its own gallery)\n",
    "    - probe_negative: For impostor scores (unknown people vs gallery)\n",
    "    \n",
    "    Returns AUC, EER, TAR@FAR, and score distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    probe_pos_data = probe_positive.get(\"all\", probe_positive)\n",
    "    probe_neg_data = probe_negative.get(\"all\", probe_negative) if probe_negative else {}\n",
    "    \n",
    "    genuine_scores = []\n",
    "    impostor_scores = []\n",
    "    \n",
    "    print(f\"Computing verification scores...\")\n",
    "    \n",
    "    # ============================================\n",
    "    # GENUINE SCORES: Positive probes vs their own gallery\n",
    "    # ============================================\n",
    "    print(\"  Computing genuine scores (positive probes vs own gallery)...\")\n",
    "    for true_name, data in tqdm(probe_pos_data.items(), desc=f\"Genuine ({aggregation})\"):\n",
    "        probe_embs = data['embeddings']\n",
    "        \n",
    "        if true_name not in gallery_embeddings:\n",
    "            print(f\"    Warning: {true_name} not in gallery, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        gallery_data = gallery_embeddings[true_name]\n",
    "        gallery_embs = gallery_data['embeddings']\n",
    "        \n",
    "        for probe_emb in probe_embs:\n",
    "            similarities = [cosine_similarity(probe_emb, g_emb) for g_emb in gallery_embs]\n",
    "            \n",
    "            if aggregation == 'max':\n",
    "                score = aggregate_max(similarities)\n",
    "            elif aggregation == 'mean':\n",
    "                score = aggregate_mean(similarities)\n",
    "            elif aggregation == 'topk':\n",
    "                score = aggregate_topk(similarities, k)\n",
    "            else:\n",
    "                score = aggregate_max(similarities)\n",
    "            \n",
    "            genuine_scores.append(score)\n",
    "    \n",
    "    # ============================================\n",
    "    # IMPOSTOR SCORES: Negative probes vs ALL gallery identities\n",
    "    # ============================================\n",
    "    if probe_neg_data:\n",
    "        print(\"  Computing impostor scores (negative probes vs all gallery)...\")\n",
    "        for impostor_name, data in tqdm(probe_neg_data.items(), desc=f\"Impostor ({aggregation})\"):\n",
    "            probe_embs = data['embeddings']\n",
    "            \n",
    "            for probe_emb in probe_embs:\n",
    "                # For each impostor probe, get BEST match against ALL gallery identities\n",
    "                # This is the worst-case scenario: impostor gets their best possible score\n",
    "                best_impostor_score = -1\n",
    "                \n",
    "                for gallery_name, gallery_data in gallery_embeddings.items():\n",
    "                    gallery_embs = gallery_data['embeddings']\n",
    "                    \n",
    "                    similarities = [cosine_similarity(probe_emb, g_emb) for g_emb in gallery_embs]\n",
    "                    \n",
    "                    if aggregation == 'max':\n",
    "                        score = aggregate_max(similarities)\n",
    "                    elif aggregation == 'mean':\n",
    "                        score = aggregate_mean(similarities)\n",
    "                    elif aggregation == 'topk':\n",
    "                        score = aggregate_topk(similarities, k)\n",
    "                    else:\n",
    "                        score = aggregate_max(similarities)\n",
    "                    \n",
    "                    best_impostor_score = max(best_impostor_score, score)\n",
    "                \n",
    "                impostor_scores.append(best_impostor_score)\n",
    "    else:\n",
    "        print(\"  Warning: No negative probes available. Impostor scores will be empty!\")\n",
    "    \n",
    "    print(f\"  Collected {len(genuine_scores)} genuine pairs\")\n",
    "    print(f\"  Collected {len(impostor_scores)} impostor scores\")\n",
    "    \n",
    "    if not genuine_scores:\n",
    "        raise ValueError(\"No genuine scores collected! Check probe_positive data.\")\n",
    "    if not impostor_scores:\n",
    "        raise ValueError(\"No impostor scores collected! Check probe_negative data.\")\n",
    "    \n",
    "    # ============================================\n",
    "    # Compute verification metrics at each threshold\n",
    "    # ============================================\n",
    "    threshold_results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        tp = sum(1 for s in genuine_scores if s >= threshold)\n",
    "        fn = len(genuine_scores) - tp\n",
    "        tn = sum(1 for s in impostor_scores if s < threshold)\n",
    "        fp = len(impostor_scores) - tn\n",
    "        \n",
    "        tar = tp / len(genuine_scores) if genuine_scores else 0\n",
    "        far = fp / len(impostor_scores) if impostor_scores else 0\n",
    "        frr = fn / len(genuine_scores) if genuine_scores else 0\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'threshold': threshold,\n",
    "            'tar': tar,\n",
    "            'far': far,\n",
    "            'frr': frr,\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'tn': tn,\n",
    "            'fn': fn\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(threshold_results)\n",
    "    \n",
    "    # ============================================\n",
    "    # Compute ROC curve\n",
    "    # ============================================\n",
    "    y_true = [1] * len(genuine_scores) + [0] * len(impostor_scores)\n",
    "    y_scores = genuine_scores + impostor_scores\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # ============================================\n",
    "    # Compute EER\n",
    "    # ============================================\n",
    "    eer_idx = (df['far'] - df['frr']).abs().idxmin()\n",
    "    eer = (df.loc[eer_idx, 'far'] + df.loc[eer_idx, 'frr']) / 2\n",
    "    eer_threshold = df.loc[eer_idx, 'threshold']\n",
    "    \n",
    "    # ============================================\n",
    "    # Compute TAR at specific FAR values\n",
    "    # ============================================\n",
    "    tar_at_far = {}\n",
    "    for target_far in [0.001, 0.01, 0.1]:\n",
    "        far_diffs = np.abs(df['far'] - target_far)\n",
    "        idx = far_diffs.idxmin()\n",
    "        tar_at_far[f'tar_at_far_{target_far}'] = df.loc[idx, 'tar']\n",
    "    \n",
    "    # ============================================\n",
    "    # Score statistics\n",
    "    # ============================================\n",
    "    genuine_mean = np.mean(genuine_scores)\n",
    "    genuine_std = np.std(genuine_scores, ddof=1)\n",
    "    impostor_mean = np.mean(impostor_scores)\n",
    "    impostor_std = np.std(impostor_scores, ddof=1)\n",
    "    \n",
    "    # d-prime\n",
    "    pooled_std = np.sqrt((genuine_std**2 + impostor_std**2) / 2)\n",
    "    dprime = (genuine_mean - impostor_mean) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Separation (same as d-prime)\n",
    "    pooled_variance = (genuine_std**2 + impostor_std**2) / 2\n",
    "    separation = abs(genuine_mean - impostor_mean) / np.sqrt(pooled_variance) if pooled_variance > 0 else 0\n",
    "    \n",
    "    genuine_ci = bootstrap_confidence_interval(genuine_scores)\n",
    "    impostor_ci = bootstrap_confidence_interval(impostor_scores)\n",
    "    \n",
    "    return {\n",
    "        'threshold_results': df,\n",
    "        'roc_auc': roc_auc,\n",
    "        'dprime': dprime,\n",
    "        'separation': separation,\n",
    "        'eer': eer,\n",
    "        'eer_threshold': eer_threshold,\n",
    "        **tar_at_far,\n",
    "        'genuine_mean': genuine_mean,\n",
    "        'genuine_std': genuine_std,\n",
    "        'impostor_mean': impostor_mean,\n",
    "        'impostor_std': impostor_std,\n",
    "        'genuine_scores': genuine_scores,\n",
    "        'impostor_scores': impostor_scores,\n",
    "        'genuine_ci': genuine_ci,\n",
    "        'impostor_ci': impostor_ci,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'aggregation': aggregation,\n",
    "        'n_genuine_pairs': len(genuine_scores),\n",
    "        'n_impostor_pairs': len(impostor_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466128a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_probes_comprehensive(gallery_embeddings: Dict[str, Dict],\n",
    "                                 probe_embeddings: Dict[str, Dict],\n",
    "                                 thresholds: List[float],\n",
    "                                 aggregation: str = 'mean',\n",
    "                                 k: int = 3) -> Dict:\n",
    "    probe_data = probe_embeddings.get(\"all\", probe_embeddings)\n",
    "    all_predictions = []\n",
    "    genuine_scores = []\n",
    "    impostor_scores = []\n",
    "    per_identity = {}  \n",
    "    \n",
    "    for true_name, data in tqdm(probe_data.items(), desc=f\"Processing probes ({aggregation})\"):\n",
    "        probe_embs = data['embeddings']\n",
    "        \n",
    "        for probe_emb in probe_embs:\n",
    "            predicted_name, best_score, identity_scores = identify_probe(\n",
    "                probe_emb, gallery_embeddings, threshold=0.0,\n",
    "                aggregation=aggregation, k=k\n",
    "            )\n",
    "            \n",
    "            rank_metrics = compute_rank_metrics(identity_scores, true_name)\n",
    "            \n",
    "            all_predictions.append({\n",
    "                'true_identity': true_name,\n",
    "                'predicted_identity': predicted_name,\n",
    "                'score': best_score,\n",
    "                'identity_scores': identity_scores,\n",
    "                'rank_metrics': rank_metrics\n",
    "            })\n",
    "\n",
    "            if true_name not in per_identity:\n",
    "                per_identity[true_name] = {\n",
    "                    \"rank1_total\": 0,\n",
    "                    \"rank5_total\": 0,\n",
    "                    \"rank10_total\": 0,\n",
    "                    \"mrr_sum\": 0,\n",
    "                    \"count\": 0,\n",
    "                    \"genuine_scores\": [],\n",
    "                    \"impostor_scores\": []\n",
    "                }\n",
    "\n",
    "            pid = per_identity[true_name]\n",
    "            pid[\"rank1_total\"] += 1 if rank_metrics[\"rank1\"] else 0\n",
    "            pid[\"rank5_total\"] += 1 if rank_metrics[\"rank5\"] else 0\n",
    "            pid[\"rank10_total\"] += 1 if rank_metrics[\"rank10\"] else 0\n",
    "            pid[\"mrr_sum\"] += rank_metrics[\"reciprocal_rank\"]\n",
    "            pid[\"count\"] += 1\n",
    "\n",
    "            # store score for genuine / impostor histogram\n",
    "            pid[\"genuine_scores\"].append(identity_scores.get(true_name, 0))\n",
    "            pid[\"impostor_scores\"].extend(\n",
    "                [v for id2, v in identity_scores.items() if id2 != true_name]\n",
    "            )\n",
    "            \n",
    "            if true_name in identity_scores:\n",
    "                genuine_scores.append(identity_scores[true_name])\n",
    "            \n",
    "            # Only collect impostor scores once per probe (use the best impostor match)\n",
    "            impostor_matches = [score for name, score in identity_scores.items() if name != true_name]\n",
    "            if impostor_matches:\n",
    "                impostor_scores.append(max(impostor_matches))  # or mean(impostor_matches)\n",
    "    \n",
    "    threshold_results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        tp = fp = tn = fn = 0\n",
    "        rank1_correct = rank5_correct = rank10_correct = 0\n",
    "        mrr_sum = 0\n",
    "        \n",
    "        correct_scores = []\n",
    "        incorrect_scores = []\n",
    "        \n",
    "        for pred in all_predictions:\n",
    "            true_name = pred['true_identity']\n",
    "            predicted_name = pred['predicted_identity']\n",
    "            score = pred['score']\n",
    "            rank_metrics = pred['rank_metrics']\n",
    "            \n",
    "            if score >= threshold:\n",
    "                if predicted_name == true_name:\n",
    "                    tp += 1\n",
    "                    correct_scores.append(score)\n",
    "                else:\n",
    "                    fp += 1\n",
    "                    incorrect_scores.append(score)\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "            if rank_metrics['rank1']:\n",
    "                rank1_correct += 1\n",
    "            if rank_metrics['rank5']:\n",
    "                rank5_correct += 1\n",
    "            if rank_metrics['rank10']:\n",
    "                rank10_correct += 1\n",
    "            mrr_sum += rank_metrics['reciprocal_rank']\n",
    "        \n",
    "        n_probes = len(all_predictions)\n",
    "\n",
    "        rank1_acc = rank1_correct / n_probes if n_probes > 0 else 0\n",
    "        rank5_acc = rank5_correct / n_probes if n_probes > 0 else 0\n",
    "        rank10_acc = rank10_correct / n_probes if n_probes > 0 else 0\n",
    "        mrr = mrr_sum / n_probes if n_probes > 0 else 0\n",
    "        \n",
    "        far = fp / n_probes if n_probes > 0 else 0\n",
    "        frr = fn / n_probes if n_probes > 0 else 0\n",
    "        tar = tp / n_probes if n_probes > 0 else 0\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'threshold': threshold,\n",
    "            'rank1_accuracy': rank1_acc,\n",
    "            'rank5_accuracy': rank5_acc,\n",
    "            'rank10_accuracy': rank10_acc,\n",
    "            'mrr': mrr,\n",
    "            'tar': tar,\n",
    "            'far': far,\n",
    "            'frr': frr,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'n_probes': n_probes,\n",
    "            'avg_correct_score': np.mean(correct_scores) if correct_scores else 0,\n",
    "            'avg_incorrect_score': np.mean(incorrect_scores) if incorrect_scores else 0,\n",
    "        })\n",
    "\n",
    "    dprime = compute_dprime(genuine_scores, impostor_scores)\n",
    "\n",
    "    # FIXED: Use consistent sample standard deviation (ddof=1)\n",
    "    genuine_mean = np.mean(genuine_scores)\n",
    "    genuine_std = np.std(genuine_scores, ddof=1)\n",
    "    impostor_mean = np.mean(impostor_scores)\n",
    "    impostor_std = np.std(impostor_scores, ddof=1)\n",
    "    \n",
    "    # FIXED: Correct separation formula - divide by sqrt of average variance\n",
    "    pooled_variance = (genuine_std**2 + impostor_std**2) / 2\n",
    "    separation = abs(genuine_mean - impostor_mean) / np.sqrt(pooled_variance)\n",
    "    \n",
    "    # Calculate EER\n",
    "    df_thresh = pd.DataFrame(threshold_results)\n",
    "    eer_idx = (df_thresh['far'] - df_thresh['frr']).abs().idxmin()\n",
    "    eer = (df_thresh.loc[eer_idx, 'far'] + df_thresh.loc[eer_idx, 'frr']) / 2\n",
    "    eer_threshold = df_thresh.loc[eer_idx, 'threshold']\n",
    "    \n",
    "    # Calculate TAR at specific FAR values\n",
    "    tar_at_far = {}\n",
    "    for target_far in [0.001, 0.01, 0.1]:\n",
    "        # Find threshold where FAR is closest to target\n",
    "        far_diffs = np.abs(df_thresh['far'] - target_far)\n",
    "        idx = far_diffs.idxmin()\n",
    "        tar_at_far[f'tar_at_far_{target_far}'] = df_thresh.loc[idx, 'tar']\n",
    "    \n",
    "    genuine_ci = bootstrap_confidence_interval(genuine_scores)\n",
    "    impostor_ci = bootstrap_confidence_interval(impostor_scores)\n",
    "    \n",
    "    per_identity_results = {}\n",
    "    for identity, stats in per_identity.items():\n",
    "        c = stats[\"count\"]\n",
    "        per_identity_results[identity] = {\n",
    "            \"rank1\": stats[\"rank1_total\"] / c,\n",
    "            \"rank5\": stats[\"rank5_total\"] / c,\n",
    "            \"rank10\": stats[\"rank10_total\"] / c,\n",
    "            \"mrr\": stats[\"mrr_sum\"] / c,\n",
    "            \"n_samples\": c,\n",
    "            \"genuine_scores\": stats[\"genuine_scores\"],\n",
    "            \"impostor_scores\": stats[\"impostor_scores\"]\n",
    "        }\n",
    "\n",
    "    print(f\"\\nDEBUG - Score Collection:\")\n",
    "    print(f\"  Total predictions: {len(all_predictions)}\")\n",
    "    print(f\"  Genuine scores collected: {len(genuine_scores)}\")\n",
    "    print(f\"  Impostor scores collected: {len(impostor_scores)}\")\n",
    "    print(f\"  Genuine mean: {np.mean(genuine_scores):.4f}\")\n",
    "    print(f\"  Impostor mean: {np.mean(impostor_scores):.4f}\")\n",
    "    print(f\"  Genuine min/max: {np.min(genuine_scores):.4f} / {np.max(genuine_scores):.4f}\")\n",
    "    print(f\"  Impostor min/max: {np.min(impostor_scores):.4f} / {np.max(impostor_scores):.4f}\")\n",
    "\n",
    "    # DEBUG: EER calculation\n",
    "    print(f\"\\nDEBUG - EER Calculation:\")\n",
    "    print(f\"  FAR range: {df_thresh['far'].min():.4f} to {df_thresh['far'].max():.4f}\")\n",
    "    print(f\"  FRR range: {df_thresh['frr'].min():.4f} to {df_thresh['frr'].max():.4f}\")\n",
    "    print(f\"  EER: {eer:.4f} at threshold {eer_threshold:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'threshold_results': pd.DataFrame(threshold_results),\n",
    "        'aggregation': aggregation,\n",
    "        'all_predictions': all_predictions,\n",
    "        'per_identity': per_identity_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_impostors_comprehensive(gallery_embeddings: Dict[str, Dict],\n",
    "                                    impostor_embeddings: Dict[str, Dict],\n",
    "                                    thresholds: List[float],\n",
    "                                    aggregation: str = 'mean',\n",
    "                                    k: int = 3) -> Dict:\n",
    "    impostor_scores = []\n",
    "    \n",
    "    for dataset_name, data in tqdm(impostor_embeddings.items(), desc=f\"Processing impostors ({aggregation})\"):\n",
    "        impostor_embs = data['embeddings']\n",
    "        \n",
    "        for impostor_emb in impostor_embs:\n",
    "            _, score, _ = identify_probe(\n",
    "                impostor_emb, gallery_embeddings, threshold=0.0,\n",
    "                aggregation=aggregation, k=k\n",
    "            )\n",
    "            impostor_scores.append(score)\n",
    "    \n",
    "    threshold_results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        tn = sum(1 for s in impostor_scores if s < threshold)\n",
    "        fp = sum(1 for s in impostor_scores if s >= threshold)\n",
    "        n_impostors = len(impostor_scores)\n",
    "        \n",
    "        rejection_rate = tn / n_impostors if n_impostors > 0 else 0\n",
    "        far = fp / n_impostors if n_impostors > 0 else 0\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'threshold': threshold,\n",
    "            'rejection_rate': rejection_rate,\n",
    "            'far': far,\n",
    "            'tn': tn,\n",
    "            'fp': fp,\n",
    "            'n_impostors': n_impostors,\n",
    "            'avg_impostor_score': np.mean(impostor_scores)\n",
    "        })\n",
    "    \n",
    "    impostor_ci = bootstrap_confidence_interval(impostor_scores)\n",
    "    \n",
    "    return {\n",
    "        'threshold_results': pd.DataFrame(threshold_results),\n",
    "        'impostor_scores': impostor_scores,\n",
    "        'impostor_ci': impostor_ci,\n",
    "        'mean_impostor_score': np.mean(impostor_scores),\n",
    "        'std_impostor_score': np.std(impostor_scores),\n",
    "        'aggregation': aggregation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4490280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmented_comprehensive(gallery_embeddings: Dict[str, Dict],\n",
    "                                    probe_positive: Dict[str, Dict],\n",
    "                                    probe_negative: Dict[str, Dict],\n",
    "                                    thresholds: List[float],\n",
    "                                    aggregation: str = 'mean',\n",
    "                                    k: int = 3,\n",
    "                                    include_verification: bool = True) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Segmented evaluation with both identification and verification.\n",
    "    \n",
    "    Args:\n",
    "        probe_positive: Segmented positive probes\n",
    "        probe_negative: Negative probes (not segmented, but used for all segments)\n",
    "    \"\"\"\n",
    "    segment_results = {}\n",
    "    segments = [k for k in probe_positive.keys() if k != 'all']\n",
    "    \n",
    "    print(f\"Found {len(segments)} segments: {segments}\")\n",
    "    \n",
    "    for segment_name in tqdm(segments, desc=f\"Processing segments ({aggregation})\"):\n",
    "        segment_data = probe_positive[segment_name]\n",
    "        segment_probe = {'all': segment_data}\n",
    "        \n",
    "        # Identification\n",
    "        id_results = evaluate_probes_comprehensive(\n",
    "            gallery_embeddings, segment_probe, thresholds,\n",
    "            aggregation=aggregation, k=k\n",
    "        )\n",
    "        \n",
    "        # Verification\n",
    "        ver_results = None\n",
    "        if include_verification and probe_negative is not None:\n",
    "            try:\n",
    "                ver_results = evaluate_verification_comprehensive(\n",
    "                    gallery_embeddings, segment_probe, probe_negative, thresholds,\n",
    "                    aggregation=aggregation, k=k\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Verification failed for {segment_name}: {e}\")\n",
    "                ver_results = None\n",
    "        \n",
    "        segment_results[segment_name] = {\n",
    "            'identification': id_results,\n",
    "            'verification': ver_results  # Can be None\n",
    "        }\n",
    "    \n",
    "    return segment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a99bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparison_summary(all_model_results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Generate comprehensive comparison table across all models\"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        basic_results = model_data.get('basic_probe', {})\n",
    "        \n",
    "        for gallery_name, gallery_results in basic_results.items():\n",
    "            # Skip the per_identity key - it's metadata, not gallery results\n",
    "            if gallery_name == 'per_identity':\n",
    "                continue\n",
    "                \n",
    "            for agg_method, combined_results in gallery_results.items():\n",
    "                # NEW: Extract nested structure\n",
    "                if not isinstance(combined_results, dict):\n",
    "                    continue\n",
    "                \n",
    "                id_results = combined_results.get('identification')\n",
    "                ver_results = combined_results.get('verification')\n",
    "                \n",
    "                if id_results is None:\n",
    "                    continue\n",
    "                \n",
    "                df = id_results['threshold_results']\n",
    "                best_idx = df['rank1_accuracy'].idxmax()\n",
    "                best_row = df.loc[best_idx]\n",
    "                \n",
    "                summary_entry = {\n",
    "                    'Model': model_name,\n",
    "                    'Gallery': gallery_name,\n",
    "                    'Aggregation': agg_method,\n",
    "                    'Rank-1': best_row['rank1_accuracy'],\n",
    "                    'Rank-5': best_row['rank5_accuracy'],\n",
    "                    'Rank-10': best_row['rank10_accuracy'],\n",
    "                    'MRR': best_row['mrr'],\n",
    "                    'Best_Threshold': best_row['threshold'],\n",
    "                }\n",
    "                \n",
    "                # Add verification metrics if available\n",
    "                if ver_results is not None:\n",
    "                    summary_entry.update({\n",
    "                        'ROC-AUC': ver_results['roc_auc'],\n",
    "                        'EER': ver_results['eer'],\n",
    "                        'd-prime': ver_results['dprime'],\n",
    "                        'Separation': ver_results['separation'],\n",
    "                        'TAR@0.1%FAR': ver_results['tar_at_far_0.001'],\n",
    "                        'TAR@1%FAR': ver_results['tar_at_far_0.01'],\n",
    "                        'TAR@10%FAR': ver_results['tar_at_far_0.1'],\n",
    "                    })\n",
    "                else:\n",
    "                    summary_entry.update({\n",
    "                        'ROC-AUC': np.nan,\n",
    "                        'EER': np.nan,\n",
    "                        'd-prime': np.nan,\n",
    "                        'Separation': np.nan,\n",
    "                        'TAR@0.1%FAR': np.nan,\n",
    "                        'TAR@1%FAR': np.nan,\n",
    "                        'TAR@10%FAR': np.nan,\n",
    "                    })\n",
    "                \n",
    "                summary_data.append(summary_entry)\n",
    "    \n",
    "    return pd.DataFrame(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segmented_comparison_table(all_model_results: Dict, \n",
    "                                     gallery_type: str = 'oneshot',\n",
    "                                     metric_type: str = 'identification') -> pd.DataFrame:\n",
    "    \"\"\"Create comparison table for segmented evaluations\n",
    "    \n",
    "    Args:\n",
    "        metric_type: 'identification' or 'verification'\n",
    "    \"\"\"\n",
    "    segment_data = []\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        seg_key = f'segmented_{gallery_type}'\n",
    "        if seg_key not in model_data:\n",
    "            continue\n",
    "            \n",
    "        segment_results = model_data[seg_key]\n",
    "        \n",
    "        for segment_name, results in segment_results.items():\n",
    "            # Handle new nested structure\n",
    "            if metric_type in results:\n",
    "                metric_results = results[metric_type]\n",
    "            else:\n",
    "                metric_results = results  # Backward compatibility\n",
    "            \n",
    "            if metric_type == 'identification':\n",
    "                df = metric_results['threshold_results']\n",
    "                best_idx = df['rank1_accuracy'].idxmax()\n",
    "                \n",
    "                segment_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Segment': segment_name,\n",
    "                    'Rank-1': df.loc[best_idx, 'rank1_accuracy'],\n",
    "                    'Rank-5': df.loc[best_idx, 'rank5_accuracy'],\n",
    "                    'MRR': df.loc[best_idx, 'mrr']\n",
    "                })\n",
    "            else:  # verification\n",
    "                segment_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Segment': segment_name,\n",
    "                    'AUC': metric_results['roc_auc'],\n",
    "                    'EER': metric_results['eer'],\n",
    "                    'd-prime': metric_results['dprime'],\n",
    "                    'TAR@1%FAR': metric_results['tar_at_far_0.01']\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(segment_data)\n",
    "    \n",
    "    # Pivot\n",
    "    value_col = 'Rank-1' if metric_type == 'identification' else 'AUC'\n",
    "    pivot = df.pivot(index='Model', columns='Segment', values=value_col)\n",
    "    pivot['Mean'] = pivot.mean(axis=1)\n",
    "    pivot['Std'] = pivot.std(axis=1)\n",
    "    pivot['Min'] = pivot.drop(['Mean', 'Std'], axis=1).min(axis=1)\n",
    "    pivot['Max'] = pivot.drop(['Mean', 'Std', 'Min'], axis=1).max(axis=1)\n",
    "    \n",
    "    return pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949df826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gallery_strategies(all_model_results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Compare oneshot vs fewshot, base vs augmented\"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        basic_results = model_data.get('basic_probe', {})\n",
    "        \n",
    "        # Get best performance for each configuration\n",
    "        configs = {}\n",
    "        for gallery_name, gallery_results in basic_results.items():\n",
    "            if gallery_name == 'per_identity':\n",
    "                continue\n",
    "            best_rank1 = 0\n",
    "            best_agg = None\n",
    "            for agg_method, combined_results in gallery_results.items():\n",
    "                # NEW: Handle nested structure\n",
    "                if not isinstance(combined_results, dict):\n",
    "                    continue\n",
    "                \n",
    "                id_results = combined_results.get('identification')\n",
    "                if id_results is None:\n",
    "                    continue\n",
    "                \n",
    "                df = id_results['threshold_results']\n",
    "                rank1 = df['rank1_accuracy'].max()\n",
    "                if rank1 > best_rank1:\n",
    "                    best_rank1 = rank1\n",
    "                    best_agg = agg_method\n",
    "            configs[gallery_name] = {'rank1': best_rank1, 'agg': best_agg}\n",
    "        \n",
    "        # Calculate improvements\n",
    "        oneshot_base = configs.get('oneshot_base', {}).get('rank1', 0)\n",
    "        oneshot_aug = configs.get('oneshot_augmented', {}).get('rank1', 0)\n",
    "        fewshot_base = configs.get('fewshot_base', {}).get('rank1', 0)\n",
    "        fewshot_aug = configs.get('fewshot_augmented', {}).get('rank1', 0)\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Oneshot_Base': oneshot_base,\n",
    "            'Oneshot_Aug': oneshot_aug,\n",
    "            'Fewshot_Base': fewshot_base,\n",
    "            'Fewshot_Aug': fewshot_aug,\n",
    "            'Aug_Improvement_Oneshot': oneshot_aug - oneshot_base,\n",
    "            'Aug_Improvement_Fewshot': fewshot_aug - fewshot_base,\n",
    "            'Fewshot_Improvement_Base': fewshot_base - oneshot_base,\n",
    "            'Fewshot_Improvement_Aug': fewshot_aug - oneshot_aug,\n",
    "            'Best_Config': max(configs.items(), key=lambda x: x[1]['rank1'])[0] if configs else 'N/A',\n",
    "            'Best_Rank1': max(c['rank1'] for c in configs.values()) if configs else 0\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384420ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_aggregation_performance(all_model_results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Analyze which aggregation method works best\"\"\"\n",
    "    agg_data = []\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        basic_results = model_data.get('basic_probe', {})\n",
    "        \n",
    "        for gallery_name, gallery_results in basic_results.items():\n",
    "            if gallery_name == 'per_identity':\n",
    "                continue\n",
    "            agg_scores = {}\n",
    "            for agg_method, combined_results in gallery_results.items():\n",
    "                # NEW: Handle nested structure\n",
    "                if not isinstance(combined_results, dict):\n",
    "                    continue\n",
    "                \n",
    "                id_results = combined_results.get('identification')\n",
    "                if id_results is None:\n",
    "                    continue\n",
    "                \n",
    "                df = id_results['threshold_results']\n",
    "                agg_scores[agg_method] = df['rank1_accuracy'].max()\n",
    "            \n",
    "            if not agg_scores:\n",
    "                continue\n",
    "            \n",
    "            best_agg = max(agg_scores.items(), key=lambda x: x[1])\n",
    "            \n",
    "            agg_data.append({\n",
    "                'Model': model_name,\n",
    "                'Gallery': gallery_name,\n",
    "                'Best_Aggregation': best_agg[0],\n",
    "                'MAX_Score': agg_scores.get('max', 0),\n",
    "                'MEAN_Score': agg_scores.get('mean', 0),\n",
    "                'TOPK_Score': agg_scores.get('topk', 0),\n",
    "                'Best_Score': best_agg[1],\n",
    "                'Score_Range': max(agg_scores.values()) - min(agg_scores.values()) if agg_scores else 0\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(agg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116bd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_operating_thresholds(all_model_results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Recommend thresholds for different operating points\"\"\"\n",
    "    threshold_recs = []\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        basic_results = model_data.get('basic_probe', {})\n",
    "        \n",
    "        for gallery_name, gallery_results in basic_results.items():\n",
    "            if gallery_name == 'per_identity':\n",
    "                continue\n",
    "            for agg_method, combined_results in gallery_results.items():\n",
    "                # NEW: Handle nested structure\n",
    "                if not isinstance(combined_results, dict):\n",
    "                    continue\n",
    "                \n",
    "                id_results = combined_results.get('identification')\n",
    "                ver_results = combined_results.get('verification')\n",
    "                \n",
    "                if id_results is None:\n",
    "                    continue\n",
    "                \n",
    "                df = id_results['threshold_results']\n",
    "                \n",
    "                # Find various operating points based on identification\n",
    "                rank1_max_idx = df['rank1_accuracy'].idxmax()\n",
    "                \n",
    "                threshold_rec = {\n",
    "                    'Model': model_name,\n",
    "                    'Gallery': gallery_name,\n",
    "                    'Aggregation': agg_method,\n",
    "                    'Threshold_BestRank1': df.loc[rank1_max_idx, 'threshold'],\n",
    "                    'Rank1_at_BestThreshold': df.loc[rank1_max_idx, 'rank1_accuracy'],\n",
    "                }\n",
    "                \n",
    "                # Add verification thresholds if available\n",
    "                if ver_results is not None:\n",
    "                    ver_df = ver_results['threshold_results']\n",
    "                    \n",
    "                    # EER threshold\n",
    "                    eer_idx = (ver_df['far'] - ver_df['frr']).abs().idxmin()\n",
    "                    \n",
    "                    # FAR targets\n",
    "                    far_001_idx = (ver_df['far'] - 0.001).abs().idxmin()\n",
    "                    far_01_idx = (ver_df['far'] - 0.01).abs().idxmin()\n",
    "                    \n",
    "                    threshold_rec.update({\n",
    "                        'Threshold_EER': ver_df.loc[eer_idx, 'threshold'],\n",
    "                        'EER': (ver_df.loc[eer_idx, 'far'] + ver_df.loc[eer_idx, 'frr']) / 2,\n",
    "                        'Threshold_FAR0.1%': ver_df.loc[far_001_idx, 'threshold'],\n",
    "                        'TAR_at_FAR0.1%': ver_df.loc[far_001_idx, 'tar'],\n",
    "                        'Threshold_FAR1%': ver_df.loc[far_01_idx, 'threshold'],\n",
    "                        'TAR_at_FAR1%': ver_df.loc[far_01_idx, 'tar'],\n",
    "                    })\n",
    "                \n",
    "                threshold_recs.append(threshold_rec)\n",
    "    \n",
    "    return pd.DataFrame(threshold_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_failure_cases(all_model_results: Dict) -> Dict:\n",
    "    \"\"\"Analyze failure patterns\"\"\"\n",
    "    failure_analysis = {}\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        basic_results = model_data.get('basic_probe', {})\n",
    "        \n",
    "        for gallery_name, gallery_results in basic_results.items():\n",
    "            if gallery_name == 'per_identity':\n",
    "                continue\n",
    "            # Use mean aggregation for analysis\n",
    "            if 'mean' not in gallery_results:\n",
    "                continue\n",
    "            \n",
    "            combined_results = gallery_results['mean']\n",
    "            \n",
    "            # NEW: Handle nested structure\n",
    "            if not isinstance(combined_results, dict):\n",
    "                continue\n",
    "            \n",
    "            id_results = combined_results.get('identification')\n",
    "            if id_results is None:\n",
    "                continue\n",
    "            \n",
    "            predictions = id_results.get('all_predictions', [])\n",
    "            \n",
    "            if not predictions:\n",
    "                continue\n",
    "            \n",
    "            # Find misclassifications\n",
    "            misclassified = [p for p in predictions if p['predicted_identity'] != p['true_identity']]\n",
    "            \n",
    "            # Count confusion pairs\n",
    "            confusion_pairs = {}\n",
    "            identity_errors = {}\n",
    "            \n",
    "            for pred in misclassified:\n",
    "                true_id = pred['true_identity']\n",
    "                pred_id = pred['predicted_identity']\n",
    "                \n",
    "                if pred_id is None:\n",
    "                    pred_id = \"REJECTED\"\n",
    "                \n",
    "                pair = f\"{true_id} -> {pred_id}\"\n",
    "                confusion_pairs[pair] = confusion_pairs.get(pair, 0) + 1\n",
    "                \n",
    "                identity_errors[true_id] = identity_errors.get(true_id, 0) + 1\n",
    "            \n",
    "            # Sort by frequency\n",
    "            top_confusions = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            top_errors = sorted(identity_errors.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            \n",
    "            failure_analysis[f\"{model_name}_{gallery_name}\"] = {\n",
    "                'total_predictions': len(predictions),\n",
    "                'total_errors': len(misclassified),\n",
    "                'error_rate': len(misclassified) / len(predictions) if predictions else 0,\n",
    "                'top_confusion_pairs': top_confusions,\n",
    "                'most_confused_identities': top_errors\n",
    "            }\n",
    "    \n",
    "    return failure_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5acdbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_statistical(all_model_results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Statistical significance testing between models\"\"\"\n",
    "    stat_comparisons = []\n",
    "    \n",
    "    models = list(all_model_results.keys())\n",
    "    \n",
    "    for i, model1 in enumerate(models):\n",
    "        for model2 in models[i+1:]:\n",
    "            # Compare on fewshot_augmented + mean (best config)\n",
    "            try:\n",
    "                combined1 = all_model_results[model1]['basic_probe']['fewshot_augmented']['mean']\n",
    "                combined2 = all_model_results[model2]['basic_probe']['fewshot_augmented']['mean']\n",
    "                \n",
    "                # NEW: Handle nested structure\n",
    "                id_results1 = combined1.get('identification')\n",
    "                id_results2 = combined2.get('identification')\n",
    "                \n",
    "                if id_results1 is None or id_results2 is None:\n",
    "                    continue\n",
    "                \n",
    "                scores1 = [p['score'] if p['predicted_identity'] == p['true_identity'] else 0 \n",
    "                          for p in id_results1['all_predictions']]\n",
    "                scores2 = [p['score'] if p['predicted_identity'] == p['true_identity'] else 0 \n",
    "                          for p in id_results2['all_predictions']]\n",
    "                \n",
    "                # Paired t-test\n",
    "                t_stat, p_value = stats.ttest_rel(scores1, scores2)\n",
    "                \n",
    "                # Effect size (Cohen's d)\n",
    "                mean_diff = np.mean(scores1) - np.mean(scores2)\n",
    "                pooled_std = np.sqrt((np.std(scores1)**2 + np.std(scores2)**2) / 2)\n",
    "                cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "                \n",
    "                stat_comparisons.append({\n",
    "                    'Model_A': model1,\n",
    "                    'Model_B': model2,\n",
    "                    'Mean_Diff': mean_diff,\n",
    "                    't_statistic': t_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'Significant': 'Yes' if p_value < 0.05 else 'No',\n",
    "                    'Cohens_d': cohens_d,\n",
    "                    'Effect_Size': 'Small' if abs(cohens_d) < 0.5 else ('Medium' if abs(cohens_d) < 0.8 else 'Large')\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not compare {model1} vs {model2}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(stat_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab05fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_executive_summary(all_model_results: Dict, \n",
    "                              comparison_summary: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate auto-summary of key findings\"\"\"\n",
    "    \n",
    "    # Best overall model\n",
    "    best_row = comparison_summary.loc[comparison_summary['Rank-1'].idxmax()]\n",
    "    \n",
    "    # Best per gallery type\n",
    "    best_per_gallery = comparison_summary.groupby('Gallery').apply(\n",
    "        lambda x: x.loc[x['Rank-1'].idxmax()]\n",
    "    )\n",
    "    \n",
    "    summary = f\"\"\"\n",
    "================================================================================\n",
    "EXECUTIVE SUMMARY - Face Recognition Evaluation\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "================================================================================\n",
    "\n",
    "KEY FINDINGS:\n",
    "\n",
    "1. OVERALL BEST PERFORMANCE\n",
    "   Model: {best_row['Model']}\n",
    "   Configuration: {best_row['Gallery']} + {best_row['Aggregation']}\n",
    "   Rank-1 Accuracy: {best_row['Rank-1']:.2%}\n",
    "   ROC-AUC: {best_row['ROC-AUC']:.4f}\n",
    "   d-prime: {best_row['d-prime']:.3f}\n",
    "\n",
    "2. BEST CONFIGURATION PER GALLERY TYPE\n",
    "\"\"\"\n",
    "    \n",
    "    for gallery, row in best_per_gallery.iterrows():\n",
    "        summary += f\"\"\"\n",
    "   {gallery.upper()}:\n",
    "   - Model: {row['Model']} ({row['Aggregation']})\n",
    "   - Rank-1: {row['Rank-1']:.2%}\n",
    "   - ROC-AUC: {row['ROC-AUC']:.4f}\n",
    "\"\"\"\n",
    "    \n",
    "    # Model rankings\n",
    "    model_rankings = comparison_summary.groupby('Model')['Rank-1'].max().sort_values(ascending=False)\n",
    "    \n",
    "    summary += f\"\"\"\n",
    "3. MODEL RANKINGS (by best Rank-1 accuracy)\n",
    "\"\"\"\n",
    "    for idx, (model, score) in enumerate(model_rankings.items(), 1):\n",
    "        summary += f\"   {idx}. {model}: {score:.2%}\\n\"\n",
    "    \n",
    "    # Aggregation method analysis\n",
    "    agg_wins = comparison_summary.groupby(['Gallery', 'Aggregation'])['Rank-1'].max()\n",
    "    best_agg_per_gallery = agg_wins.groupby('Gallery').idxmax()\n",
    "    \n",
    "    summary += f\"\"\"\n",
    "4. BEST AGGREGATION METHOD PER GALLERY\n",
    "\"\"\"\n",
    "    for gallery, (_, agg) in best_agg_per_gallery.items():\n",
    "        summary += f\"   {gallery}: {agg.upper()}\\n\"\n",
    "    \n",
    "    summary += f\"\"\"\n",
    "5. KEY RECOMMENDATIONS\n",
    "   - Use {best_row['Model']} with {best_row['Gallery']} gallery for best accuracy\n",
    "   - {best_row['Aggregation'].upper()} aggregation works best for this configuration\n",
    "   - Operating threshold: {best_row['Best_Threshold']:.3f} for optimal performance\n",
    "   - All models achieve 100% impostor rejection at threshold  0.35\n",
    "\n",
    "6. LIMITATIONS\n",
    "   - Performance degrades significantly on high pitch and high yaw conditions\n",
    "   - Low quality images reduce accuracy by ~15-30%\n",
    "   - Baseline/frontal images show best performance (>90% Rank-1)\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics(results: Dict, title: str, save_path: Path):\n",
    "    df = results['threshold_results']\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(df['threshold'], df['rank1_accuracy'], 'b-', linewidth=2, label='Rank-1')\n",
    "    ax1.plot(df['threshold'], df['rank5_accuracy'], 'g-', linewidth=2, label='Rank-5')\n",
    "    ax1.plot(df['threshold'], df['rank10_accuracy'], 'r-', linewidth=2, label='Rank-10')\n",
    "    ax1.set_xlabel('Threshold')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Rank-k Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(df['threshold'], df['mrr'], 'purple', linewidth=2)\n",
    "    ax2.set_xlabel('Threshold')\n",
    "    ax2.set_ylabel('MRR')\n",
    "    ax2.set_title('Mean Reciprocal Rank')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.plot(df['threshold'], df['far'], 'r-', linewidth=2, label='FAR')\n",
    "    ax3.plot(df['threshold'], df['frr'], 'g-', linewidth=2, label='FRR')\n",
    "    ax3.plot(df['threshold'], df['tar'], 'b-', linewidth=2, label='TAR')\n",
    "    ax3.set_xlabel('Threshold')\n",
    "    ax3.set_ylabel('Rate')\n",
    "    ax3.set_title('FAR/FRR/TAR')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    ax4.plot(results['fpr'], results['tpr'], 'b-', linewidth=2)\n",
    "    ax4.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    ax4.set_xlabel('False Positive Rate')\n",
    "    ax4.set_ylabel('True Positive Rate')\n",
    "    ax4.set_title(f'ROC Curve (AUC={results[\"roc_auc\"]:.4f})')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    ax5 = fig.add_subplot(gs[1, 0])\n",
    "    ax5.plot(df['threshold'], df['precision'], 'b-', linewidth=2, label='Precision')\n",
    "    ax5.plot(df['threshold'], df['recall'], 'orange', linewidth=2, label='Recall')\n",
    "    ax5.plot(df['threshold'], df['f1_score'], 'purple', linewidth=2, label='F1-Score')\n",
    "    ax5.set_xlabel('Threshold')\n",
    "    ax5.set_ylabel('Score')\n",
    "    ax5.set_title('Precision/Recall/F1')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "    ax6 = fig.add_subplot(gs[1, 1])\n",
    "    ax6.hist(results['genuine_scores'], bins=50, alpha=0.5, label='Genuine', color='green')\n",
    "    ax6.hist(results['impostor_scores'], bins=50, alpha=0.5, label='Impostor', color='red')\n",
    "    ax6.axvline(np.mean(results['genuine_scores']), color='green', linestyle='--', linewidth=2)\n",
    "    ax6.axvline(np.mean(results['impostor_scores']), color='red', linestyle='--', linewidth=2)\n",
    "    ax6.set_xlabel('Similarity Score')\n",
    "    ax6.set_ylabel('Frequency')\n",
    "    ax6.set_title(f\"Score Distributions (d'={results['dprime']:.3f})\")\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "  \n",
    "    ax7 = fig.add_subplot(gs[1, 2])\n",
    "    ax7.plot(df['far'], df['frr'], 'b-', linewidth=2)\n",
    "    ax7.set_xlabel('False Accept Rate')\n",
    "    ax7.set_ylabel('False Reject Rate')\n",
    "    ax7.set_title('DET Curve')\n",
    "    ax7.set_xscale('log')\n",
    "    ax7.set_yscale('log')\n",
    "    ax7.grid(True, alpha=0.3, which='both')\n",
    " \n",
    "    ax8 = fig.add_subplot(gs[1, 3])\n",
    "    best_threshold_idx = df['rank1_accuracy'].idxmax()\n",
    "    ranks = [1, 5, 10]\n",
    "    cmc_scores = [\n",
    "        df.loc[best_threshold_idx, 'rank1_accuracy'],\n",
    "        df.loc[best_threshold_idx, 'rank5_accuracy'],\n",
    "        df.loc[best_threshold_idx, 'rank10_accuracy']\n",
    "    ]\n",
    "    ax8.plot(ranks, cmc_scores, 'bo-', linewidth=2, markersize=8)\n",
    "    ax8.set_xlabel('Rank')\n",
    "    ax8.set_ylabel('Identification Rate')\n",
    "    ax8.set_title('CMC Curve')\n",
    "    ax8.set_xticks(ranks)\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "\n",
    "    ax9 = fig.add_subplot(gs[2, 0])\n",
    "    ax9.plot(df['threshold'], df['avg_correct_score'], 'g-', linewidth=2, label='Correct Matches')\n",
    "    ax9.plot(df['threshold'], df['avg_incorrect_score'], 'r-', linewidth=2, label='Incorrect Matches')\n",
    "    ax9.set_xlabel('Threshold')\n",
    "    ax9.set_ylabel('Average Score')\n",
    "    ax9.set_title('Score Analysis')\n",
    "    ax9.legend()\n",
    "    ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax10 = fig.add_subplot(gs[2, 1])\n",
    "    genuine_mean = np.mean(results['genuine_scores'])\n",
    "    impostor_mean = np.mean(results['impostor_scores'])\n",
    "    genuine_ci = results['genuine_ci']\n",
    "    impostor_ci = results['impostor_ci']\n",
    "    \n",
    "    categories = ['Genuine', 'Impostor']\n",
    "    means = [genuine_mean, impostor_mean]\n",
    "    errors_lower = [genuine_mean - genuine_ci[0], impostor_mean - impostor_ci[0]]\n",
    "    errors_upper = [genuine_ci[1] - genuine_mean, impostor_ci[1] - impostor_mean]\n",
    "    \n",
    "    ax10.bar(categories, means, yerr=[errors_lower, errors_upper], \n",
    "            capsize=10, alpha=0.7, color=['green', 'red'])\n",
    "    ax10.set_ylabel('Similarity Score')\n",
    "    ax10.set_title('Mean Scores with 95% CI')\n",
    "    ax10.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    ax11 = fig.add_subplot(gs[2, 2])\n",
    "    target_fars = [0.1, 0.01, 0.001]\n",
    "    tars_at_far = []\n",
    "    for target_far in target_fars:\n",
    "        idx = (df['far'] - target_far).abs().idxmin()\n",
    "        tars_at_far.append(df.loc[idx, 'tar'])\n",
    "    \n",
    "    ax11.bar([f'FAR={f}' for f in target_fars], tars_at_far, alpha=0.7)\n",
    "    ax11.set_ylabel('TAR')\n",
    "    ax11.set_title('TAR @ FAR')\n",
    "    ax11.set_ylim([0, 1])\n",
    "    ax11.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    ax12 = fig.add_subplot(gs[2, 3])\n",
    "    ax12.axis('off')\n",
    "    \n",
    "    best_idx = df['rank1_accuracy'].idxmax()\n",
    "    best_row = df.loc[best_idx]\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    SUMMARY STATISTICS\n",
    "    ==================\n",
    "    Aggregation: {results['aggregation'].upper()}\n",
    "    \n",
    "    Best Rank-1: {best_row['rank1_accuracy']:.4f}\n",
    "    @ Threshold: {best_row['threshold']:.3f}\n",
    "    \n",
    "    Rank-5: {best_row['rank5_accuracy']:.4f}\n",
    "    Rank-10: {best_row['rank10_accuracy']:.4f}\n",
    "    MRR: {best_row['mrr']:.4f}\n",
    "    \n",
    "    ROC-AUC: {results['roc_auc']:.4f}\n",
    "    Avg Precision: {results['average_precision']:.4f}\n",
    "    d-prime: {results['dprime']:.3f}\n",
    "    \n",
    "    TAR@FAR=0.01: {tars_at_far[1]:.4f}\n",
    "    \n",
    "    Best F1: {df['f1_score'].max():.4f}\n",
    "    @ Threshold: {df.loc[df['f1_score'].idxmax(), 'threshold']:.3f}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax12.text(0.1, 0.5, summary_text, fontsize=10, family='monospace',\n",
    "             verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_core_metrics_identification(results: Dict, title: str, save_path: Path):\n",
    "    \"\"\"Plot identification metrics only (Rank-K, CMC)\"\"\"\n",
    "    df = results['threshold_results']\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Rank-k Accuracy vs Threshold\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(df['threshold'], df['rank1_accuracy'], 'b-', linewidth=2.5, label='Rank-1')\n",
    "    ax1.plot(df['threshold'], df['rank5_accuracy'], 'g-', linewidth=2.5, label='Rank-5')\n",
    "    ax1.plot(df['threshold'], df['rank10_accuracy'], 'r-', linewidth=2.5, label='Rank-10')\n",
    "    ax1.set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('Rank-k Accuracy vs Threshold', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. MRR vs Threshold\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(df['threshold'], df['mrr'], 'purple', linewidth=2.5)\n",
    "    ax2.set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylabel('MRR', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('Mean Reciprocal Rank', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. CMC Curve (at best threshold)\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    best_threshold_idx = df['rank1_accuracy'].idxmax()\n",
    "    ranks = [1, 5, 10]\n",
    "    cmc_scores = [\n",
    "        df.loc[best_threshold_idx, 'rank1_accuracy'],\n",
    "        df.loc[best_threshold_idx, 'rank5_accuracy'],\n",
    "        df.loc[best_threshold_idx, 'rank10_accuracy']\n",
    "    ]\n",
    "    ax3.plot(ranks, cmc_scores, 'bo-', linewidth=2.5, markersize=10)\n",
    "    ax3.set_xlabel('Rank', fontsize=11, fontweight='bold')\n",
    "    ax3.set_ylabel('Identification Rate', fontsize=11, fontweight='bold')\n",
    "    ax3.set_title('CMC Curve (at best threshold)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xticks(ranks)\n",
    "    ax3.set_ylim([0, 1.05])\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
    "    \n",
    "    # 4. Summary Statistics\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    best_idx = df['rank1_accuracy'].idxmax()\n",
    "    best_row = df.loc[best_idx]\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "IDENTIFICATION SUMMARY\n",
    "{'='*35}\n",
    "Aggregation: {results['aggregation'].upper()}\n",
    "\n",
    "Best Rank-1:     {best_row['rank1_accuracy']:.4f}\n",
    "@ Threshold:     {best_row['threshold']:.3f}\n",
    "\n",
    "Rank-5:          {best_row['rank5_accuracy']:.4f}\n",
    "Rank-10:         {best_row['rank10_accuracy']:.4f}\n",
    "MRR:             {best_row['mrr']:.4f}\n",
    "\n",
    "Total Probes:    {best_row['n_probes']}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax4.text(0.1, 0.5, summary_text, fontsize=11, family='monospace',\n",
    "             verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Identification plot saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff248a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_core_metrics_verification(results: Dict, title: str, save_path: Path):\n",
    "    \"\"\"Plot verification metrics only (ROC, DET, Score distributions)\"\"\"\n",
    "    df = results['threshold_results']\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. ROC Curve\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(results['fpr'], results['tpr'], 'b-', linewidth=2.5)\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=1.5)\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title(f'ROC Curve (AUC={results[\"roc_auc\"]:.4f})', fontsize=12, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # 2. FAR/FRR/TAR\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(df['threshold'], df['far'], 'r-', linewidth=2.5, label='FAR')\n",
    "    ax2.plot(df['threshold'], df['frr'], 'orange', linewidth=2.5, label='FRR')\n",
    "    ax2.plot(df['threshold'], df['tar'], 'b-', linewidth=2.5, label='TAR')\n",
    "    \n",
    "    # Mark EER point\n",
    "    eer_idx = (df['far'] - df['frr']).abs().idxmin()\n",
    "    eer_threshold = df.loc[eer_idx, 'threshold']\n",
    "    eer_value = (df.loc[eer_idx, 'far'] + df.loc[eer_idx, 'frr']) / 2\n",
    "    ax2.plot(eer_threshold, eer_value, 'ko', markersize=8, label=f'EER={eer_value:.3f}')\n",
    "    \n",
    "    ax2.set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylabel('Rate', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('FAR/FRR/TAR Analysis', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. TAR @ FAR\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    target_fars = [0.1, 0.01, 0.001]\n",
    "    tars_at_far = []\n",
    "    for target_far in target_fars:\n",
    "        idx = (df['far'] - target_far).abs().idxmin()\n",
    "        tars_at_far.append(df.loc[idx, 'tar'])\n",
    "    \n",
    "    bars = ax3.bar([f'FAR={f}' for f in target_fars], tars_at_far, alpha=0.7, color='steelblue')\n",
    "    ax3.set_ylabel('TAR', fontsize=11, fontweight='bold')\n",
    "    ax3.set_title('TAR @ FAR', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylim([0, 1.1])\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, tar in zip(bars, tars_at_far):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{tar:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 4. Score Distribution\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    ax4.hist(results['genuine_scores'], bins=50, alpha=0.6, label='Genuine', \n",
    "             color='green', edgecolor='black')\n",
    "    ax4.hist(results['impostor_scores'], bins=50, alpha=0.6, label='Impostor', \n",
    "             color='red', edgecolor='black')\n",
    "    ax4.axvline(np.mean(results['genuine_scores']), color='darkgreen', \n",
    "                linestyle='--', linewidth=2, label='Genuine Mean')\n",
    "    ax4.axvline(np.mean(results['impostor_scores']), color='darkred', \n",
    "                linestyle='--', linewidth=2, label='Impostor Mean')\n",
    "    ax4.set_xlabel('Similarity Score', fontsize=11, fontweight='bold')\n",
    "    ax4.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "    ax4.set_title('Score Distributions', fontsize=12, fontweight='bold')\n",
    "    ax4.legend(fontsize=9)\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 5. Score Confidence Intervals\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    genuine_mean = np.mean(results['genuine_scores'])\n",
    "    impostor_mean = np.mean(results['impostor_scores'])\n",
    "    genuine_ci = results['genuine_ci']\n",
    "    impostor_ci = results['impostor_ci']\n",
    "    \n",
    "    categories = ['Genuine', 'Impostor']\n",
    "    means = [genuine_mean, impostor_mean]\n",
    "    errors_lower = [genuine_mean - genuine_ci[0], impostor_mean - impostor_ci[0]]\n",
    "    errors_upper = [genuine_ci[1] - genuine_mean, impostor_ci[1] - impostor_mean]\n",
    "    \n",
    "    bars = ax5.bar(categories, means, yerr=[errors_lower, errors_upper], \n",
    "                   capsize=10, alpha=0.7, color=['green', 'red'], \n",
    "                   edgecolor='black', linewidth=1.5)\n",
    "    ax5.set_ylabel('Similarity Score', fontsize=11, fontweight='bold')\n",
    "    ax5.set_title('Mean Scores with 95% CI', fontsize=12, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, mean in zip(bars, means):\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{mean:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 6. Summary Statistics\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "VERIFICATION SUMMARY\n",
    "{'='*35}\n",
    "Aggregation: {results['aggregation'].upper()}\n",
    "\n",
    "ROC-AUC:         {results['roc_auc']:.4f}\n",
    "EER:             {results['eer']:.4f}\n",
    "EER Threshold:   {results['eer_threshold']:.3f}\n",
    "\n",
    "TAR @ FAR=10%:   {tars_at_far[0]:.4f}\n",
    "TAR @ FAR=1%:    {tars_at_far[1]:.4f}\n",
    "TAR @ FAR=0.1%:  {tars_at_far[2]:.4f}\n",
    "\n",
    "d-prime:         {results['dprime']:.4f}\n",
    "Separation:      {results['separation']:.4f}\n",
    "\n",
    "Genuine :       {genuine_mean:.4f}\n",
    "Impostor :      {impostor_mean:.4f}\n",
    ":              {abs(genuine_mean - impostor_mean):.4f}\n",
    "\n",
    "Pairs:\n",
    "  Genuine:       {results['n_genuine_pairs']}\n",
    "  Impostor:      {results['n_impostor_pairs']}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.1, 0.5, summary_text, fontsize=10, family='monospace',\n",
    "             verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Verification plot saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_paper_figures_identification(results: Dict, model_name: str, save_dir: Path):\n",
    "    \"\"\"Generate publication-quality identification plots\"\"\"\n",
    "    df = results['threshold_results']\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. CMC Curve\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    best_threshold_idx = df['rank1_accuracy'].idxmax()\n",
    "    available_ranks = [1, 5, 10]\n",
    "    cmc_scores = [\n",
    "        df.loc[best_threshold_idx, 'rank1_accuracy'],\n",
    "        df.loc[best_threshold_idx, 'rank5_accuracy'],\n",
    "        df.loc[best_threshold_idx, 'rank10_accuracy']\n",
    "    ]\n",
    "    \n",
    "    ax.plot(available_ranks, cmc_scores, 'bo-', linewidth=3, markersize=10, \n",
    "            markeredgecolor='white', markeredgewidth=2)\n",
    "    ax.set_xlabel('Rank', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Identification Rate', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'CMC Curve - {model_name}', fontsize=16, fontweight='bold')\n",
    "    ax.set_xticks(available_ranks)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / f'{model_name}_cmc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"CMC curve saved: {save_dir / f'{model_name}_cmc_curve.png'}\")\n",
    "\n",
    "\n",
    "def plot_paper_figures_verification(results: Dict, model_name: str, save_dir: Path):\n",
    "    \"\"\"Generate publication-quality verification plots\"\"\"\n",
    "    df = results['threshold_results']\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. ROC Curve\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(results['fpr'], results['tpr'], 'b-', linewidth=3, \n",
    "            label=f'AUC = {results[\"roc_auc\"]:.4f}')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.4, linewidth=2, label='Random')\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'ROC Curve - {model_name}', fontsize=16, fontweight='bold')\n",
    "    ax.legend(fontsize=12, loc='lower right')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / f'{model_name}_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"ROC curve saved: {save_dir / f'{model_name}_roc_curve.png'}\")\n",
    "    \n",
    "    # 2. TAR @ FAR\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    target_fars = [0.1, 0.01, 0.001, 0.0001]\n",
    "    far_labels = ['10%', '1%', '0.1%', '0.01%']\n",
    "    tars_at_far = []\n",
    "    \n",
    "    for target_far in target_fars:\n",
    "        idx = (df['far'] - target_far).abs().idxmin()\n",
    "        tars_at_far.append(df.loc[idx, 'tar'])\n",
    "    \n",
    "    bars = ax.bar(far_labels, tars_at_far, alpha=0.7, color='steelblue', \n",
    "                   edgecolor='black', linewidth=2)\n",
    "    \n",
    "    for bar, tar in zip(bars, tars_at_far):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{tar:.3f}', ha='center', va='bottom', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('False Accept Rate', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('True Accept Rate', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'TAR @ FAR - {model_name}', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / f'{model_name}_tar_at_far.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"TAR@FAR plot saved: {save_dir / f'{model_name}_tar_at_far.png'}\")\n",
    "    \n",
    "    # 3. Score Distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.hist(results['genuine_scores'], bins=60, alpha=0.6, label='Genuine (Intra-class)', \n",
    "            color='green', edgecolor='black', linewidth=1.2)\n",
    "    ax.hist(results['impostor_scores'], bins=60, alpha=0.6, label='Impostor (Inter-class)', \n",
    "            color='red', edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    genuine_mean = np.mean(results['genuine_scores'])\n",
    "    impostor_mean = np.mean(results['impostor_scores'])\n",
    "    \n",
    "    ax.axvline(genuine_mean, color='darkgreen', linestyle='--', linewidth=2.5, \n",
    "               label=f'Genuine ={genuine_mean:.3f}')\n",
    "    ax.axvline(impostor_mean, color='darkred', linestyle='--', linewidth=2.5, \n",
    "               label=f'Impostor ={impostor_mean:.3f}')\n",
    "    \n",
    "    ax.set_xlabel('Cosine Similarity Score', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'Score Distribution - {model_name}', fontsize=16, fontweight='bold')\n",
    "    ax.legend(fontsize=11, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / f'{model_name}_score_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Score distribution saved: {save_dir / f'{model_name}_score_distribution.png'}\")\n",
    "    \n",
    "    print(f\"\\nAll verification figures saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ab2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison_charts(all_model_results: Dict, \n",
    "                                comparison_summary: pd.DataFrame,\n",
    "                                save_dir: Path):\n",
    "    \"\"\"Create comprehensive comparison visualizations\"\"\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Bar chart: Rank-1 across models & galleries\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    pivot = comparison_summary.pivot_table(\n",
    "        values='Rank-1', \n",
    "        index='Model', \n",
    "        columns='Gallery',\n",
    "        aggfunc='max'\n",
    "    )\n",
    "    \n",
    "    pivot.plot(kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_ylabel('Rank-1 Accuracy', fontsize=12)\n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_title('Rank-1 Accuracy Comparison Across Models and Galleries', fontsize=14, fontweight='bold')\n",
    "    ax.legend(title='Gallery Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'comparison_rank1_bar.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. ROC curves overlaid (if verification available)\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(all_model_results)))\n",
    "    has_verification = False\n",
    "    \n",
    "    for (model_name, model_data), color in zip(all_model_results.items(), colors):\n",
    "        try:\n",
    "            # Use fewshot_augmented + mean as reference\n",
    "            combined = model_data['basic_probe']['fewshot_augmented']['mean']\n",
    "            ver_results = combined.get('verification')\n",
    "            \n",
    "            if ver_results is None:\n",
    "                continue\n",
    "            \n",
    "            has_verification = True\n",
    "            ax.plot(ver_results['fpr'], ver_results['tpr'], \n",
    "                   label=f\"{model_name} (AUC={ver_results['roc_auc']:.3f})\",\n",
    "                   linewidth=2, color=color)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not plot ROC for {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if has_verification:\n",
    "        ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "        ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "        ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "        ax.set_title('ROC Curve Comparison (Fewshot Augmented + Mean)', fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / 'comparison_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"ROC comparison saved: {save_dir / 'comparison_roc_curves.png'}\")\n",
    "    else:\n",
    "        print(\"Warning: No verification data available for ROC comparison\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Heatmap: Models vs Aggregation methods\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    pivot_agg = comparison_summary[comparison_summary['Gallery'] == 'fewshot_augmented'].pivot(\n",
    "        index='Model',\n",
    "        columns='Aggregation',\n",
    "        values='Rank-1'\n",
    "    )\n",
    "    \n",
    "    if not pivot_agg.empty:\n",
    "        sns.heatmap(pivot_agg, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                    vmin=0.0, vmax=1.0, ax=ax, cbar_kws={'label': 'Rank-1 Accuracy'})\n",
    "        ax.set_title('Rank-1 Accuracy: Models vs Aggregation Methods (Fewshot Augmented)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / 'comparison_aggregation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Aggregation heatmap saved: {save_dir / 'comparison_aggregation_heatmap.png'}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Score distributions (if verification available)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, model_name in enumerate(list(all_model_results.keys())[:4]):\n",
    "        try:\n",
    "            combined = all_model_results[model_name]['basic_probe']['fewshot_augmented']['mean']\n",
    "            ver_results = combined.get('verification')\n",
    "            \n",
    "            if ver_results is None:\n",
    "                axes[idx].text(0.5, 0.5, f'{model_name}\\n(No verification data)', \n",
    "                              ha='center', va='center', transform=axes[idx].transAxes)\n",
    "                axes[idx].set_title(f'{model_name}', fontweight='bold', fontsize=12)\n",
    "                continue\n",
    "            \n",
    "            genuine = ver_results['genuine_scores']\n",
    "            impostor = ver_results['impostor_scores']\n",
    "            \n",
    "            # Create overlaid histograms\n",
    "            axes[idx].hist(genuine, bins=40, alpha=0.6, label='Genuine', \n",
    "                        color='green', density=True, edgecolor='black')\n",
    "            axes[idx].hist(impostor, bins=40, alpha=0.6, label='Impostor', \n",
    "                        color='red', density=True, edgecolor='black')\n",
    "            \n",
    "            # Add mean lines\n",
    "            axes[idx].axvline(np.mean(genuine), color='darkgreen', \n",
    "                            linestyle='--', linewidth=2, label=f'Genuine ={np.mean(genuine):.3f}')\n",
    "            axes[idx].axvline(np.mean(impostor), color='darkred', \n",
    "                            linestyle='--', linewidth=2, label=f'Impostor ={np.mean(impostor):.3f}')\n",
    "            \n",
    "            # Add d-prime annotation\n",
    "            dprime = ver_results.get('dprime', 0)\n",
    "            axes[idx].text(0.05, 0.95, f\"d'={dprime:.3f}\\nSep={np.mean(genuine)-np.mean(impostor):.3f}\",\n",
    "                        transform=axes[idx].transAxes, fontsize=10, verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            axes[idx].set_title(f'{model_name}', fontweight='bold', fontsize=12)\n",
    "            axes[idx].set_xlabel('Similarity Score')\n",
    "            axes[idx].set_ylabel('Density')\n",
    "            axes[idx].legend(loc='upper right', fontsize=9)\n",
    "            axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not plot score distribution for {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    plt.suptitle('Score Distribution Comparison (Normalized)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'comparison_score_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Score distributions saved: {save_dir / 'comparison_score_distributions.png'}\")\n",
    "        \n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ea958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segmented_heatmap(segmented_table: pd.DataFrame, \n",
    "                          save_path: Path,\n",
    "                          title: str = \"Segmented Performance\"):\n",
    "    \"\"\"Create heatmap for segmented evaluation results with logical grouping\"\"\"\n",
    "    \n",
    "    # Drop summary columns for heatmap\n",
    "    plot_data = segmented_table.drop(['Mean', 'Std', 'Min', 'Max'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Define segment order with logical grouping\n",
    "    segment_categories = {\n",
    "        'Quality': ['high_quality', 'low_quality'],\n",
    "        'Face Size': ['face_large', 'face_medium', 'face_small'],\n",
    "        'Pose': ['pose_easy', 'pose_medium', 'pose_hard'],\n",
    "        'Blur': ['blur_sharp', 'blur_blurry']\n",
    "    }\n",
    "    \n",
    "    # Build ordered segment list from available columns\n",
    "    ordered_segments = []\n",
    "    for category, segments in segment_categories.items():\n",
    "        for seg in segments:\n",
    "            if seg in plot_data.columns:\n",
    "                ordered_segments.append(seg)\n",
    "    \n",
    "    # Reorder columns\n",
    "    plot_data = plot_data[ordered_segments]\n",
    "    \n",
    "    # Create figure with more height for better readability\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(plot_data, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                vmin=0.0, vmax=1.0, ax=ax, \n",
    "                cbar_kws={'label': 'Rank-1 Accuracy'},\n",
    "                linewidths=0.5, linecolor='gray')\n",
    "    \n",
    "    # Add category separators\n",
    "    category_positions = [0]  # Start position\n",
    "    current_pos = 0\n",
    "    for category, segments in segment_categories.items():\n",
    "        available = [s for s in segments if s in ordered_segments]\n",
    "        current_pos += len(available)\n",
    "        if current_pos < len(ordered_segments):\n",
    "            category_positions.append(current_pos)\n",
    "            # Draw vertical line separator\n",
    "            ax.axvline(x=current_pos, color='black', linewidth=2.5, zorder=10)\n",
    "    \n",
    "    # Add category labels at the top\n",
    "    current_pos = 0\n",
    "    label_y = -0.15  # Position above the heatmap\n",
    "    for category, segments in segment_categories.items():\n",
    "        available = [s for s in segments if s in ordered_segments]\n",
    "        if available:\n",
    "            category_width = len(available)\n",
    "            center_pos = current_pos + category_width / 2\n",
    "            ax.text(center_pos, label_y, category, \n",
    "                   ha='center', va='top', fontsize=11, fontweight='bold',\n",
    "                   transform=ax.get_xaxis_transform())\n",
    "            current_pos += category_width\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=30)\n",
    "    ax.set_xlabel('', fontsize=12)  # Remove xlabel, we have category labels\n",
    "    ax.set_ylabel('Model', fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Segmented heatmap saved: {save_path}\")\n",
    "    \n",
    "    # Print category summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HEATMAP CATEGORY SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    for category, segments in segment_categories.items():\n",
    "        available = [s for s in segments if s in ordered_segments]\n",
    "        if available:\n",
    "            print(f\"\\n{category}:\")\n",
    "            for seg in available:\n",
    "                mean_acc = plot_data[seg].mean()\n",
    "                print(f\"  {seg:20s}: {mean_acc:.1%} (mean)\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity_analysis(segmented_table: pd.DataFrame,\n",
    "                              save_path: Path,\n",
    "                              title: str = \"Model Sensitivity Across Segments\"):\n",
    "    \"\"\"Create individual line plots for each model showing sensitivity to different face quality conditions\"\"\"\n",
    "    \n",
    "    # Drop summary columns\n",
    "    plot_data = segmented_table.drop(['Mean', 'Std', 'Min', 'Max'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Dynamically sort segments by mean accuracy across all models (best  worst)\n",
    "    segment_means = plot_data.mean(axis=0).sort_values(ascending=False)\n",
    "    available_segments = segment_means.index.tolist()\n",
    "    \n",
    "    # Reorder columns based on sorted segments\n",
    "    plot_data = plot_data[available_segments]\n",
    "    \n",
    "    # Define colors for each model\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#06A77D']\n",
    "    markers = ['o', 's', '^', 'D']\n",
    "    \n",
    "    # Create a subplot for each model\n",
    "    num_models = len(plot_data)\n",
    "    fig, axes = plt.subplots(num_models, 1, figsize=(14, 5 * num_models))\n",
    "    \n",
    "    # Handle case of single model\n",
    "    if num_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot each model in its own subplot\n",
    "    for idx, (model_name, row) in enumerate(plot_data.iterrows()):\n",
    "        ax = axes[idx]\n",
    "        color = colors[idx % len(colors)]\n",
    "        marker = markers[idx % len(markers)]\n",
    "        \n",
    "        ax.plot(available_segments, row.values, \n",
    "                marker=marker, \n",
    "                linewidth=3, \n",
    "                markersize=10,\n",
    "                color=color,\n",
    "                markeredgecolor='white',\n",
    "                markeredgewidth=2)\n",
    "        \n",
    "        # Formatting\n",
    "        ax.set_ylabel('Rank-1 Accuracy', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f'{model_name}', fontsize=14, fontweight='bold', pad=15)\n",
    "        \n",
    "        # Grid\n",
    "        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        ax.set_axisbelow(True)\n",
    "        \n",
    "        # Y-axis limits\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))\n",
    "        \n",
    "        # X-axis\n",
    "        ax.set_xticks(range(len(available_segments)))\n",
    "        if idx == num_models - 1:  # Only show x-label on bottom plot\n",
    "            ax.set_xticklabels(available_segments, rotation=45, ha='right')\n",
    "            ax.set_xlabel('Quality Segment (Best  Worst)', fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            ax.set_xticklabels(available_segments, rotation=45, ha='right')\n",
    "        \n",
    "        # Add a subtle background gradient to show quality degradation\n",
    "        gradient = ax.imshow([[0, 1]], cmap='RdYlGn_r', aspect='auto',\n",
    "                            extent=[0, len(available_segments)-1, 0, 1.05],\n",
    "                            alpha=0.1, zorder=0)\n",
    "        \n",
    "        # Add performance stats as text box\n",
    "        best_acc = row.max()\n",
    "        worst_acc = row.min()\n",
    "        degradation = best_acc - worst_acc\n",
    "        stats_text = f'Best: {best_acc:.1%} | Worst: {worst_acc:.1%} | : {degradation:.1%}'\n",
    "        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,\n",
    "                fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold', y=0.995)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Sensitivity plot saved: {save_path}\")\n",
    "    \n",
    "    # Print numerical summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SENSITIVITY ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSegment order (best  worst by mean accuracy):\")\n",
    "    for seg in available_segments:\n",
    "        mean_acc = segment_means[seg]\n",
    "        print(f\"  {seg:20s}: {mean_acc:.1%}\")\n",
    "    \n",
    "    print(\"\\nPerformance degradation per model:\")\n",
    "    for model_name, row in plot_data.iterrows():\n",
    "        best_acc = row.max()\n",
    "        worst_acc = row.min()\n",
    "        degradation = best_acc - worst_acc\n",
    "        best_seg = row.idxmax()\n",
    "        worst_seg = row.idxmin()\n",
    "        print(f\"  {model_name:30s}: {best_acc:.1%} ({best_seg})  {worst_acc:.1%} ({worst_seg}) |  = {degradation:.1%}\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70efafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery_strategy_comparison(strategy_df: pd.DataFrame, save_path: Path):\n",
    "    \"\"\"Visualize gallery strategy analysis\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Raw scores comparison\n",
    "    ax = axes[0, 0]\n",
    "    strategy_df.set_index('Model')[['Oneshot_Base', 'Oneshot_Aug', \n",
    "                                     'Fewshot_Base', 'Fewshot_Aug']].plot(\n",
    "        kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_ylabel('Rank-1 Accuracy')\n",
    "    ax.set_title('Gallery Strategy Comparison', fontweight='bold')\n",
    "    ax.legend(title='Configuration')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 2. Augmentation improvement\n",
    "    ax = axes[0, 1]\n",
    "    strategy_df.set_index('Model')[['Aug_Improvement_Oneshot', \n",
    "                                     'Aug_Improvement_Fewshot']].plot(\n",
    "        kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_ylabel('Rank-1 Improvement')\n",
    "    ax.set_title('Augmentation Benefit', fontweight='bold')\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.legend(title='Gallery Type')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 3. Fewshot improvement\n",
    "    ax = axes[1, 0]\n",
    "    strategy_df.set_index('Model')[['Fewshot_Improvement_Base', \n",
    "                                     'Fewshot_Improvement_Aug']].plot(\n",
    "        kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_ylabel('Rank-1 Improvement')\n",
    "    ax.set_title('Fewshot vs Oneshot Benefit', fontweight='bold')\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.legend(title='Augmentation')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 4. Best configuration per model\n",
    "    ax = axes[1, 1]\n",
    "    best_configs = strategy_df.groupby('Best_Config').size()\n",
    "    best_configs.plot(kind='bar', ax=ax, color='steelblue')\n",
    "    ax.set_ylabel('Number of Models')\n",
    "    ax.set_title('Most Common Best Configuration', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.suptitle('Gallery Strategy Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Gallery strategy plot saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023bbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_comprehensive_report(all_model_results: Dict,\n",
    "                               all_summaries: Dict,\n",
    "                               save_dir: Path):\n",
    "    \"\"\"Export complete results to multiple formats\"\"\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Excel workbook with multiple sheets\n",
    "    excel_path = save_dir / 'comprehensive_report.xlsx'\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        all_summaries['comparison_summary'].to_excel(writer, sheet_name='Overall_Comparison', index=False)\n",
    "        all_summaries['gallery_strategy'].to_excel(writer, sheet_name='Gallery_Strategy', index=False)\n",
    "        all_summaries['aggregation_analysis'].to_excel(writer, sheet_name='Aggregation_Analysis', index=False)\n",
    "        all_summaries['threshold_recommendations'].to_excel(writer, sheet_name='Threshold_Recommendations', index=False)\n",
    "        \n",
    "        if 'segmented_oneshot' in all_summaries:\n",
    "            all_summaries['segmented_oneshot'].to_excel(writer, sheet_name='Segmented_Oneshot')\n",
    "        if 'segmented_fewshot' in all_summaries:\n",
    "            all_summaries['segmented_fewshot'].to_excel(writer, sheet_name='Segmented_Fewshot')\n",
    "        if 'statistical_comparison' in all_summaries:\n",
    "            all_summaries['statistical_comparison'].to_excel(writer, sheet_name='Statistical_Tests', index=False)\n",
    "    \n",
    "    print(f\"Excel report saved: {excel_path}\")\n",
    "    \n",
    "    # 2. JSON export\n",
    "    json_data = {\n",
    "        'metadata': {\n",
    "            'generated': datetime.now().isoformat(),\n",
    "            'models_evaluated': list(all_model_results.keys())\n",
    "        },\n",
    "        'summaries': {\n",
    "            key: df.to_dict(orient='records') if isinstance(df, pd.DataFrame) else df\n",
    "            for key, df in all_summaries.items()\n",
    "            if key != 'executive_summary'\n",
    "        },\n",
    "        'executive_summary': all_summaries.get('executive_summary', '')\n",
    "    }\n",
    "    \n",
    "    json_path = save_dir / 'comprehensive_report.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "    \n",
    "    print(f\"JSON report saved: {json_path}\")\n",
    "    \n",
    "    # 3. Text summary\n",
    "    txt_path = save_dir / 'executive_summary.txt'\n",
    "    with open(txt_path, 'w') as f:\n",
    "        f.write(all_summaries.get('executive_summary', ''))\n",
    "    \n",
    "    print(f\"Text summary saved: {txt_path}\")\n",
    "    \n",
    "    # 4. LaTeX tables\n",
    "    latex_path = save_dir / 'latex_tables.tex'\n",
    "    with open(latex_path, 'w') as f:\n",
    "        f.write(\"% Comparison Summary\\n\")\n",
    "        f.write(all_summaries['comparison_summary'].to_latex(index=False, float_format=\"%.4f\"))\n",
    "        f.write(\"\\n\\n% Gallery Strategy\\n\")\n",
    "        f.write(all_summaries['gallery_strategy'].to_latex(index=False, float_format=\"%.4f\"))\n",
    "    \n",
    "    print(f\"LaTeX tables saved: {latex_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1997484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_impostor_metrics(results: Dict, title: str, save_path: Path):\n",
    "    df = results['threshold_results']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(df['threshold'], df['rejection_rate'], 'g-', linewidth=2)\n",
    "    ax.set_xlabel('Threshold')\n",
    "    ax.set_ylabel('Rejection Rate')\n",
    "    ax.set_title('Impostor Rejection Rate')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(df['threshold'], df['far'], 'r-', linewidth=2)\n",
    "    ax.set_xlabel('Threshold')\n",
    "    ax.set_ylabel('False Accept Rate')\n",
    "    ax.set_title('False Accept Rate')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    ax = axes[1, 0]\n",
    "    ax.hist(results['impostor_scores'], bins=50, alpha=0.7, color='red')\n",
    "    ax.axvline(np.mean(results['impostor_scores']), color='darkred', \n",
    "              linestyle='--', linewidth=2, label='Mean')\n",
    "    ax.axvline(results['impostor_ci'][0], color='orange', \n",
    "              linestyle=':', linewidth=2, label='95% CI')\n",
    "    ax.axvline(results['impostor_ci'][1], color='orange', \n",
    "              linestyle=':', linewidth=2)\n",
    "    ax.set_xlabel('Similarity Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Impostor Score Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    best_idx = df['rejection_rate'].idxmax()\n",
    "    best_row = df.loc[best_idx]\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    IMPOSTOR REJECTION SUMMARY\n",
    "    ==========================\n",
    "    Aggregation: {results['aggregation'].upper()}\n",
    "    \n",
    "    Best Rejection: {best_row['rejection_rate']:.4f}\n",
    "    @ Threshold: {best_row['threshold']:.3f}\n",
    "    \n",
    "    FAR at best: {best_row['far']:.4f}\n",
    "    \n",
    "    Total Impostors: {best_row['n_impostors']}\n",
    "    \n",
    "    Mean Score: {np.mean(results['impostor_scores']):.4f}\n",
    "    Std Score: {np.std(results['impostor_scores']):.4f}\n",
    "    \n",
    "    95% CI: [{results['impostor_ci'][0]:.4f}, \n",
    "             {results['impostor_ci'][1]:.4f}]\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.text(0.1, 0.5, summary_text, fontsize=11, family='monospace',\n",
    "           verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic_probe_evaluation(model_name: str, embeddings: Dict, \n",
    "                               results_dir: Path, plots_dir: Path):\n",
    "    \"\"\"Run basic probe evaluation with BOTH identification AND verification\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BASIC PROBE EVALUATION: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    probe_positive = embeddings['probe_positive_unsegmented']\n",
    "    probe_negative = embeddings['probe_negative']\n",
    "\n",
    "    if probe_positive is None:\n",
    "        print(\"Missing positive probe embeddings!\")\n",
    "        return None\n",
    "    \n",
    "    if probe_negative is None:\n",
    "        print(\"Warning: Missing negative probe embeddings! Verification metrics will be skipped.\")\n",
    "\n",
    "    gallery_types = {\n",
    "        'oneshot_base': 'gallery_oneshot_base',\n",
    "        'oneshot_augmented': 'gallery_oneshot_augmented', \n",
    "        'fewshot_base': 'gallery_fewshot_base',\n",
    "        'fewshot_augmented': 'gallery_fewshot_augmented'\n",
    "    }\n",
    "        \n",
    "    thresholds = np.arange(0.2, 0.91, 0.05)\n",
    "    aggregations = ['mean']\n",
    "    \n",
    "    all_results = {}\n",
    "    per_identity_consolidated = None\n",
    "\n",
    "    for gallery_name, gallery_key in gallery_types.items():\n",
    "        gallery = embeddings.get(gallery_key)\n",
    "        \n",
    "        if gallery is None:\n",
    "            print(f\"Missing {gallery_name} gallery, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'-'*70}\")\n",
    "        print(f\"GALLERY: {gallery_name.upper()}\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        \n",
    "        gallery_results = {}\n",
    "    \n",
    "        for agg in aggregations:\n",
    "            # ============================================\n",
    "            # IDENTIFICATION EVALUATION\n",
    "            # ============================================\n",
    "            print(f\"\\n[IDENTIFICATION] Evaluating with {agg.upper()} aggregation...\")\n",
    "            id_results = evaluate_probes_comprehensive(\n",
    "                gallery, probe_positive, thresholds, aggregation=agg, k=3\n",
    "            )\n",
    "            \n",
    "            if per_identity_consolidated is None and 'per_identity' in id_results:\n",
    "                per_identity_consolidated = id_results['per_identity']\n",
    "            \n",
    "            # ============================================\n",
    "            # VERIFICATION EVALUATION\n",
    "            # ============================================\n",
    "            ver_results = None\n",
    "            if probe_negative is not None:\n",
    "                print(f\"\\n[VERIFICATION] Evaluating with {agg.upper()} aggregation...\")\n",
    "                try:\n",
    "                    ver_results = evaluate_verification_comprehensive(\n",
    "                        gallery, probe_positive, probe_negative, thresholds, aggregation=agg, k=3\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error in verification evaluation: {e}\")\n",
    "                    ver_results = None\n",
    "            \n",
    "            # Combine both results\n",
    "            combined_results = {\n",
    "                'identification': id_results,\n",
    "                'verification': ver_results  # Can be None if probe_negative missing\n",
    "            }\n",
    "            \n",
    "            # ============================================\n",
    "            # SAVE CSV RESULTS\n",
    "            # ============================================\n",
    "            csv_path_id = results_dir / model_name / f'basic_probe_{gallery_name}_{agg}_identification.csv'\n",
    "            csv_path_id.parent.mkdir(parents=True, exist_ok=True)\n",
    "            id_results['threshold_results'].to_csv(csv_path_id, index=False)\n",
    "            \n",
    "            if ver_results is not None:\n",
    "                csv_path_ver = results_dir / model_name / f'basic_probe_{gallery_name}_{agg}_verification.csv'\n",
    "                ver_results['threshold_results'].to_csv(csv_path_ver, index=False)\n",
    "\n",
    "            # ============================================\n",
    "            # GENERATE PLOTS - IDENTIFICATION\n",
    "            # ============================================\n",
    "            plot_path_id = plots_dir / model_name / f'basic_probe_{gallery_name}_{agg}_identification_core.png'\n",
    "            plot_path_id.parent.mkdir(parents=True, exist_ok=True)\n",
    "            plot_core_metrics_identification(\n",
    "                id_results, \n",
    "                f\"{model_name} - Identification - {gallery_name.upper()} ({agg.upper()})\", \n",
    "                plot_path_id\n",
    "            )\n",
    "            \n",
    "            # ============================================\n",
    "            # GENERATE PLOTS - VERIFICATION\n",
    "            # ============================================\n",
    "            if ver_results is not None:\n",
    "                plot_path_ver = plots_dir / model_name / f'basic_probe_{gallery_name}_{agg}_verification_core.png'\n",
    "                plot_core_metrics_verification(\n",
    "                    ver_results, \n",
    "                    f\"{model_name} - Verification - {gallery_name.upper()} ({agg.upper()})\", \n",
    "                    plot_path_ver\n",
    "                )\n",
    "            \n",
    "            # ============================================\n",
    "            # GENERATE PAPER FIGURES - IDENTIFICATION\n",
    "            # ============================================\n",
    "            paper_figs_dir_id = plots_dir / model_name / 'paper_figures' / f'basic_{gallery_name}_{agg}_identification'\n",
    "            plot_paper_figures_identification(\n",
    "                id_results,\n",
    "                model_name=f\"{model_name}_{gallery_name}_{agg}\",\n",
    "                save_dir=paper_figs_dir_id\n",
    "            )\n",
    "            \n",
    "            # ============================================\n",
    "            # GENERATE PAPER FIGURES - VERIFICATION\n",
    "            # ============================================\n",
    "            if ver_results is not None:\n",
    "                paper_figs_dir_ver = plots_dir / model_name / 'paper_figures' / f'basic_{gallery_name}_{agg}_verification'\n",
    "                plot_paper_figures_verification(\n",
    "                    ver_results,\n",
    "                    model_name=f\"{model_name}_{gallery_name}_{agg}\",\n",
    "                    save_dir=paper_figs_dir_ver\n",
    "                )\n",
    "                        \n",
    "            gallery_results[agg] = combined_results\n",
    "\n",
    "            # ============================================\n",
    "            # PRINT SUMMARY - IDENTIFICATION\n",
    "            # ============================================\n",
    "            print(f\"\\n  IDENTIFICATION Metrics:\")\n",
    "            print(f\"  {''*50}\")\n",
    "            df_id = id_results['threshold_results']\n",
    "            best_idx = df_id['rank1_accuracy'].idxmax()\n",
    "            print(f\"  Best Rank-1: {df_id.loc[best_idx, 'rank1_accuracy']:.4f} \"\n",
    "                f\"@ threshold {df_id.loc[best_idx, 'threshold']:.2f}\")\n",
    "            print(f\"  Rank-5: {df_id.loc[best_idx, 'rank5_accuracy']:.4f}\")\n",
    "            print(f\"  Rank-10: {df_id.loc[best_idx, 'rank10_accuracy']:.4f}\")\n",
    "            print(f\"  MRR: {df_id.loc[best_idx, 'mrr']:.4f}\")\n",
    "            \n",
    "            # ============================================\n",
    "            # PRINT SUMMARY - VERIFICATION\n",
    "            # ============================================\n",
    "            if ver_results is not None:\n",
    "                print(f\"\\n  VERIFICATION Metrics:\")\n",
    "                print(f\"  {''*50}\")\n",
    "                print(f\"  ROC-AUC: {ver_results['roc_auc']:.4f}\")\n",
    "                print(f\"  EER: {ver_results['eer']:.4f} @ threshold {ver_results['eer_threshold']:.2f}\")\n",
    "                print(f\"  TAR@FAR=0.1%: {ver_results['tar_at_far_0.001']:.4f}\")\n",
    "                print(f\"  TAR@FAR=1%: {ver_results['tar_at_far_0.01']:.4f}\")\n",
    "                print(f\"  TAR@FAR=10%: {ver_results['tar_at_far_0.1']:.4f}\")\n",
    "                print(f\"  d-prime: {ver_results['dprime']:.4f}\")\n",
    "                print(f\"  Separation: {ver_results['separation']:.4f}\")\n",
    "                \n",
    "                print(f\"\\n  Score Distribution Statistics:\")\n",
    "                print(f\"  {''*50}\")\n",
    "                print(f\"  _genuine:  {ver_results['genuine_mean']:.4f} ( = {ver_results['genuine_std']:.4f})\")\n",
    "                print(f\"  _impostor: {ver_results['impostor_mean']:.4f} ( = {ver_results['impostor_std']:.4f})\")\n",
    "                print(f\"   = {abs(ver_results['genuine_mean'] - ver_results['impostor_mean']):.4f}\")\n",
    "                print(f\"  n_genuine_pairs: {ver_results['n_genuine_pairs']}\")\n",
    "                print(f\"  n_impostor_pairs: {ver_results['n_impostor_pairs']}\")\n",
    "            else:\n",
    "                print(f\"\\n  VERIFICATION Metrics: SKIPPED (no negative probes)\")\n",
    "            \n",
    "        all_results[gallery_name] = gallery_results\n",
    "    \n",
    "    all_results['per_identity'] = per_identity_consolidated\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_impostor_evaluation(model_name: str, embeddings: Dict,\n",
    "                           results_dir: Path, plots_dir: Path):\n",
    "    \"\"\"Run impostor evaluation (ORIGINAL + ENHANCED OUTPUT)\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"IMPOSTOR EVALUATION: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    gallery = embeddings['gallery_oneshot_augmented']\n",
    "    impostor = embeddings['probe_negative']\n",
    "    \n",
    "    if gallery is None or impostor is None:\n",
    "        print(\"Missing embeddings!\")\n",
    "        return None\n",
    "    \n",
    "    thresholds = np.arange(0.2, 0.91, 0.05)\n",
    "    \n",
    "    print(\"\\nEvaluating with MEAN aggregation...\")\n",
    "    results = evaluate_impostors_comprehensive(\n",
    "        gallery, impostor, thresholds, aggregation='mean', k=3\n",
    "    )\n",
    "    \n",
    "    csv_path = results_dir / model_name / 'impostor_metrics.csv'\n",
    "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    results['threshold_results'].to_csv(csv_path, index=False)\n",
    "    \n",
    "    plot_path = plots_dir / model_name / 'impostor_plot.png'\n",
    "    plot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plot_impostor_metrics(results, f\"{model_name} - Impostor Rejection\", plot_path)\n",
    "\n",
    "    df = results['threshold_results']\n",
    "    best_idx = df['rejection_rate'].idxmax()\n",
    "    print(f\"  Best Rejection Rate: {df.loc[best_idx, 'rejection_rate']:.4f} \"\n",
    "          f\"@ threshold {df.loc[best_idx, 'threshold']:.2f}\")\n",
    "    print(f\"  FAR at best: {df.loc[best_idx, 'far']:.4f}\")\n",
    "    print(f\"  Total impostors: {df.loc[best_idx, 'n_impostors']}\")\n",
    "    print(f\"  Mean impostor score: {results['mean_impostor_score']:.4f}\")\n",
    "    print(f\"  Std impostor score: {results['std_impostor_score']:.4f}\")\n",
    "    print(f\"  95% CI: [{results['impostor_ci'][0]:.4f}, {results['impostor_ci'][1]:.4f}]\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496cc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmented_evaluation(model_name: str, embeddings: Dict,\n",
    "                             results_dir: Path, plots_dir: Path,\n",
    "                             gallery_type: str):\n",
    "    \"\"\"Run segmented evaluation with BOTH identification AND verification\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SEGMENTED EVALUATION: {model_name} ({gallery_type})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    gallery_key = f'gallery_{gallery_type}_augmented'\n",
    "    gallery = embeddings[gallery_key]\n",
    "    probe_positive = embeddings['probe_positive_segmented']\n",
    "    probe_negative = embeddings['probe_negative']\n",
    "    \n",
    "    if gallery is None or probe_positive is None:\n",
    "        print(\"Missing embeddings!\")\n",
    "        return None\n",
    "    \n",
    "    if probe_negative is None:\n",
    "        print(\"Warning: No negative probes. Verification will be skipped for segmented evaluation.\")\n",
    "    \n",
    "    thresholds = np.arange(0.2, 0.91, 0.05)\n",
    "    \n",
    "    print(\"\\nEvaluating with MEAN aggregation...\")\n",
    "    segment_results = evaluate_segmented_comprehensive(\n",
    "        gallery, probe_positive, probe_negative, thresholds, \n",
    "        aggregation='mean', k=3, include_verification=True\n",
    "    )\n",
    "    \n",
    "    for segment_name, combined_results in segment_results.items():\n",
    "        # Extract identification and verification results\n",
    "        id_results = combined_results['identification']\n",
    "        ver_results = combined_results['verification']  # Can be None\n",
    "        \n",
    "        # Save CSVs\n",
    "        csv_path_id = results_dir / model_name / f'segmented_{gallery_type}_{segment_name}_identification.csv'\n",
    "        csv_path_id.parent.mkdir(parents=True, exist_ok=True)\n",
    "        id_results['threshold_results'].to_csv(csv_path_id, index=False)\n",
    "        \n",
    "        if ver_results is not None:\n",
    "            csv_path_ver = results_dir / model_name / f'segmented_{gallery_type}_{segment_name}_verification.csv'\n",
    "            ver_results['threshold_results'].to_csv(csv_path_ver, index=False)\n",
    "\n",
    "        # Plots - Identification\n",
    "        plot_path_id = plots_dir / model_name / f'segmented_{gallery_type}_{segment_name}_identification_core.png'\n",
    "        plot_path_id.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plot_core_metrics_identification(\n",
    "            id_results, \n",
    "            f\"{model_name} - {segment_name} - ID ({gallery_type})\", \n",
    "            plot_path_id\n",
    "        )\n",
    "        \n",
    "        # Plots - Verification\n",
    "        if ver_results is not None:\n",
    "            plot_path_ver = plots_dir / model_name / f'segmented_{gallery_type}_{segment_name}_verification_core.png'\n",
    "            plot_core_metrics_verification(\n",
    "                ver_results, \n",
    "                f\"{model_name} - {segment_name} - Ver ({gallery_type})\", \n",
    "                plot_path_ver\n",
    "            )\n",
    "        \n",
    "        # Paper figures\n",
    "        paper_figs_dir_id = plots_dir / model_name / 'paper_figures' / f'segmented_{gallery_type}_{segment_name}_identification'\n",
    "        plot_paper_figures_identification(\n",
    "            id_results,\n",
    "            model_name=f\"{model_name}_{segment_name}\",\n",
    "            save_dir=paper_figs_dir_id\n",
    "        )\n",
    "        \n",
    "        if ver_results is not None:\n",
    "            paper_figs_dir_ver = plots_dir / model_name / 'paper_figures' / f'segmented_{gallery_type}_{segment_name}_verification'\n",
    "            plot_paper_figures_verification(\n",
    "                ver_results,\n",
    "                model_name=f\"{model_name}_{segment_name}\",\n",
    "                save_dir=paper_figs_dir_ver\n",
    "            )\n",
    "        \n",
    "        # Print summaries\n",
    "        print(f\"\\n  {segment_name}:\")\n",
    "        print(f\"  {''*50}\")\n",
    "        print(f\"  IDENTIFICATION:\")\n",
    "        df_id = id_results['threshold_results']\n",
    "        best_idx = df_id['rank1_accuracy'].idxmax()\n",
    "        print(f\"    Rank-1: {df_id.loc[best_idx, 'rank1_accuracy']:.4f} @ threshold {df_id.loc[best_idx, 'threshold']:.2f}\")\n",
    "        print(f\"    Rank-5: {df_id.loc[best_idx, 'rank5_accuracy']:.4f}\")\n",
    "        print(f\"    Rank-10: {df_id.loc[best_idx, 'rank10_accuracy']:.4f}\")\n",
    "        print(f\"    MRR: {df_id.loc[best_idx, 'mrr']:.4f}\")\n",
    "        \n",
    "        if ver_results is not None:\n",
    "            print(f\"\\n  VERIFICATION:\")\n",
    "            print(f\"    ROC-AUC: {ver_results['roc_auc']:.4f}\")\n",
    "            print(f\"    EER: {ver_results['eer']:.4f} @ threshold {ver_results['eer_threshold']:.2f}\")\n",
    "            print(f\"    TAR@FAR=0.1%: {ver_results['tar_at_far_0.001']:.4f}\")\n",
    "            print(f\"    d-prime: {ver_results['dprime']:.4f}\")\n",
    "            print(f\"    Separation: {ver_results['separation']:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n  VERIFICATION: SKIPPED\")\n",
    "    \n",
    "    return segment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rank1_per_identity_all_models(all_model_results: Dict[str, Dict],\n",
    "                                       save_path: Path,\n",
    "                                       sort_by_average: bool = True):\n",
    "    \"\"\"\n",
    "    Plot Rank-1 per identity for all models in a single chart.\n",
    "    Expects each model to contain:\n",
    "        results[\"basic_probe\"][\"per_identity\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Gather all identities across all models ---\n",
    "    identities = set()\n",
    "    for model_name, results in all_model_results.items():\n",
    "        per_id = results[\"basic_probe\"].get(\"per_identity\", {})\n",
    "        identities.update(per_id.keys())\n",
    "    identities = list(identities)\n",
    "\n",
    "    # --- Build table: identity -> {model: rank1} ---\n",
    "    rank1_table = {idn: {} for idn in identities}\n",
    "    model_names = list(all_model_results.keys())\n",
    "\n",
    "    for model_name, results in all_model_results.items():\n",
    "        per_id = results[\"basic_probe\"].get(\"per_identity\", {})\n",
    "        for idn in identities:\n",
    "            rank1_table[idn][model_name] = per_id.get(idn, {}).get(\"rank1\", 0)\n",
    "\n",
    "    # --- Sort identities (optional but recommended) ---\n",
    "    if sort_by_average:\n",
    "        identities.sort(\n",
    "            key=lambda idn: np.mean(list(rank1_table[idn].values())),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "    # --- Plot ---\n",
    "    x = np.arange(len(identities))\n",
    "    fig, ax = plt.subplots(figsize=(22, 8))\n",
    "\n",
    "    for model_name in model_names:\n",
    "        y = [rank1_table[idn][model_name] for idn in identities]\n",
    "        ax.plot(x, y, marker='o', linewidth=2, label=model_name)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(identities, rotation=90)\n",
    "    ax.set_ylabel(\"Rank-1 Accuracy\")\n",
    "    ax.set_title(\"Rank-1 Accuracy per Identity Across Models\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(save_path, dpi=300)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_evaluation_pipeline(all_embeddings: Dict, output_base_dir: Path):\n",
    "    \"\"\"\n",
    "    Complete evaluation pipeline with all analysis and comparisons\n",
    "    \n",
    "    Args:\n",
    "        all_embeddings: Dict with structure {model_name: embeddings_dict}\n",
    "        output_base_dir: Base directory for all outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE FACE RECOGNITION EVALUATION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results_dir = output_base_dir / 'evaluation_results'\n",
    "    plots_dir = output_base_dir / 'plots'\n",
    "    comparison_dir = output_base_dir / 'comparisons'\n",
    "\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "    comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (comparison_dir / \"charts\").mkdir(parents=True, exist_ok=True)\n",
    "    (comparison_dir / \"reports\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    all_model_results = {}\n",
    "    \n",
    "    # Run individual model evaluations\n",
    "    for model_name, embeddings in all_embeddings.items():\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"# PROCESSING MODEL: {model_name}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        model_results = {}\n",
    "        \n",
    "        # 1. Basic probe evaluation\n",
    "        model_results['basic_probe'] = run_basic_probe_evaluation(\n",
    "            model_name, embeddings, results_dir, plots_dir\n",
    "        )\n",
    "        \n",
    "        # 2. Impostor evaluation\n",
    "        model_results['impostor'] = run_impostor_evaluation(\n",
    "            model_name, embeddings, results_dir, plots_dir\n",
    "        )\n",
    "        \n",
    "        # 3. Segmented evaluation - oneshot\n",
    "        model_results['segmented_oneshot'] = run_segmented_evaluation(\n",
    "            model_name, embeddings, results_dir, plots_dir, 'oneshot'\n",
    "        )\n",
    "        \n",
    "        # 4. Segmented evaluation - fewshot\n",
    "        model_results['segmented_fewshot'] = run_segmented_evaluation(\n",
    "            model_name, embeddings, results_dir, plots_dir, 'fewshot'\n",
    "        )\n",
    "        \n",
    "        all_model_results[model_name] = model_results\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DISPLAY KEY METRICS SUMMARY\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(\"# KEY METRICS SUMMARY\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "    \n",
    "    for model_name, model_results in all_model_results.items():\n",
    "        print(f\"\\n{model_name.upper()}\")\n",
    "        print(f\"{''*70}\")\n",
    "        \n",
    "        # Extract metrics from basic_probe oneshot_base mean (most common baseline)\n",
    "        # Extract metrics from basic_probe oneshot_base mean\n",
    "        if (model_results.get('basic_probe') and \n",
    "            'oneshot_base' in model_results['basic_probe'] and\n",
    "            'mean' in model_results['basic_probe']['oneshot_base']):\n",
    "            \n",
    "            combined = model_results['basic_probe']['oneshot_base']['mean']\n",
    "            ver_results = combined.get('verification')  # Can be None\n",
    "            \n",
    "            if ver_results is not None:\n",
    "                print(f\"  AUC:               {ver_results['roc_auc']:.4f}\")\n",
    "                print(f\"  EER:               {ver_results['eer']*100:.2f}%\")\n",
    "                print(f\"  EER Threshold:     {ver_results['eer_threshold']:.4f}\")\n",
    "                print(f\"  TAR @ 0.1% FAR:    {ver_results['tar_at_far_0.001']*100:.2f}%\")\n",
    "                print(f\"  TAR @ 1% FAR:      {ver_results['tar_at_far_0.01']*100:.2f}%\")\n",
    "                print(f\"  TAR @ 10% FAR:     {ver_results['tar_at_far_0.1']*100:.2f}%\")\n",
    "                print(f\"  Separation:        {ver_results['separation']:.4f}\")\n",
    "                print(f\"  d-prime:           {ver_results['dprime']:.4f}\")\n",
    "                print(f\"  _genuine:         {ver_results['genuine_mean']:.4f} (={ver_results['genuine_std']:.4f})\")\n",
    "                print(f\"  _impostor:        {ver_results['impostor_mean']:.4f} (={ver_results['impostor_std']:.4f})\")\n",
    "            else:\n",
    "                print(f\"  Verification metrics: NOT AVAILABLE (missing negative probes)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # COMPARATIVE ANALYSIS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(\"# COMPARATIVE ANALYSIS\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    all_summaries = {}\n",
    "    \n",
    "    # 1. Generate comparison summary\n",
    "    print(\"\\n1. Generating comparison summary...\")\n",
    "    all_summaries['comparison_summary'] = generate_comparison_summary(all_model_results)\n",
    "    all_summaries['comparison_summary'].to_csv(comparison_dir / 'comparison_summary.csv', index=False)\n",
    "    print(f\"   Saved: {comparison_dir / 'comparison_summary.csv'}\")\n",
    "    \n",
    "    # 2. Gallery strategy analysis\n",
    "    print(\"\\n2. Analyzing gallery strategies...\")\n",
    "    all_summaries['gallery_strategy'] = analyze_gallery_strategies(all_model_results)\n",
    "    all_summaries['gallery_strategy'].to_csv(comparison_dir / 'gallery_strategy_analysis.csv', index=False)\n",
    "    print(f\"   Saved: {comparison_dir / 'gallery_strategy_analysis.csv'}\")\n",
    "    \n",
    "    # 3. Aggregation method analysis\n",
    "    print(\"\\n3. Analyzing aggregation methods...\")\n",
    "    all_summaries['aggregation_analysis'] = summarize_aggregation_performance(all_model_results)\n",
    "    all_summaries['aggregation_analysis'].to_csv(comparison_dir / 'aggregation_analysis.csv', index=False)\n",
    "    print(f\"   Saved: {comparison_dir / 'aggregation_analysis.csv'}\")\n",
    "    \n",
    "    # 4. Threshold recommendations\n",
    "    print(\"\\n4. Generating threshold recommendations...\")\n",
    "    all_summaries['threshold_recommendations'] = recommend_operating_thresholds(all_model_results)\n",
    "    all_summaries['threshold_recommendations'].to_csv(comparison_dir / 'threshold_recommendations.csv', index=False)\n",
    "    print(f\"   Saved: {comparison_dir / 'threshold_recommendations.csv'}\")\n",
    "    \n",
    "    # 5. Segmented comparison tables - IDENTIFICATION\n",
    "    print(\"\\n5a. Creating segmented IDENTIFICATION comparison tables...\")\n",
    "    all_summaries['segmented_oneshot_identification'] = create_segmented_comparison_table(\n",
    "        all_model_results, 'oneshot', 'identification')\n",
    "    all_summaries['segmented_oneshot_identification'].to_csv(\n",
    "        comparison_dir / 'segmented_oneshot_identification.csv')\n",
    "\n",
    "    all_summaries['segmented_fewshot_identification'] = create_segmented_comparison_table(\n",
    "        all_model_results, 'fewshot', 'identification')\n",
    "    all_summaries['segmented_fewshot_identification'].to_csv(\n",
    "        comparison_dir / 'segmented_fewshot_identification.csv')\n",
    "\n",
    "    # 5b. Segmented comparison tables - VERIFICATION\n",
    "    print(\"\\n5b. Creating segmented VERIFICATION comparison tables...\")\n",
    "    all_summaries['segmented_oneshot_verification'] = create_segmented_comparison_table(\n",
    "        all_model_results, 'oneshot', 'verification')\n",
    "    all_summaries['segmented_oneshot_verification'].to_csv(\n",
    "        comparison_dir / 'segmented_oneshot_verification.csv')\n",
    "\n",
    "    all_summaries['segmented_fewshot_verification'] = create_segmented_comparison_table(\n",
    "        all_model_results, 'fewshot', 'verification')\n",
    "    all_summaries['segmented_fewshot_verification'].to_csv(\n",
    "        comparison_dir / 'segmented_fewshot_verification.csv')\n",
    "    \n",
    "    # 6. Failure analysis\n",
    "    print(\"\\n6. Analyzing failure cases...\")\n",
    "    all_summaries['failure_analysis'] = analyze_failure_cases(all_model_results)\n",
    "    with open(comparison_dir / 'failure_analysis.json', 'w') as f:\n",
    "        json.dump(all_summaries['failure_analysis'], f, indent=2)\n",
    "    print(f\"   Saved: {comparison_dir / 'failure_analysis.json'}\")\n",
    "    \n",
    "    # 7. Statistical comparison\n",
    "    print(\"\\n7. Performing statistical comparisons...\")\n",
    "    all_summaries['statistical_comparison'] = compare_models_statistical(all_model_results)\n",
    "    all_summaries['statistical_comparison'].to_csv(comparison_dir / 'statistical_comparison.csv', index=False)\n",
    "    print(f\"   Saved: {comparison_dir / 'statistical_comparison.csv'}\")\n",
    "    \n",
    "    # 8. Executive summary\n",
    "    print(\"\\n8. Generating executive summary...\")\n",
    "    all_summaries['executive_summary'] = generate_executive_summary(\n",
    "        all_model_results, \n",
    "        all_summaries['comparison_summary']\n",
    "    )\n",
    "    print(all_summaries['executive_summary'])\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VISUALIZATIONS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(\"# GENERATING COMPARISON VISUALIZATIONS\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    # 1. Model comparison charts\n",
    "    print(\"\\n1. Creating model comparison charts...\")\n",
    "    plot_model_comparison_charts(\n",
    "        all_model_results,\n",
    "        all_summaries['comparison_summary'],\n",
    "        comparison_dir / 'charts'\n",
    "    )\n",
    "    print(\"\\n1b. Creating Rank-1 per-identity comparison chart...\")\n",
    "    plot_rank1_per_identity_all_models(\n",
    "        all_model_results,\n",
    "        comparison_dir / \"charts\" / \"rank1_per_identity_all_models.png\"\n",
    "    )\n",
    "    print(f\"   Saved: {comparison_dir / 'charts' / 'rank1_per_identity_all_models.png'}\")\n",
    "    \n",
    "    # 2. Segmented heatmaps - IDENTIFICATION\n",
    "    print(\"\\n2a. Creating segmented IDENTIFICATION heatmaps...\")\n",
    "    if 'segmented_oneshot_identification' in all_summaries:\n",
    "        plot_segmented_heatmap(\n",
    "            all_summaries['segmented_oneshot_identification'],  #  CORRECT\n",
    "            comparison_dir / 'charts' / 'segmented_oneshot_identification_heatmap.png',\n",
    "            'Segmented Identification Performance - Oneshot'\n",
    "        )\n",
    "        \n",
    "    if 'segmented_fewshot_identification' in all_summaries:\n",
    "        plot_segmented_heatmap(\n",
    "            all_summaries['segmented_fewshot_identification'],  #  CORRECT\n",
    "            comparison_dir / 'charts' / 'segmented_fewshot_identification_heatmap.png',\n",
    "            'Segmented Identification Performance - Fewshot'\n",
    "        )\n",
    "\n",
    "    # 2b. Segmented heatmaps - VERIFICATION\n",
    "    print(\"\\n2b. Creating segmented VERIFICATION heatmaps...\")\n",
    "    if 'segmented_oneshot_verification' in all_summaries:\n",
    "        plot_segmented_heatmap(\n",
    "            all_summaries['segmented_oneshot_verification'],  #  CORRECT\n",
    "            comparison_dir / 'charts' / 'segmented_oneshot_verification_heatmap.png',\n",
    "            'Segmented Verification Performance - Oneshot'\n",
    "        )\n",
    "\n",
    "    if 'segmented_fewshot_verification' in all_summaries:\n",
    "        plot_segmented_heatmap(\n",
    "            all_summaries['segmented_fewshot_verification'],  #  CORRECT\n",
    "            comparison_dir / 'charts' / 'segmented_fewshot_verification_heatmap.png',\n",
    "            'Segmented Verification Performance - Fewshot'\n",
    "        )\n",
    "\n",
    "    # 3. Sensitivity analysis - IDENTIFICATION\n",
    "    print(\"\\n3a. Creating sensitivity analysis plots - IDENTIFICATION...\")\n",
    "    if 'segmented_oneshot_identification' in all_summaries:\n",
    "        plot_sensitivity_analysis(\n",
    "            all_summaries['segmented_oneshot_identification'],  #  CORRECT\n",
    "            comparison_dir / 'charts' / 'sensitivity_analysis_oneshot_identification.png',\n",
    "            'Sensitivity Analysis - Oneshot Identification'\n",
    "        )\n",
    "\n",
    "    if 'segmented_fewshot_identification' in all_summaries:\n",
    "        plot_sensitivity_analysis(\n",
    "            all_summaries['segmented_fewshot_identification'],  #  CORRECT\n",
    "            comparison_dir / 'charts' / 'sensitivity_analysis_fewshot_identification.png',\n",
    "            'Sensitivity Analysis - Fewshot Identification'\n",
    "        )\n",
    "\n",
    "    # 3b. Sensitivity analysis - VERIFICATION\n",
    "    print(\"\\n3b. Creating sensitivity analysis plots - VERIFICATION...\")\n",
    "    if 'segmented_oneshot_verification' in all_summaries:\n",
    "        plot_sensitivity_analysis(\n",
    "            all_summaries['segmented_oneshot_verification'],  #  CORRECT\n",
    "            comparison_dir / 'charts' / 'sensitivity_analysis_oneshot_verification.png',\n",
    "            'Sensitivity Analysis - Oneshot Verification'\n",
    "        )\n",
    "\n",
    "    if 'segmented_fewshot_verification' in all_summaries:\n",
    "        plot_sensitivity_analysis(\n",
    "            all_summaries['segmented_fewshot_verification'],  #  CORRECT\n",
    "            comparison_dir / 'charts' / 'sensitivity_analysis_fewshot_verification.png',\n",
    "            'Sensitivity Analysis - Fewshot Verification'\n",
    "        )\n",
    "    # 3. Gallery strategy visualization\n",
    "    print(\"\\n3. Creating gallery strategy visualizations...\")\n",
    "    plot_gallery_strategy_comparison(\n",
    "        all_summaries['gallery_strategy'],\n",
    "        comparison_dir / 'charts' / 'gallery_strategy_comparison.png'\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXPORT REPORTS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(\"# EXPORTING COMPREHENSIVE REPORTS\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    export_comprehensive_report(\n",
    "        all_model_results,\n",
    "        all_summaries,\n",
    "        comparison_dir / 'reports'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION PIPELINE COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nAll results saved to: {output_base_dir}\")\n",
    "    print(f\"  - Individual results: {results_dir}\")\n",
    "    print(f\"  - Individual plots: {plots_dir}\")\n",
    "    print(f\"  - Comparisons: {comparison_dir}\")\n",
    "    print(f\"  - Reports: {comparison_dir / 'reports'}\")\n",
    "    \n",
    "    return all_model_results, all_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(path: Path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def load_all_embeddings(base_dir: Path):\n",
    "    \"\"\"\n",
    "    Auto-loads all embeddings following this structure:\n",
    "\n",
    "    base_dir /\n",
    "        adaface_ir_101 /\n",
    "            gallery_few-shot_augmented.pkl\n",
    "            gallery_few-shot_base.pkl\n",
    "            ...\n",
    "        arcface_ir_50 /\n",
    "            ...\n",
    "\n",
    "    Returns: dict formatted exactly for run_complete_evaluation_pipeline()\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"gallery_one-shot_base.pkl\": \"gallery_oneshot_base\",\n",
    "        \"gallery_one-shot_augmented.pkl\": \"gallery_oneshot_augmented\",\n",
    "        \"gallery_few-shot_base.pkl\": \"gallery_fewshot_base\",\n",
    "        \"gallery_few-shot_augmented.pkl\": \"gallery_fewshot_augmented\",\n",
    "        \"probe_negative.pkl\": \"probe_negative\",\n",
    "        \"probe_positive_segmented.pkl\": \"probe_positive_segmented\",\n",
    "        \"probe_positive_unsegmented.pkl\": \"probe_positive_unsegmented\",\n",
    "    }\n",
    "\n",
    "    all_embeddings = {}\n",
    "\n",
    "    for model_dir in base_dir.iterdir():\n",
    "        if not model_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        model_name = model_dir.name\n",
    "        all_embeddings[model_name] = {}\n",
    "\n",
    "        for file in model_dir.iterdir():\n",
    "            if file.suffix != \".pkl\":\n",
    "                continue\n",
    "\n",
    "            key = mapping.get(file.name)\n",
    "            if key is None:\n",
    "                print(f\"Warning: Unrecognized file {file.name}, skipping\")\n",
    "                continue\n",
    "\n",
    "            all_embeddings[model_name][key] = load_pkl(file)\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470219b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = {}\n",
    "\n",
    "for model in models:\n",
    "    model_dir = embeddings_root / model\n",
    "    if not model_dir.exists():\n",
    "        print(f\"Warning: {model_dir} not found, skipping...\")\n",
    "        continue\n",
    "\n",
    "    all_embeddings[model] = {}\n",
    "\n",
    "    for file in model_dir.glob(\"*.pkl\"):\n",
    "        fname = file.name\n",
    "\n",
    "        # Map filename  dictionary key\n",
    "        if \"one-shot_base\" in fname:\n",
    "            key = \"gallery_oneshot_base\"\n",
    "        elif \"one-shot_augmented\" in fname:\n",
    "            key = \"gallery_oneshot_augmented\"\n",
    "        elif \"few-shot_base\" in fname:\n",
    "            key = \"gallery_fewshot_base\"\n",
    "        elif \"few-shot_augmented\" in fname:\n",
    "            key = \"gallery_fewshot_augmented\"\n",
    "        else:\n",
    "            # probe_negative, probe_positive_segmented, etc.\n",
    "            key = fname.replace(\".pkl\", \"\")\n",
    "\n",
    "        with open(file, \"rb\") as f:\n",
    "            all_embeddings[model][key] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1befcc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results, all_summaries = run_complete_evaluation_pipeline(\n",
    "    all_embeddings,\n",
    "    output_root\n",
    ")\n",
    "\n",
    "print(all_summaries['executive_summary'])\n",
    "print(all_summaries['comparison_summary'])\n",
    "print(all_summaries['gallery_strategy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
