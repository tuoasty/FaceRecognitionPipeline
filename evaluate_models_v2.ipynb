{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d78e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddce237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: d:\\KEVIN\\0SLC\\RIG\n",
      "Output Root: d:\\KEVIN\\0SLC\\RIG\\output\\v1\n"
     ]
    }
   ],
   "source": [
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "def find_project_root(start: Path, target_folder=\"RIG\"):\n",
    "  for parent in [start] + list(start.parents):\n",
    "    if parent.name == target_folder:\n",
    "      return parent\n",
    "  raise RuntimeError(f\"Could not find project root '{target_folder}'\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(NOTEBOOK_DIR)\n",
    "OUTPUT_ROOT = PROJECT_ROOT / \"output\" / \"v1\"\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Output Root: {OUTPUT_ROOT}\")\n",
    "\n",
    "output_root = Path(OUTPUT_ROOT)\n",
    "embeddings_root = output_root / 'embeddings'\n",
    "results_dir = output_root / 'evaluation_results'\n",
    "plots_dir = results_dir / 'plots'\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "models = [\n",
    "  'adaface_ir_50',\n",
    "  'adaface_ir_101',\n",
    "  'arcface_ir_50',\n",
    "  'arcface_ir_101'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f0249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model_name: str) -> Dict:\n",
    "  model_dir = embeddings_root / model_name\n",
    "  \n",
    "  if not model_dir.exists():\n",
    "    raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n",
    "  \n",
    "  embeddings = {}\n",
    "\n",
    "  embedding_files = {\n",
    "    'gallery_oneshot_base': 'gallery_one-shot_base.pkl',\n",
    "    'gallery_oneshot_augmented': 'gallery_one-shot_augmented.pkl',\n",
    "    'gallery_fewshot_base': 'gallery_few-shot_base.pkl',\n",
    "    'gallery_fewshot_augmented': 'gallery_few-shot_augmented.pkl',\n",
    "    'probe_positive_unsegmented': 'probe_positive_unsegmented.pkl',\n",
    "    'probe_positive_segmented': 'probe_positive_segmented.pkl',\n",
    "    'probe_negative': 'probe_negative.pkl'\n",
    "  }\n",
    "  \n",
    "  for key, filename in embedding_files.items():\n",
    "    file_path = model_dir / filename\n",
    "    if file_path.exists():\n",
    "      with open(file_path, 'rb') as f:\n",
    "        embeddings[key] = pickle.load(f)\n",
    "    else:\n",
    "        embeddings[key] = None\n",
    "  \n",
    "  return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2dc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "    norm1 = np.linalg.norm(emb1)\n",
    "    norm2 = np.linalg.norm(emb2)\n",
    "    if abs(norm1 - 1.0) < 0.01 and abs(norm2 - 1.0) < 0.01:\n",
    "        return np.dot(emb1, emb2)\n",
    "    return np.dot(emb1, emb2) / (norm1 * norm2)\n",
    "\n",
    "def compute_all_similarities(probe_emb: np.ndarray, \n",
    "                            gallery_embeddings: Dict[str, Dict]) -> List[Tuple[str, float]]:\n",
    "    similarities = []\n",
    "    for name, data in gallery_embeddings.items():\n",
    "        gallery_embs = data['embeddings']\n",
    "        for gallery_emb in gallery_embs:\n",
    "            sim = cosine_similarity(probe_emb, gallery_emb)\n",
    "            similarities.append((name, sim))\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0706e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_max(similarities: List[float]) -> float:\n",
    "    return max(similarities) if similarities else -1\n",
    "\n",
    "def aggregate_mean(similarities: List[float]) -> float:\n",
    "    return np.mean(similarities) if similarities else -1\n",
    "\n",
    "def aggregate_topk(similarities: List[float], k: int = 3) -> float:\n",
    "    if not similarities:\n",
    "        return -1\n",
    "    sorted_sims = sorted(similarities, reverse=True)\n",
    "    return np.mean(sorted_sims[:min(k, len(sorted_sims))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e39f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_probe(probe_embedding: np.ndarray, \n",
    "                   gallery_embeddings: Dict[str, Dict],\n",
    "                   threshold: float,\n",
    "                   aggregation: str = 'mean',\n",
    "                   k: int = 3) -> Tuple[Optional[str], float, Dict[str, float]]:\n",
    "    identity_scores = {}\n",
    "    \n",
    "    for name, data in gallery_embeddings.items():\n",
    "        gallery_embs = data['embeddings']\n",
    "        similarities = [cosine_similarity(probe_embedding, g_emb) for g_emb in gallery_embs]\n",
    "        \n",
    "        if aggregation == 'max':\n",
    "            score = aggregate_max(similarities)\n",
    "        elif aggregation == 'mean':\n",
    "            score = aggregate_mean(similarities)\n",
    "        elif aggregation == 'topk':\n",
    "            score = aggregate_topk(similarities, k)\n",
    "        else:\n",
    "            score = aggregate_max(similarities)\n",
    "        \n",
    "        identity_scores[name] = score\n",
    "    \n",
    "    if not identity_scores:\n",
    "        return None, -1, {}\n",
    "    \n",
    "    sorted_identities = sorted(identity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    best_name, best_score = sorted_identities[0]\n",
    "    \n",
    "    if best_score < threshold:\n",
    "        return None, best_score, identity_scores\n",
    "    \n",
    "    return best_name, best_score, identity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2584d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rank_metrics(identity_scores: Dict[str, float], \n",
    "                        true_identity: str,\n",
    "                        ranks: List[int] = [1, 5, 10]) -> Dict[str, bool]:\n",
    "    sorted_identities = sorted(identity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    rank_results = {}\n",
    "    for k in ranks:\n",
    "        top_k = [name for name, _ in sorted_identities[:k]]\n",
    "        rank_results[f'rank{k}'] = true_identity in top_k\n",
    "\n",
    "    try:\n",
    "        true_rank = [name for name, _ in sorted_identities].index(true_identity) + 1\n",
    "        rank_results['reciprocal_rank'] = 1.0 / true_rank\n",
    "    except ValueError:\n",
    "        rank_results['reciprocal_rank'] = 0.0\n",
    "    \n",
    "    return rank_results\n",
    "\n",
    "def compute_dprime(genuine_scores: List[float], impostor_scores: List[float]) -> float:\n",
    "    if not genuine_scores or not impostor_scores:\n",
    "        return 0.0\n",
    "    \n",
    "    mean_genuine = np.mean(genuine_scores)\n",
    "    mean_impostor = np.mean(impostor_scores)\n",
    "    std_genuine = np.std(genuine_scores)\n",
    "    std_impostor = np.std(impostor_scores)\n",
    "    \n",
    "    pooled_std = np.sqrt((std_genuine**2 + std_impostor**2) / 2)\n",
    "    \n",
    "    if pooled_std == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (mean_genuine - mean_impostor) / pooled_std\n",
    "\n",
    "def bootstrap_confidence_interval(data: List[float], \n",
    "                                 n_bootstrap: int = 1000, \n",
    "                                 confidence: float = 0.95) -> Tuple[float, float]:\n",
    "    if not data:\n",
    "        return (0.0, 0.0)\n",
    "    \n",
    "    bootstrap_means = []\n",
    "    n = len(data)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = np.random.choice(data, size=n, replace=True)\n",
    "        bootstrap_means.append(np.mean(sample))\n",
    "    \n",
    "    alpha = 1 - confidence\n",
    "    lower = np.percentile(bootstrap_means, alpha/2 * 100)\n",
    "    upper = np.percentile(bootstrap_means, (1 - alpha/2) * 100)\n",
    "    \n",
    "    return (lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "466128a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_probes_comprehensive(gallery_embeddings: Dict[str, Dict],\n",
    "                                 probe_embeddings: Dict[str, Dict],\n",
    "                                 thresholds: List[float],\n",
    "                                 aggregation: str = 'mean',\n",
    "                                 k: int = 3) -> Dict:\n",
    "    probe_data = probe_embeddings.get(\"all\", probe_embeddings)\n",
    "    all_predictions = []\n",
    "    genuine_scores = []\n",
    "    impostor_scores = []\n",
    "    per_identity = {}  \n",
    "    \n",
    "    for true_name, data in tqdm(probe_data.items(), desc=f\"Processing probes ({aggregation})\"):\n",
    "        probe_embs = data['embeddings']\n",
    "        \n",
    "        for probe_emb in probe_embs:\n",
    "            predicted_name, best_score, identity_scores = identify_probe(\n",
    "                probe_emb, gallery_embeddings, threshold=0.0,\n",
    "                aggregation=aggregation, k=k\n",
    "            )\n",
    "            \n",
    "            rank_metrics = compute_rank_metrics(identity_scores, true_name)\n",
    "\n",
    "            \n",
    "            all_predictions.append({\n",
    "                'true_identity': true_name,\n",
    "                'predicted_identity': predicted_name,\n",
    "                'score': best_score,\n",
    "                'identity_scores': identity_scores,\n",
    "                'rank_metrics': rank_metrics\n",
    "            })\n",
    "\n",
    "            if true_name not in per_identity:\n",
    "                per_identity[true_name] = {\n",
    "                    \"rank1_total\": 0,\n",
    "                    \"rank5_total\": 0,\n",
    "                    \"rank10_total\": 0,\n",
    "                    \"mrr_sum\": 0,\n",
    "                    \"count\": 0,\n",
    "                    \"genuine_scores\": [],\n",
    "                    \"impostor_scores\": []\n",
    "                }\n",
    "\n",
    "            pid = per_identity[true_name]\n",
    "            pid[\"rank1_total\"] += 1 if rank_metrics[\"rank1\"] else 0\n",
    "            pid[\"rank5_total\"] += 1 if rank_metrics[\"rank5\"] else 0\n",
    "            pid[\"rank10_total\"] += 1 if rank_metrics[\"rank10\"] else 0\n",
    "            pid[\"mrr_sum\"] += rank_metrics[\"reciprocal_rank\"]\n",
    "            pid[\"count\"] += 1\n",
    "\n",
    "            # store score for genuine / impostor histogram\n",
    "            pid[\"genuine_scores\"].append(identity_scores.get(true_name, 0))\n",
    "            pid[\"impostor_scores\"].extend(\n",
    "                [v for id2, v in identity_scores.items() if id2 != true_name]\n",
    "            )\n",
    "            \n",
    "            if true_name in identity_scores:\n",
    "                genuine_scores.append(identity_scores[true_name])\n",
    "            \n",
    "            for name, score in identity_scores.items():\n",
    "                if name != true_name:\n",
    "                    impostor_scores.append(score)\n",
    "\n",
    "    threshold_results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        tp = fp = tn = fn = 0\n",
    "        rank1_correct = rank5_correct = rank10_correct = 0\n",
    "        mrr_sum = 0\n",
    "        \n",
    "        correct_scores = []\n",
    "        incorrect_scores = []\n",
    "        \n",
    "        for pred in all_predictions:\n",
    "            true_name = pred['true_identity']\n",
    "            predicted_name = pred['predicted_identity']\n",
    "            score = pred['score']\n",
    "            rank_metrics = pred['rank_metrics']\n",
    "            \n",
    "            if score >= threshold:\n",
    "                if predicted_name == true_name:\n",
    "                    tp += 1\n",
    "                    correct_scores.append(score)\n",
    "                else:\n",
    "                    fp += 1\n",
    "                    incorrect_scores.append(score)\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "            if rank_metrics['rank1']:\n",
    "                rank1_correct += 1\n",
    "            if rank_metrics['rank5']:\n",
    "                rank5_correct += 1\n",
    "            if rank_metrics['rank10']:\n",
    "                rank10_correct += 1\n",
    "            mrr_sum += rank_metrics['reciprocal_rank']\n",
    "        \n",
    "        n_probes = len(all_predictions)\n",
    "\n",
    "        rank1_acc = rank1_correct / n_probes if n_probes > 0 else 0\n",
    "        rank5_acc = rank5_correct / n_probes if n_probes > 0 else 0\n",
    "        rank10_acc = rank10_correct / n_probes if n_probes > 0 else 0\n",
    "        mrr = mrr_sum / n_probes if n_probes > 0 else 0\n",
    "        \n",
    "        far = fp / n_probes if n_probes > 0 else 0\n",
    "        frr = fn / n_probes if n_probes > 0 else 0\n",
    "        tar = tp / n_probes if n_probes > 0 else 0\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'threshold': threshold,\n",
    "            'rank1_accuracy': rank1_acc,\n",
    "            'rank5_accuracy': rank5_acc,\n",
    "            'rank10_accuracy': rank10_acc,\n",
    "            'mrr': mrr,\n",
    "            'tar': tar,\n",
    "            'far': far,\n",
    "            'frr': frr,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'n_probes': n_probes,\n",
    "            'avg_correct_score': np.mean(correct_scores) if correct_scores else 0,\n",
    "            'avg_incorrect_score': np.mean(incorrect_scores) if incorrect_scores else 0,\n",
    "        })\n",
    "    \n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    for pred in all_predictions:\n",
    "        y_true.append(1 if pred['predicted_identity'] == pred['true_identity'] else 0)\n",
    "        y_scores.append(pred['score'])\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    avg_precision = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    dprime = compute_dprime(genuine_scores, impostor_scores)\n",
    "    \n",
    "    genuine_ci = bootstrap_confidence_interval(genuine_scores)\n",
    "    impostor_ci = bootstrap_confidence_interval(impostor_scores)\n",
    "    per_identity_results = {}\n",
    "    for identity, stats in per_identity.items():\n",
    "        c = stats[\"count\"]\n",
    "        per_identity_results[identity] = {\n",
    "            \"rank1\": stats[\"rank1_total\"] / c,\n",
    "            \"rank5\": stats[\"rank5_total\"] / c,\n",
    "            \"rank10\": stats[\"rank10_total\"] / c,\n",
    "            \"mrr\": stats[\"mrr_sum\"] / c,\n",
    "            \"n_samples\": c,\n",
    "            \"genuine_scores\": stats[\"genuine_scores\"],\n",
    "            \"impostor_scores\": stats[\"impostor_scores\"]\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'threshold_results': pd.DataFrame(threshold_results),\n",
    "        'roc_auc': roc_auc,\n",
    "        'average_precision': avg_precision,\n",
    "        'dprime': dprime,\n",
    "        'genuine_scores': genuine_scores,\n",
    "        'impostor_scores': impostor_scores,\n",
    "        'genuine_ci': genuine_ci,\n",
    "        'impostor_ci': impostor_ci,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'aggregation': aggregation,\n",
    "        'all_predictions': all_predictions,\n",
    "        'per_identity': per_identity_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d6e696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_impostors_comprehensive(gallery_embeddings: Dict[str, Dict],\n",
    "                                    impostor_embeddings: Dict[str, Dict],\n",
    "                                    thresholds: List[float],\n",
    "                                    aggregation: str = 'mean',\n",
    "                                    k: int = 3) -> Dict:\n",
    "    impostor_scores = []\n",
    "    \n",
    "    for dataset_name, data in tqdm(impostor_embeddings.items(), desc=f\"Processing impostors ({aggregation})\"):\n",
    "        impostor_embs = data['embeddings']\n",
    "        \n",
    "        for impostor_emb in impostor_embs:\n",
    "            _, score, _ = identify_probe(\n",
    "                impostor_emb, gallery_embeddings, threshold=0.0,\n",
    "                aggregation=aggregation, k=k\n",
    "            )\n",
    "            impostor_scores.append(score)\n",
    "    \n",
    "    threshold_results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        tn = sum(1 for s in impostor_scores if s < threshold)\n",
    "        fp = sum(1 for s in impostor_scores if s >= threshold)\n",
    "        n_impostors = len(impostor_scores)\n",
    "        \n",
    "        rejection_rate = tn / n_impostors if n_impostors > 0 else 0\n",
    "        far = fp / n_impostors if n_impostors > 0 else 0\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'threshold': threshold,\n",
    "            'rejection_rate': rejection_rate,\n",
    "            'far': far,\n",
    "            'tn': tn,\n",
    "            'fp': fp,\n",
    "            'n_impostors': n_impostors,\n",
    "            'avg_impostor_score': np.mean(impostor_scores)\n",
    "        })\n",
    "    \n",
    "    impostor_ci = bootstrap_confidence_interval(impostor_scores)\n",
    "    \n",
    "    return {\n",
    "        'threshold_results': pd.DataFrame(threshold_results),\n",
    "        'impostor_scores': impostor_scores,\n",
    "        'impostor_ci': impostor_ci,\n",
    "        'mean_impostor_score': np.mean(impostor_scores),\n",
    "        'std_impostor_score': np.std(impostor_scores),\n",
    "        'aggregation': aggregation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4490280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmented_comprehensive(gallery_embeddings: Dict[str, Dict],\n",
    "                                    probe_embeddings: Dict[str, Dict],\n",
    "                                    thresholds: List[float],\n",
    "                                    aggregation: str = 'mean',\n",
    "                                    k: int = 3) -> Dict[str, Dict]:  \n",
    "    segment_results = {}\n",
    "    segments = [k for k in probe_embeddings.keys() if k != 'all']\n",
    "    \n",
    "    print(f\"Found {len(segments)} segments: {segments}\")\n",
    "    \n",
    "    for segment_name in tqdm(segments, desc=f\"Processing segments ({aggregation})\"):\n",
    "        segment_data = probe_embeddings[segment_name]\n",
    "        segment_probe = {'all': segment_data}\n",
    "        \n",
    "        results = evaluate_probes_comprehensive(\n",
    "            gallery_embeddings, segment_probe, thresholds,\n",
    "            aggregation=aggregation, k=k\n",
    "        )\n",
    "        \n",
    "        segment_results[segment_name] = results\n",
    "    \n",
    "    return segment_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a99bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_comparison_summary(all_model_results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Generate comprehensive comparison table across all models\"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        basic_results = model_data.get('basic_probe', {})\n",
    "        \n",
    "        for gallery_name, gallery_results in basic_results.items():\n",
    "            for agg_method, results in gallery_results.items():\n",
    "                df = results['threshold_results']\n",
    "                best_idx = df['rank1_accuracy'].idxmax()\n",
    "                best_row = df.loc[best_idx]\n",
    "                \n",
    "                summary_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Gallery': gallery_name,\n",
    "                    'Aggregation': agg_method,\n",
    "                    'Rank-1': best_row['rank1_accuracy'],\n",
    "                    'Rank-5': best_row['rank5_accuracy'],\n",
    "                    'Rank-10': best_row['rank10_accuracy'],\n",
    "                    'MRR': best_row['mrr'],\n",
    "                    'ROC-AUC': results['roc_auc'],\n",
    "                    'd-prime': results['dprime'],\n",
    "                    'Best_Threshold': best_row['threshold'],\n",
    "                    'F1-Score': best_row['f1_score'],\n",
    "                    'TAR': best_row['tar'],\n",
    "                    'FAR': best_row['far']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4444bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_segmented_comparison_table(all_model_results: Dict, \n",
    "                                     gallery_type: str = 'oneshot') -> pd.DataFrame:\n",
    "    \"\"\"Create comparison table for segmented evaluations\"\"\"\n",
    "    segment_data = []\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        seg_key = f'segmented_{gallery_type}'\n",
    "        if seg_key not in model_data:\n",
    "            continue\n",
    "            \n",
    "        segment_results = model_data[seg_key]\n",
    "        \n",
    "        for segment_name, results in segment_results.items():\n",
    "            df = results['threshold_results']\n",
    "            best_idx = df['rank1_accuracy'].idxmax()\n",
    "            \n",
    "            segment_data.append({\n",
    "                'Model': model_name,\n",
    "                'Segment': segment_name,\n",
    "                'Rank-1': df.loc[best_idx, 'rank1_accuracy'],\n",
    "                'ROC-AUC': results['roc_auc'],\n",
    "                'd-prime': results['dprime'],\n",
    "                'MRR': df.loc[best_idx, 'mrr']\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(segment_data)\n",
    "    \n",
    "    # Pivot for better visualization\n",
    "    pivot_rank1 = df.pivot(index='Model', columns='Segment', values='Rank-1')\n",
    "    pivot_rank1['Mean'] = pivot_rank1.mean(axis=1)\n",
    "    pivot_rank1['Std'] = pivot_rank1.std(axis=1)\n",
    "    pivot_rank1['Min'] = pivot_rank1.drop(['Mean', 'Std'], axis=1).min(axis=1)\n",
    "    pivot_rank1['Max'] = pivot_rank1.drop(['Mean', 'Std', 'Min'], axis=1).max(axis=1)\n",
    "    \n",
    "    return pivot_rank1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "949df826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gallery_strategies(all_model_results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Compare oneshot vs fewshot, base vs augmented\"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        basic_results = model_data.get('basic_probe', {})\n",
    "        \n",
    "        # Get best performance for each configuration\n",
    "        configs = {}\n",
    "        for gallery_name, gallery_results in basic_results.items():\n",
    "            best_rank1 = 0\n",
    "            best_agg = None\n",
    "            for agg_method, results in gallery_results.items():\n",
    "                df = results['threshold_results']\n",
    "                rank1 = df['rank1_accuracy'].max()\n",
    "                if rank1 > best_rank1:\n",
    "                    best_rank1 = rank1\n",
    "                    best_agg = agg_method\n",
    "            configs[gallery_name] = {'rank1': best_rank1, 'agg': best_agg}\n",
    "        \n",
    "        # Calculate improvements\n",
    "        oneshot_base = configs.get('oneshot_base', {}).get('rank1', 0)\n",
    "        oneshot_aug = configs.get('oneshot_augmented', {}).get('rank1', 0)\n",
    "        fewshot_base = configs.get('fewshot_base', {}).get('rank1', 0)\n",
    "        fewshot_aug = configs.get('fewshot_augmented', {}).get('rank1', 0)\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Oneshot_Base': oneshot_base,\n",
    "            'Oneshot_Aug': oneshot_aug,\n",
    "            'Fewshot_Base': fewshot_base,\n",
    "            'Fewshot_Aug': fewshot_aug,\n",
    "            'Aug_Improvement_Oneshot': oneshot_aug - oneshot_base,\n",
    "            'Aug_Improvement_Fewshot': fewshot_aug - fewshot_base,\n",
    "            'Fewshot_Improvement_Base': fewshot_base - oneshot_base,\n",
    "            'Fewshot_Improvement_Aug': fewshot_aug - oneshot_aug,\n",
    "            'Best_Config': max(configs.items(), key=lambda x: x[1]['rank1'])[0],\n",
    "            'Best_Rank1': max(c['rank1'] for c in configs.values())\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "384420ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_aggregation_performance(all_model_results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Analyze which aggregation method works best\"\"\"\n",
    "    agg_data = []\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        basic_results = model_data.get('basic_probe', {})\n",
    "        \n",
    "        for gallery_name, gallery_results in basic_results.items():\n",
    "            agg_scores = {}\n",
    "            for agg_method, results in gallery_results.items():\n",
    "                df = results['threshold_results']\n",
    "                agg_scores[agg_method] = df['rank1_accuracy'].max()\n",
    "            \n",
    "            best_agg = max(agg_scores.items(), key=lambda x: x[1])\n",
    "            \n",
    "            agg_data.append({\n",
    "                'Model': model_name,\n",
    "                'Gallery': gallery_name,\n",
    "                'Best_Aggregation': best_agg[0],\n",
    "                'MAX_Score': agg_scores.get('max', 0),\n",
    "                'MEAN_Score': agg_scores.get('mean', 0),\n",
    "                'TOPK_Score': agg_scores.get('topk', 0),\n",
    "                'Best_Score': best_agg[1],\n",
    "                'Score_Range': max(agg_scores.values()) - min(agg_scores.values())\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(agg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116bd073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_operating_thresholds(all_model_results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Recommend thresholds for different operating points\"\"\"\n",
    "    threshold_recs = []\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        basic_results = model_data.get('basic_probe', {})\n",
    "        \n",
    "        for gallery_name, gallery_results in basic_results.items():\n",
    "            for agg_method, results in gallery_results.items():\n",
    "                df = results['threshold_results']\n",
    "                \n",
    "                # Find various operating points\n",
    "                tar_99_idx = (df['tar'] - 0.99).abs().idxmin()\n",
    "                far_001_idx = (df['far'] - 0.001).abs().idxmin()\n",
    "                f1_max_idx = df['f1_score'].idxmax()\n",
    "                \n",
    "                # Balanced TAR/FAR (minimize |TAR - (1-FAR)|)\n",
    "                df['tar_far_diff'] = abs(df['tar'] - (1 - df['far']))\n",
    "                balanced_idx = df['tar_far_diff'].idxmin()\n",
    "                \n",
    "                threshold_recs.append({\n",
    "                    'Model': model_name,\n",
    "                    'Gallery': gallery_name,\n",
    "                    'Aggregation': agg_method,\n",
    "                    'Threshold_TAR99': df.loc[tar_99_idx, 'threshold'],\n",
    "                    'TAR_at_TAR99': df.loc[tar_99_idx, 'tar'],\n",
    "                    'Threshold_FAR0.001': df.loc[far_001_idx, 'threshold'],\n",
    "                    'TAR_at_FAR0.001': df.loc[far_001_idx, 'tar'],\n",
    "                    'Threshold_BestF1': df.loc[f1_max_idx, 'threshold'],\n",
    "                    'F1_Score': df.loc[f1_max_idx, 'f1_score'],\n",
    "                    'Threshold_Balanced': df.loc[balanced_idx, 'threshold'],\n",
    "                    'TAR_Balanced': df.loc[balanced_idx, 'tar'],\n",
    "                    'FAR_Balanced': df.loc[balanced_idx, 'far']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(threshold_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "646f185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_failure_cases(all_model_results: Dict) -> Dict:\n",
    "    \"\"\"Analyze failure patterns\"\"\"\n",
    "    failure_analysis = {}\n",
    "    \n",
    "    for model_name, model_data in all_model_results.items():\n",
    "        basic_results = model_data.get('basic_probe', {})\n",
    "        \n",
    "        for gallery_name, gallery_results in basic_results.items():\n",
    "            # Use mean aggregation for analysis\n",
    "            if 'mean' not in gallery_results:\n",
    "                continue\n",
    "                \n",
    "            results = gallery_results['mean']\n",
    "            predictions = results.get('all_predictions', [])\n",
    "            \n",
    "            if not predictions:\n",
    "                continue\n",
    "            \n",
    "            # Find misclassifications\n",
    "            misclassified = [p for p in predictions if p['predicted_identity'] != p['true_identity']]\n",
    "            \n",
    "            # Count confusion pairs\n",
    "            confusion_pairs = {}\n",
    "            identity_errors = {}\n",
    "            \n",
    "            for pred in misclassified:\n",
    "                true_id = pred['true_identity']\n",
    "                pred_id = pred['predicted_identity']\n",
    "                \n",
    "                pair = f\"{true_id} -> {pred_id}\"\n",
    "                confusion_pairs[pair] = confusion_pairs.get(pair, 0) + 1\n",
    "                \n",
    "                identity_errors[true_id] = identity_errors.get(true_id, 0) + 1\n",
    "            \n",
    "            # Sort by frequency\n",
    "            top_confusions = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            top_errors = sorted(identity_errors.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            \n",
    "            failure_analysis[f\"{model_name}_{gallery_name}\"] = {\n",
    "                'total_predictions': len(predictions),\n",
    "                'total_errors': len(misclassified),\n",
    "                'error_rate': len(misclassified) / len(predictions),\n",
    "                'top_confusion_pairs': top_confusions,\n",
    "                'most_confused_identities': top_errors\n",
    "            }\n",
    "    \n",
    "    return failure_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5acdbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_statistical(all_model_results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Statistical significance testing between models\"\"\"\n",
    "    stat_comparisons = []\n",
    "    \n",
    "    models = list(all_model_results.keys())\n",
    "    \n",
    "    for i, model1 in enumerate(models):\n",
    "        for model2 in models[i+1:]:\n",
    "            # Compare on fewshot_augmented + mean (best config)\n",
    "            try:\n",
    "                results1 = all_model_results[model1]['basic_probe']['fewshot_augmented']['mean']\n",
    "                results2 = all_model_results[model2]['basic_probe']['fewshot_augmented']['mean']\n",
    "                \n",
    "                scores1 = [p['score'] if p['predicted_identity'] == p['true_identity'] else 0 \n",
    "                          for p in results1['all_predictions']]\n",
    "                scores2 = [p['score'] if p['predicted_identity'] == p['true_identity'] else 0 \n",
    "                          for p in results2['all_predictions']]\n",
    "                \n",
    "                # Paired t-test\n",
    "                t_stat, p_value = stats.ttest_rel(scores1, scores2)\n",
    "                \n",
    "                # Effect size (Cohen's d)\n",
    "                mean_diff = np.mean(scores1) - np.mean(scores2)\n",
    "                pooled_std = np.sqrt((np.std(scores1)**2 + np.std(scores2)**2) / 2)\n",
    "                cohens_d = mean_diff / pooled_std if pooled_std > 0 else 0\n",
    "                \n",
    "                stat_comparisons.append({\n",
    "                    'Model_A': model1,\n",
    "                    'Model_B': model2,\n",
    "                    'Mean_Diff': mean_diff,\n",
    "                    't_statistic': t_stat,\n",
    "                    'p_value': p_value,\n",
    "                    'Significant': 'Yes' if p_value < 0.05 else 'No',\n",
    "                    'Cohens_d': cohens_d,\n",
    "                    'Effect_Size': 'Small' if abs(cohens_d) < 0.5 else ('Medium' if abs(cohens_d) < 0.8 else 'Large')\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(stat_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ab05fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_executive_summary(all_model_results: Dict, \n",
    "                              comparison_summary: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate auto-summary of key findings\"\"\"\n",
    "    \n",
    "    # Best overall model\n",
    "    best_row = comparison_summary.loc[comparison_summary['Rank-1'].idxmax()]\n",
    "    \n",
    "    # Best per gallery type\n",
    "    best_per_gallery = comparison_summary.groupby('Gallery').apply(\n",
    "        lambda x: x.loc[x['Rank-1'].idxmax()]\n",
    "    )\n",
    "    \n",
    "    summary = f\"\"\"\n",
    "================================================================================\n",
    "EXECUTIVE SUMMARY - Face Recognition Evaluation\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "================================================================================\n",
    "\n",
    "KEY FINDINGS:\n",
    "\n",
    "1. OVERALL BEST PERFORMANCE\n",
    "   Model: {best_row['Model']}\n",
    "   Configuration: {best_row['Gallery']} + {best_row['Aggregation']}\n",
    "   Rank-1 Accuracy: {best_row['Rank-1']:.2%}\n",
    "   ROC-AUC: {best_row['ROC-AUC']:.4f}\n",
    "   d-prime: {best_row['d-prime']:.3f}\n",
    "\n",
    "2. BEST CONFIGURATION PER GALLERY TYPE\n",
    "\"\"\"\n",
    "    \n",
    "    for gallery, row in best_per_gallery.iterrows():\n",
    "        summary += f\"\"\"\n",
    "   {gallery.upper()}:\n",
    "   - Model: {row['Model']} ({row['Aggregation']})\n",
    "   - Rank-1: {row['Rank-1']:.2%}\n",
    "   - ROC-AUC: {row['ROC-AUC']:.4f}\n",
    "\"\"\"\n",
    "    \n",
    "    # Model rankings\n",
    "    model_rankings = comparison_summary.groupby('Model')['Rank-1'].max().sort_values(ascending=False)\n",
    "    \n",
    "    summary += f\"\"\"\n",
    "3. MODEL RANKINGS (by best Rank-1 accuracy)\n",
    "\"\"\"\n",
    "    for idx, (model, score) in enumerate(model_rankings.items(), 1):\n",
    "        summary += f\"   {idx}. {model}: {score:.2%}\\n\"\n",
    "    \n",
    "    # Aggregation method analysis\n",
    "    agg_wins = comparison_summary.groupby(['Gallery', 'Aggregation'])['Rank-1'].max()\n",
    "    best_agg_per_gallery = agg_wins.groupby('Gallery').idxmax()\n",
    "    \n",
    "    summary += f\"\"\"\n",
    "4. BEST AGGREGATION METHOD PER GALLERY\n",
    "\"\"\"\n",
    "    for gallery, (_, agg) in best_agg_per_gallery.items():\n",
    "        summary += f\"   {gallery}: {agg.upper()}\\n\"\n",
    "    \n",
    "    summary += f\"\"\"\n",
    "5. KEY RECOMMENDATIONS\n",
    "   - Use {best_row['Model']} with {best_row['Gallery']} gallery for best accuracy\n",
    "   - {best_row['Aggregation'].upper()} aggregation works best for this configuration\n",
    "   - Operating threshold: {best_row['Best_Threshold']:.3f} for optimal performance\n",
    "   - All models achieve 100% impostor rejection at threshold â‰¥ 0.35\n",
    "\n",
    "6. LIMITATIONS\n",
    "   - Performance degrades significantly on high pitch and high yaw conditions\n",
    "   - Low quality images reduce accuracy by ~15-30%\n",
    "   - Baseline/frontal images show best performance (>90% Rank-1)\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b41c38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics(results: Dict, title: str, save_path: Path):\n",
    "    df = results['threshold_results']\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(df['threshold'], df['rank1_accuracy'], 'b-', linewidth=2, label='Rank-1')\n",
    "    ax1.plot(df['threshold'], df['rank5_accuracy'], 'g-', linewidth=2, label='Rank-5')\n",
    "    ax1.plot(df['threshold'], df['rank10_accuracy'], 'r-', linewidth=2, label='Rank-10')\n",
    "    ax1.set_xlabel('Threshold')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Rank-k Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(df['threshold'], df['mrr'], 'purple', linewidth=2)\n",
    "    ax2.set_xlabel('Threshold')\n",
    "    ax2.set_ylabel('MRR')\n",
    "    ax2.set_title('Mean Reciprocal Rank')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.plot(df['threshold'], df['far'], 'r-', linewidth=2, label='FAR')\n",
    "    ax3.plot(df['threshold'], df['frr'], 'g-', linewidth=2, label='FRR')\n",
    "    ax3.plot(df['threshold'], df['tar'], 'b-', linewidth=2, label='TAR')\n",
    "    ax3.set_xlabel('Threshold')\n",
    "    ax3.set_ylabel('Rate')\n",
    "    ax3.set_title('FAR/FRR/TAR')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    ax4.plot(results['fpr'], results['tpr'], 'b-', linewidth=2)\n",
    "    ax4.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    ax4.set_xlabel('False Positive Rate')\n",
    "    ax4.set_ylabel('True Positive Rate')\n",
    "    ax4.set_title(f'ROC Curve (AUC={results[\"roc_auc\"]:.4f})')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    ax5 = fig.add_subplot(gs[1, 0])\n",
    "    ax5.plot(df['threshold'], df['precision'], 'b-', linewidth=2, label='Precision')\n",
    "    ax5.plot(df['threshold'], df['recall'], 'orange', linewidth=2, label='Recall')\n",
    "    ax5.plot(df['threshold'], df['f1_score'], 'purple', linewidth=2, label='F1-Score')\n",
    "    ax5.set_xlabel('Threshold')\n",
    "    ax5.set_ylabel('Score')\n",
    "    ax5.set_title('Precision/Recall/F1')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "    ax6 = fig.add_subplot(gs[1, 1])\n",
    "    ax6.hist(results['genuine_scores'], bins=50, alpha=0.5, label='Genuine', color='green')\n",
    "    ax6.hist(results['impostor_scores'], bins=50, alpha=0.5, label='Impostor', color='red')\n",
    "    ax6.axvline(np.mean(results['genuine_scores']), color='green', linestyle='--', linewidth=2)\n",
    "    ax6.axvline(np.mean(results['impostor_scores']), color='red', linestyle='--', linewidth=2)\n",
    "    ax6.set_xlabel('Similarity Score')\n",
    "    ax6.set_ylabel('Frequency')\n",
    "    ax6.set_title(f\"Score Distributions (d'={results['dprime']:.3f})\")\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "  \n",
    "    ax7 = fig.add_subplot(gs[1, 2])\n",
    "    ax7.plot(df['far'], df['frr'], 'b-', linewidth=2)\n",
    "    ax7.set_xlabel('False Accept Rate')\n",
    "    ax7.set_ylabel('False Reject Rate')\n",
    "    ax7.set_title('DET Curve')\n",
    "    ax7.set_xscale('log')\n",
    "    ax7.set_yscale('log')\n",
    "    ax7.grid(True, alpha=0.3, which='both')\n",
    " \n",
    "    ax8 = fig.add_subplot(gs[1, 3])\n",
    "    best_threshold_idx = df['rank1_accuracy'].idxmax()\n",
    "    ranks = [1, 5, 10]\n",
    "    cmc_scores = [\n",
    "        df.loc[best_threshold_idx, 'rank1_accuracy'],\n",
    "        df.loc[best_threshold_idx, 'rank5_accuracy'],\n",
    "        df.loc[best_threshold_idx, 'rank10_accuracy']\n",
    "    ]\n",
    "    ax8.plot(ranks, cmc_scores, 'bo-', linewidth=2, markersize=8)\n",
    "    ax8.set_xlabel('Rank')\n",
    "    ax8.set_ylabel('Identification Rate')\n",
    "    ax8.set_title('CMC Curve')\n",
    "    ax8.set_xticks(ranks)\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "\n",
    "    ax9 = fig.add_subplot(gs[2, 0])\n",
    "    ax9.plot(df['threshold'], df['avg_correct_score'], 'g-', linewidth=2, label='Correct Matches')\n",
    "    ax9.plot(df['threshold'], df['avg_incorrect_score'], 'r-', linewidth=2, label='Incorrect Matches')\n",
    "    ax9.set_xlabel('Threshold')\n",
    "    ax9.set_ylabel('Average Score')\n",
    "    ax9.set_title('Score Analysis')\n",
    "    ax9.legend()\n",
    "    ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax10 = fig.add_subplot(gs[2, 1])\n",
    "    genuine_mean = np.mean(results['genuine_scores'])\n",
    "    impostor_mean = np.mean(results['impostor_scores'])\n",
    "    genuine_ci = results['genuine_ci']\n",
    "    impostor_ci = results['impostor_ci']\n",
    "    \n",
    "    categories = ['Genuine', 'Impostor']\n",
    "    means = [genuine_mean, impostor_mean]\n",
    "    errors_lower = [genuine_mean - genuine_ci[0], impostor_mean - impostor_ci[0]]\n",
    "    errors_upper = [genuine_ci[1] - genuine_mean, impostor_ci[1] - impostor_mean]\n",
    "    \n",
    "    ax10.bar(categories, means, yerr=[errors_lower, errors_upper], \n",
    "            capsize=10, alpha=0.7, color=['green', 'red'])\n",
    "    ax10.set_ylabel('Similarity Score')\n",
    "    ax10.set_title('Mean Scores with 95% CI')\n",
    "    ax10.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    ax11 = fig.add_subplot(gs[2, 2])\n",
    "    target_fars = [0.1, 0.01, 0.001]\n",
    "    tars_at_far = []\n",
    "    for target_far in target_fars:\n",
    "        idx = (df['far'] - target_far).abs().idxmin()\n",
    "        tars_at_far.append(df.loc[idx, 'tar'])\n",
    "    \n",
    "    ax11.bar([f'FAR={f}' for f in target_fars], tars_at_far, alpha=0.7)\n",
    "    ax11.set_ylabel('TAR')\n",
    "    ax11.set_title('TAR @ FAR')\n",
    "    ax11.set_ylim([0, 1])\n",
    "    ax11.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    ax12 = fig.add_subplot(gs[2, 3])\n",
    "    ax12.axis('off')\n",
    "    \n",
    "    best_idx = df['rank1_accuracy'].idxmax()\n",
    "    best_row = df.loc[best_idx]\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    SUMMARY STATISTICS\n",
    "    ==================\n",
    "    Aggregation: {results['aggregation'].upper()}\n",
    "    \n",
    "    Best Rank-1: {best_row['rank1_accuracy']:.4f}\n",
    "    @ Threshold: {best_row['threshold']:.3f}\n",
    "    \n",
    "    Rank-5: {best_row['rank5_accuracy']:.4f}\n",
    "    Rank-10: {best_row['rank10_accuracy']:.4f}\n",
    "    MRR: {best_row['mrr']:.4f}\n",
    "    \n",
    "    ROC-AUC: {results['roc_auc']:.4f}\n",
    "    Avg Precision: {results['average_precision']:.4f}\n",
    "    d-prime: {results['dprime']:.3f}\n",
    "    \n",
    "    TAR@FAR=0.01: {tars_at_far[1]:.4f}\n",
    "    \n",
    "    Best F1: {df['f1_score'].max():.4f}\n",
    "    @ Threshold: {df.loc[df['f1_score'].idxmax(), 'threshold']:.3f}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax12.text(0.1, 0.5, summary_text, fontsize=10, family='monospace',\n",
    "             verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84eb1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_core_metrics(results: Dict, title: str, save_path: Path):\n",
    "    \"\"\"Simplified plot with only essential metrics for quick analysis\"\"\"\n",
    "    df = results['threshold_results']\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Rank-k Accuracy\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(df['threshold'], df['rank1_accuracy'], 'b-', linewidth=2.5, label='Rank-1')\n",
    "    ax1.plot(df['threshold'], df['rank5_accuracy'], 'g-', linewidth=2.5, label='Rank-5')\n",
    "    ax1.plot(df['threshold'], df['rank10_accuracy'], 'r-', linewidth=2.5, label='Rank-10')\n",
    "    ax1.set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('Rank-k Accuracy vs Threshold', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. ROC Curve\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(results['fpr'], results['tpr'], 'b-', linewidth=2.5)\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=1.5)\n",
    "    ax2.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax2.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title(f'ROC Curve (AUC={results[\"roc_auc\"]:.4f})', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim([0, 1])\n",
    "    ax2.set_ylim([0, 1])\n",
    "    \n",
    "    # 3. FAR/FRR/TAR\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.plot(df['threshold'], df['far'], 'r-', linewidth=2.5, label='FAR')\n",
    "    ax3.plot(df['threshold'], df['frr'], 'orange', linewidth=2.5, label='FRR')\n",
    "    ax3.plot(df['threshold'], df['tar'], 'b-', linewidth=2.5, label='TAR')\n",
    "    \n",
    "    # Mark EER point\n",
    "    eer_idx = (df['far'] - df['frr']).abs().idxmin()\n",
    "    eer_threshold = df.loc[eer_idx, 'threshold']\n",
    "    eer_value = (df.loc[eer_idx, 'far'] + df.loc[eer_idx, 'frr']) / 2\n",
    "    ax3.plot(eer_threshold, eer_value, 'ko', markersize=8, label=f'EER={eer_value:.3f}')\n",
    "    \n",
    "    ax3.set_xlabel('Threshold', fontsize=11, fontweight='bold')\n",
    "    ax3.set_ylabel('Rate', fontsize=11, fontweight='bold')\n",
    "    ax3.set_title('FAR/FRR/TAR Analysis', fontsize=12, fontweight='bold')\n",
    "    ax3.legend(fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Score Distribution\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    ax4.hist(results['genuine_scores'], bins=50, alpha=0.6, label='Genuine', color='green', edgecolor='black')\n",
    "    ax4.hist(results['impostor_scores'], bins=50, alpha=0.6, label='Impostor', color='red', edgecolor='black')\n",
    "    ax4.axvline(np.mean(results['genuine_scores']), color='darkgreen', linestyle='--', linewidth=2, label='Genuine Mean')\n",
    "    ax4.axvline(np.mean(results['impostor_scores']), color='darkred', linestyle='--', linewidth=2, label='Impostor Mean')\n",
    "    ax4.set_xlabel('Similarity Score', fontsize=11, fontweight='bold')\n",
    "    ax4.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "    ax4.set_title('Score Distributions', fontsize=12, fontweight='bold')\n",
    "    ax4.legend(fontsize=9)\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 5. Score Confidence Intervals\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    genuine_mean = np.mean(results['genuine_scores'])\n",
    "    impostor_mean = np.mean(results['impostor_scores'])\n",
    "    genuine_ci = results['genuine_ci']\n",
    "    impostor_ci = results['impostor_ci']\n",
    "    \n",
    "    categories = ['Genuine', 'Impostor']\n",
    "    means = [genuine_mean, impostor_mean]\n",
    "    errors_lower = [genuine_mean - genuine_ci[0], impostor_mean - impostor_ci[0]]\n",
    "    errors_upper = [genuine_ci[1] - genuine_mean, impostor_ci[1] - impostor_mean]\n",
    "    \n",
    "    bars = ax5.bar(categories, means, yerr=[errors_lower, errors_upper], \n",
    "                   capsize=10, alpha=0.7, color=['green', 'red'], \n",
    "                   edgecolor='black', linewidth=1.5)\n",
    "    ax5.set_ylabel('Similarity Score', fontsize=11, fontweight='bold')\n",
    "    ax5.set_title('Mean Scores with 95% CI', fontsize=12, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean in zip(bars, means):\n",
    "        height = bar.get_height()\n",
    "        ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{mean:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 6. Summary Statistics\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    best_idx = df['rank1_accuracy'].idxmax()\n",
    "    best_row = df.loc[best_idx]\n",
    "    \n",
    "    # Calculate EER\n",
    "    eer_idx = (df['far'] - df['frr']).abs().idxmin()\n",
    "    eer = (df.loc[eer_idx, 'far'] + df.loc[eer_idx, 'frr']) / 2\n",
    "    \n",
    "    # Calculate TAR at specific FARs\n",
    "    target_fars = [0.1, 0.01, 0.001]\n",
    "    tars_at_far = []\n",
    "    for target_far in target_fars:\n",
    "        idx = (df['far'] - target_far).abs().idxmin()\n",
    "        tars_at_far.append(df.loc[idx, 'tar'])\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "SUMMARY STATISTICS\n",
    "{'='*35}\n",
    "Aggregation: {results['aggregation'].upper()}\n",
    "\n",
    "Best Rank-1:     {best_row['rank1_accuracy']:.4f}\n",
    "@ Threshold:     {best_row['threshold']:.3f}\n",
    "\n",
    "Rank-5:          {best_row['rank5_accuracy']:.4f}\n",
    "Rank-10:         {best_row['rank10_accuracy']:.4f}\n",
    "\n",
    "ROC-AUC:         {results['roc_auc']:.4f}\n",
    "EER:             {eer:.4f}\n",
    "\n",
    "TAR @ FAR=10%:   {tars_at_far[0]:.4f}\n",
    "TAR @ FAR=1%:    {tars_at_far[1]:.4f}\n",
    "TAR @ FAR=0.1%:  {tars_at_far[2]:.4f}\n",
    "\n",
    "Genuine Mean:    {genuine_mean:.3f}\n",
    "Impostor Mean:   {impostor_mean:.3f}\n",
    "Separation:      {genuine_mean - impostor_mean:.3f}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.1, 0.5, summary_text, fontsize=10, family='monospace',\n",
    "             verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Core metrics plot saved: {save_path}\")\n",
    "\n",
    "\n",
    "def plot_paper_figures(results: Dict, model_name: str, save_dir: Path):\n",
    "    \"\"\"Generate publication-quality individual plots for papers\"\"\"\n",
    "    \n",
    "    df = results['threshold_results']\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. CMC Curve\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    best_threshold_idx = df['rank1_accuracy'].idxmax()\n",
    "    ranks = list(range(1, 21))  # Rank 1-20\n",
    "    \n",
    "    # Get rank accuracies (you'll need to store these in results)\n",
    "    # For now, using available ranks\n",
    "    available_ranks = [1, 5, 10]\n",
    "    cmc_scores = [\n",
    "        df.loc[best_threshold_idx, 'rank1_accuracy'],\n",
    "        df.loc[best_threshold_idx, 'rank5_accuracy'],\n",
    "        df.loc[best_threshold_idx, 'rank10_accuracy']\n",
    "    ]\n",
    "    \n",
    "    ax.plot(available_ranks, cmc_scores, 'bo-', linewidth=3, markersize=10, \n",
    "            markeredgecolor='white', markeredgewidth=2)\n",
    "    ax.set_xlabel('Rank', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Identification Rate', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'CMC Curve - {model_name}', fontsize=16, fontweight='bold')\n",
    "    ax.set_xticks(available_ranks)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / f'{model_name}_cmc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"CMC curve saved: {save_dir / f'{model_name}_cmc_curve.png'}\")\n",
    "    \n",
    "    # 2. ROC Curve\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(results['fpr'], results['tpr'], 'b-', linewidth=3, \n",
    "            label=f'AUC = {results[\"roc_auc\"]:.4f}')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.4, linewidth=2, label='Random')\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'ROC Curve - {model_name}', fontsize=16, fontweight='bold')\n",
    "    ax.legend(fontsize=12, loc='lower right')\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / f'{model_name}_roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"ROC curve saved: {save_dir / f'{model_name}_roc_curve.png'}\")\n",
    "    \n",
    "    # 3. TAR @ FAR\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    target_fars = [0.1, 0.01, 0.001, 0.0001]\n",
    "    far_labels = ['10%', '1%', '0.1%', '0.01%']\n",
    "    tars_at_far = []\n",
    "    \n",
    "    for target_far in target_fars:\n",
    "        idx = (df['far'] - target_far).abs().idxmin()\n",
    "        tars_at_far.append(df.loc[idx, 'tar'])\n",
    "    \n",
    "    bars = ax.bar(far_labels, tars_at_far, alpha=0.7, color='steelblue', \n",
    "                   edgecolor='black', linewidth=2)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, tar in zip(bars, tars_at_far):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{tar:.3f}', ha='center', va='bottom', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('False Accept Rate', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('True Accept Rate', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'TAR @ FAR - {model_name}', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / f'{model_name}_tar_at_far.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"TAR@FAR plot saved: {save_dir / f'{model_name}_tar_at_far.png'}\")\n",
    "    \n",
    "    # 4. Score Distribution (publication quality)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.hist(results['genuine_scores'], bins=60, alpha=0.6, label='Genuine (Intra-class)', \n",
    "            color='green', edgecolor='black', linewidth=1.2)\n",
    "    ax.hist(results['impostor_scores'], bins=60, alpha=0.6, label='Impostor (Inter-class)', \n",
    "            color='red', edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    genuine_mean = np.mean(results['genuine_scores'])\n",
    "    impostor_mean = np.mean(results['impostor_scores'])\n",
    "    \n",
    "    ax.axvline(genuine_mean, color='darkgreen', linestyle='--', linewidth=2.5, \n",
    "               label=f'Genuine Î¼={genuine_mean:.3f}')\n",
    "    ax.axvline(impostor_mean, color='darkred', linestyle='--', linewidth=2.5, \n",
    "               label=f'Impostor Î¼={impostor_mean:.3f}')\n",
    "    \n",
    "    ax.set_xlabel('Cosine Similarity Score', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'Score Distribution - {model_name}', fontsize=16, fontweight='bold')\n",
    "    ax.legend(fontsize=11, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / f'{model_name}_score_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Score distribution saved: {save_dir / f'{model_name}_score_distribution.png'}\")\n",
    "    \n",
    "    print(f\"\\nAll paper figures saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "337ab2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model_comparison_charts(all_model_results: Dict, \n",
    "                                comparison_summary: pd.DataFrame,\n",
    "                                save_dir: Path):\n",
    "    \"\"\"Create comprehensive comparison visualizations\"\"\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Bar chart: Rank-1 across models & galleries\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    pivot = comparison_summary.pivot_table(\n",
    "        values='Rank-1', \n",
    "        index='Model', \n",
    "        columns='Gallery',\n",
    "        aggfunc='max'\n",
    "    )\n",
    "    \n",
    "    pivot.plot(kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_ylabel('Rank-1 Accuracy', fontsize=12)\n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_title('Rank-1 Accuracy Comparison Across Models and Galleries', fontsize=14, fontweight='bold')\n",
    "    ax.legend(title='Gallery Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'comparison_rank1_bar.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. ROC curves overlaid\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(all_model_results)))\n",
    "    \n",
    "    for (model_name, model_data), color in zip(all_model_results.items(), colors):\n",
    "        try:\n",
    "            # Use fewshot_augmented + mean as reference\n",
    "            results = model_data['basic_probe']['fewshot_augmented']['mean']\n",
    "            ax.plot(results['fpr'], results['tpr'], \n",
    "                   label=f\"{model_name} (AUC={results['roc_auc']:.3f})\",\n",
    "                   linewidth=2, color=color)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title('ROC Curve Comparison (Fewshot Augmented + Mean)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'comparison_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Heatmap: Models vs Aggregation methods\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    pivot_agg = comparison_summary[comparison_summary['Gallery'] == 'fewshot_augmented'].pivot(\n",
    "        index='Model',\n",
    "        columns='Aggregation',\n",
    "        values='Rank-1'\n",
    "    )\n",
    "    \n",
    "    sns.heatmap(pivot_agg, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                vmin=0.4, vmax=0.7, ax=ax, cbar_kws={'label': 'Rank-1 Accuracy'})\n",
    "    ax.set_title('Rank-1 Accuracy: Models vs Aggregation Methods (Fewshot Augmented)', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'comparison_aggregation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Box plots: Score distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, model_name in enumerate(list(all_model_results.keys())[:4]):\n",
    "        try:\n",
    "            results = all_model_results[model_name]['basic_probe']['fewshot_augmented']['mean']\n",
    "            genuine = results['genuine_scores']\n",
    "            impostor = results['impostor_scores']\n",
    "            \n",
    "            axes[idx].boxplot([genuine, impostor], labels=['Genuine', 'Impostor'])\n",
    "            axes[idx].set_title(f'{model_name}', fontweight='bold')\n",
    "            axes[idx].set_ylabel('Similarity Score')\n",
    "            axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    plt.suptitle('Score Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / 'comparison_score_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Comparison charts saved to: {save_dir}\")\n",
    "\n",
    "\n",
    "def plot_segmented_heatmap(segmented_table: pd.DataFrame, \n",
    "                          save_path: Path,\n",
    "                          title: str = \"Segmented Performance\"):\n",
    "    \"\"\"Create heatmap for segmented evaluation results with logical grouping\"\"\"\n",
    "    \n",
    "    # Drop summary columns for heatmap\n",
    "    plot_data = segmented_table.drop(['Mean', 'Std', 'Min', 'Max'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Define segment order with logical grouping\n",
    "    segment_categories = {\n",
    "        'Baseline & Quality': ['baseline', 'low_quality'],\n",
    "        'Face Size': ['face_large', 'face_medium', 'face_small'],\n",
    "        'Pose': ['pose_easy', 'pose_medium', 'pose_hard'],\n",
    "        'Blur': ['blur_sharp', 'blur_blurry']\n",
    "    }\n",
    "    \n",
    "    # Build ordered segment list from available columns\n",
    "    ordered_segments = []\n",
    "    for category, segments in segment_categories.items():\n",
    "        for seg in segments:\n",
    "            if seg in plot_data.columns:\n",
    "                ordered_segments.append(seg)\n",
    "    \n",
    "    # Reorder columns\n",
    "    plot_data = plot_data[ordered_segments]\n",
    "    \n",
    "    # Create figure with more height for better readability\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(plot_data, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "                vmin=0.0, vmax=1.0, ax=ax, \n",
    "                cbar_kws={'label': 'Rank-1 Accuracy'},\n",
    "                linewidths=0.5, linecolor='gray')\n",
    "    \n",
    "    # Add category separators\n",
    "    category_positions = [0]  # Start position\n",
    "    current_pos = 0\n",
    "    for category, segments in segment_categories.items():\n",
    "        available = [s for s in segments if s in ordered_segments]\n",
    "        current_pos += len(available)\n",
    "        if current_pos < len(ordered_segments):\n",
    "            category_positions.append(current_pos)\n",
    "            # Draw vertical line separator\n",
    "            ax.axvline(x=current_pos, color='black', linewidth=2.5, zorder=10)\n",
    "    \n",
    "    # Add category labels at the top\n",
    "    current_pos = 0\n",
    "    label_y = -0.15  # Position above the heatmap\n",
    "    for category, segments in segment_categories.items():\n",
    "        available = [s for s in segments if s in ordered_segments]\n",
    "        if available:\n",
    "            category_width = len(available)\n",
    "            center_pos = current_pos + category_width / 2\n",
    "            ax.text(center_pos, label_y, category, \n",
    "                   ha='center', va='top', fontsize=11, fontweight='bold',\n",
    "                   transform=ax.get_xaxis_transform())\n",
    "            current_pos += category_width\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=30)\n",
    "    ax.set_xlabel('', fontsize=12)  # Remove xlabel, we have category labels\n",
    "    ax.set_ylabel('Model', fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Segmented heatmap saved: {save_path}\")\n",
    "    \n",
    "    # Print category summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HEATMAP CATEGORY SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    for category, segments in segment_categories.items():\n",
    "        available = [s for s in segments if s in ordered_segments]\n",
    "        if available:\n",
    "            print(f\"\\n{category}:\")\n",
    "            for seg in available:\n",
    "                mean_acc = plot_data[seg].mean()\n",
    "                print(f\"  {seg:20s}: {mean_acc:.1%} (mean)\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef0e5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity_analysis(segmented_table: pd.DataFrame,\n",
    "                              save_path: Path,\n",
    "                              title: str = \"Model Sensitivity Across Segments\"):\n",
    "    \"\"\"Create line plot showing model sensitivity to different face quality conditions\"\"\"\n",
    "    \n",
    "    # Drop summary columns\n",
    "    plot_data = segmented_table.drop(['Mean', 'Std', 'Min', 'Max'], axis=1, errors='ignore')\n",
    "    \n",
    "    # Dynamically sort segments by mean accuracy across all models (best â†’ worst)\n",
    "    segment_means = plot_data.mean(axis=0).sort_values(ascending=False)\n",
    "    available_segments = segment_means.index.tolist()\n",
    "    \n",
    "    # Reorder columns based on sorted segments\n",
    "    plot_data = plot_data[available_segments]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Define colors and markers for each model\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#06A77D']\n",
    "    markers = ['o', 's', '^', 'D']\n",
    "    \n",
    "    # Plot each model\n",
    "    for idx, (model_name, row) in enumerate(plot_data.iterrows()):\n",
    "        color = colors[idx % len(colors)]\n",
    "        marker = markers[idx % len(markers)]\n",
    "        \n",
    "        ax.plot(available_segments, row.values, \n",
    "                marker=marker, \n",
    "                linewidth=2.5, \n",
    "                markersize=8,\n",
    "                label=model_name,\n",
    "                color=color,\n",
    "                markeredgecolor='white',\n",
    "                markeredgewidth=1.5)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Quality Segment (Best â†’ Worst)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Rank-1 Accuracy', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=15, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Grid\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Y-axis limits\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1%}'))\n",
    "    \n",
    "    # X-axis\n",
    "    ax.set_xticks(range(len(available_segments)))\n",
    "    ax.set_xticklabels(available_segments, rotation=45, ha='right')\n",
    "    \n",
    "    # Legend\n",
    "    ax.legend(loc='best', frameon=True, shadow=True, fontsize=11)\n",
    "    \n",
    "    # Add a subtle background gradient to show quality degradation\n",
    "    gradient = ax.imshow([[0, 1]], cmap='RdYlGn_r', aspect='auto',\n",
    "                        extent=[0, len(available_segments)-1, 0, 1.05],\n",
    "                        alpha=0.1, zorder=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Sensitivity plot saved: {save_path}\")\n",
    "    \n",
    "    # Print numerical summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SENSITIVITY ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nSegment order (best â†’ worst by mean accuracy):\")\n",
    "    for seg in available_segments:\n",
    "        mean_acc = segment_means[seg]\n",
    "        print(f\"  {seg:20s}: {mean_acc:.1%}\")\n",
    "    \n",
    "    print(\"\\nPerformance degradation per model:\")\n",
    "    for model_name, row in plot_data.iterrows():\n",
    "        best_acc = row.max()\n",
    "        worst_acc = row.min()\n",
    "        degradation = best_acc - worst_acc\n",
    "        best_seg = row.idxmax()\n",
    "        worst_seg = row.idxmin()\n",
    "        print(f\"  {model_name:30s}: {best_acc:.1%} ({best_seg}) â†’ {worst_acc:.1%} ({worst_seg}) | Î” = {degradation:.1%}\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70efafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery_strategy_comparison(strategy_df: pd.DataFrame, save_path: Path):\n",
    "    \"\"\"Visualize gallery strategy analysis\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Raw scores comparison\n",
    "    ax = axes[0, 0]\n",
    "    strategy_df.set_index('Model')[['Oneshot_Base', 'Oneshot_Aug', \n",
    "                                     'Fewshot_Base', 'Fewshot_Aug']].plot(\n",
    "        kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_ylabel('Rank-1 Accuracy')\n",
    "    ax.set_title('Gallery Strategy Comparison', fontweight='bold')\n",
    "    ax.legend(title='Configuration')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 2. Augmentation improvement\n",
    "    ax = axes[0, 1]\n",
    "    strategy_df.set_index('Model')[['Aug_Improvement_Oneshot', \n",
    "                                     'Aug_Improvement_Fewshot']].plot(\n",
    "        kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_ylabel('Rank-1 Improvement')\n",
    "    ax.set_title('Augmentation Benefit', fontweight='bold')\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.legend(title='Gallery Type')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 3. Fewshot improvement\n",
    "    ax = axes[1, 0]\n",
    "    strategy_df.set_index('Model')[['Fewshot_Improvement_Base', \n",
    "                                     'Fewshot_Improvement_Aug']].plot(\n",
    "        kind='bar', ax=ax, width=0.8)\n",
    "    ax.set_ylabel('Rank-1 Improvement')\n",
    "    ax.set_title('Fewshot vs Oneshot Benefit', fontweight='bold')\n",
    "    ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    ax.legend(title='Augmentation')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 4. Best configuration per model\n",
    "    ax = axes[1, 1]\n",
    "    best_configs = strategy_df.groupby('Best_Config').size()\n",
    "    best_configs.plot(kind='bar', ax=ax, color='steelblue')\n",
    "    ax.set_ylabel('Number of Models')\n",
    "    ax.set_title('Most Common Best Configuration', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.suptitle('Gallery Strategy Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Gallery strategy plot saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "023bbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_comprehensive_report(all_model_results: Dict,\n",
    "                               all_summaries: Dict,\n",
    "                               save_dir: Path):\n",
    "    \"\"\"Export complete results to multiple formats\"\"\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Excel workbook with multiple sheets\n",
    "    excel_path = save_dir / 'comprehensive_report.xlsx'\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        all_summaries['comparison_summary'].to_excel(writer, sheet_name='Overall_Comparison', index=False)\n",
    "        all_summaries['gallery_strategy'].to_excel(writer, sheet_name='Gallery_Strategy', index=False)\n",
    "        all_summaries['aggregation_analysis'].to_excel(writer, sheet_name='Aggregation_Analysis', index=False)\n",
    "        all_summaries['threshold_recommendations'].to_excel(writer, sheet_name='Threshold_Recommendations', index=False)\n",
    "        \n",
    "        if 'segmented_oneshot' in all_summaries:\n",
    "            all_summaries['segmented_oneshot'].to_excel(writer, sheet_name='Segmented_Oneshot')\n",
    "        if 'segmented_fewshot' in all_summaries:\n",
    "            all_summaries['segmented_fewshot'].to_excel(writer, sheet_name='Segmented_Fewshot')\n",
    "        if 'statistical_comparison' in all_summaries:\n",
    "            all_summaries['statistical_comparison'].to_excel(writer, sheet_name='Statistical_Tests', index=False)\n",
    "    \n",
    "    print(f\"Excel report saved: {excel_path}\")\n",
    "    \n",
    "    # 2. JSON export\n",
    "    json_data = {\n",
    "        'metadata': {\n",
    "            'generated': datetime.now().isoformat(),\n",
    "            'models_evaluated': list(all_model_results.keys())\n",
    "        },\n",
    "        'summaries': {\n",
    "            key: df.to_dict(orient='records') if isinstance(df, pd.DataFrame) else df\n",
    "            for key, df in all_summaries.items()\n",
    "            if key != 'executive_summary'\n",
    "        },\n",
    "        'executive_summary': all_summaries.get('executive_summary', '')\n",
    "    }\n",
    "    \n",
    "    json_path = save_dir / 'comprehensive_report.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "    \n",
    "    print(f\"JSON report saved: {json_path}\")\n",
    "    \n",
    "    # 3. Text summary\n",
    "    txt_path = save_dir / 'executive_summary.txt'\n",
    "    with open(txt_path, 'w') as f:\n",
    "        f.write(all_summaries.get('executive_summary', ''))\n",
    "    \n",
    "    print(f\"Text summary saved: {txt_path}\")\n",
    "    \n",
    "    # 4. LaTeX tables\n",
    "    latex_path = save_dir / 'latex_tables.tex'\n",
    "    with open(latex_path, 'w') as f:\n",
    "        f.write(\"% Comparison Summary\\n\")\n",
    "        f.write(all_summaries['comparison_summary'].to_latex(index=False, float_format=\"%.4f\"))\n",
    "        f.write(\"\\n\\n% Gallery Strategy\\n\")\n",
    "        f.write(all_summaries['gallery_strategy'].to_latex(index=False, float_format=\"%.4f\"))\n",
    "    \n",
    "    print(f\"LaTeX tables saved: {latex_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1997484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_impostor_metrics(results: Dict, title: str, save_path: Path):\n",
    "    df = results['threshold_results']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(df['threshold'], df['rejection_rate'], 'g-', linewidth=2)\n",
    "    ax.set_xlabel('Threshold')\n",
    "    ax.set_ylabel('Rejection Rate')\n",
    "    ax.set_title('Impostor Rejection Rate')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(df['threshold'], df['far'], 'r-', linewidth=2)\n",
    "    ax.set_xlabel('Threshold')\n",
    "    ax.set_ylabel('False Accept Rate')\n",
    "    ax.set_title('False Accept Rate')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    ax = axes[1, 0]\n",
    "    ax.hist(results['impostor_scores'], bins=50, alpha=0.7, color='red')\n",
    "    ax.axvline(np.mean(results['impostor_scores']), color='darkred', \n",
    "              linestyle='--', linewidth=2, label='Mean')\n",
    "    ax.axvline(results['impostor_ci'][0], color='orange', \n",
    "              linestyle=':', linewidth=2, label='95% CI')\n",
    "    ax.axvline(results['impostor_ci'][1], color='orange', \n",
    "              linestyle=':', linewidth=2)\n",
    "    ax.set_xlabel('Similarity Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Impostor Score Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    best_idx = df['rejection_rate'].idxmax()\n",
    "    best_row = df.loc[best_idx]\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    IMPOSTOR REJECTION SUMMARY\n",
    "    ==========================\n",
    "    Aggregation: {results['aggregation'].upper()}\n",
    "    \n",
    "    Best Rejection: {best_row['rejection_rate']:.4f}\n",
    "    @ Threshold: {best_row['threshold']:.3f}\n",
    "    \n",
    "    FAR at best: {best_row['far']:.4f}\n",
    "    \n",
    "    Total Impostors: {best_row['n_impostors']}\n",
    "    \n",
    "    Mean Score: {np.mean(results['impostor_scores']):.4f}\n",
    "    Std Score: {np.std(results['impostor_scores']):.4f}\n",
    "    \n",
    "    95% CI: [{results['impostor_ci'][0]:.4f}, \n",
    "             {results['impostor_ci'][1]:.4f}]\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.text(0.1, 0.5, summary_text, fontsize=11, family='monospace',\n",
    "           verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a113fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic_probe_evaluation(model_name: str, embeddings: Dict, \n",
    "                               results_dir: Path, plots_dir: Path):\n",
    "    \"\"\"Run basic probe evaluation (ORIGINAL + FIXED)\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BASIC PROBE EVALUATION: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    probe = embeddings['probe_positive_unsegmented']\n",
    "\n",
    "    if probe is None:\n",
    "        print(\"Missing probe embeddings!\")\n",
    "        return None\n",
    "\n",
    "    gallery_types = {\n",
    "        'oneshot_base': 'gallery_oneshot_base',\n",
    "        'oneshot_augmented': 'gallery_oneshot_augmented', \n",
    "        'fewshot_base': 'gallery_fewshot_base',\n",
    "        'fewshot_augmented': 'gallery_fewshot_augmented'\n",
    "    }\n",
    "        \n",
    "    thresholds = np.arange(0.2, 0.91, 0.05)\n",
    "    aggregations = ['mean']\n",
    "    \n",
    "    all_results = {}\n",
    "\n",
    "    for gallery_name, gallery_key in gallery_types.items():\n",
    "        gallery = embeddings.get(gallery_key)\n",
    "        \n",
    "        if gallery is None:\n",
    "            print(f\"Missing {gallery_name} gallery, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'-'*70}\")\n",
    "        print(f\"GALLERY: {gallery_name.upper()}\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        \n",
    "        gallery_results = {}\n",
    "    \n",
    "        for agg in aggregations:\n",
    "            print(f\"\\nEvaluating with {agg.upper()} aggregation...\")\n",
    "            results = evaluate_probes_comprehensive(\n",
    "                gallery, probe, thresholds, aggregation=agg, k=3\n",
    "            )\n",
    "            \n",
    "            # Save CSV results\n",
    "            csv_path = results_dir / model_name / f'basic_probe_{gallery_name}_{agg}_metrics.csv'\n",
    "            csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            results['threshold_results'].to_csv(csv_path, index=False)\n",
    "\n",
    "            # Generate core metrics plot\n",
    "            plot_path = plots_dir / model_name / f'basic_probe_{gallery_name}_{agg}_core.png'\n",
    "            plot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            plot_core_metrics(\n",
    "                results, \n",
    "                f\"{model_name} - Basic Probe - {gallery_name.upper()} ({agg.upper()})\", \n",
    "                plot_path\n",
    "            )\n",
    "            \n",
    "            # Generate paper figures\n",
    "            paper_figs_dir = plots_dir / model_name / 'paper_figures' / f'basic_{gallery_name}_{agg}'\n",
    "            plot_paper_figures(\n",
    "                results,\n",
    "                model_name=f\"{model_name}_{gallery_name}_{agg}\",\n",
    "                save_dir=paper_figs_dir\n",
    "            )\n",
    "                        \n",
    "            gallery_results[agg] = results\n",
    "\n",
    "            # Print summary\n",
    "            df = results['threshold_results']\n",
    "            best_idx = df['rank1_accuracy'].idxmax()\n",
    "            eer_idx = (df['far'] - df['frr']).abs().idxmin()\n",
    "            eer = (df.loc[eer_idx, 'far'] + df.loc[eer_idx, 'frr']) / 2\n",
    "            \n",
    "            print(f\"  Best Rank-1: {df.loc[best_idx, 'rank1_accuracy']:.4f} \"\n",
    "                f\"@ threshold {df.loc[best_idx, 'threshold']:.2f}\")\n",
    "            print(f\"  Rank-5: {df.loc[best_idx, 'rank5_accuracy']:.4f}\")\n",
    "            print(f\"  Rank-10: {df.loc[best_idx, 'rank10_accuracy']:.4f}\")\n",
    "            print(f\"  ROC-AUC: {results['roc_auc']:.4f}\")\n",
    "            print(f\"  EER: {eer:.4f}\")\n",
    "\n",
    "        all_results[gallery_name] = gallery_results\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8f8cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_impostor_evaluation(model_name: str, embeddings: Dict,\n",
    "                           results_dir: Path, plots_dir: Path):\n",
    "    \"\"\"Run impostor evaluation (ORIGINAL + ENHANCED OUTPUT)\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"IMPOSTOR EVALUATION: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    gallery = embeddings['gallery_oneshot_augmented']\n",
    "    impostor = embeddings['probe_negative']\n",
    "    \n",
    "    if gallery is None or impostor is None:\n",
    "        print(\"Missing embeddings!\")\n",
    "        return None\n",
    "    \n",
    "    thresholds = np.arange(0.2, 0.91, 0.05)\n",
    "    \n",
    "    print(\"\\nEvaluating with MEAN aggregation...\")\n",
    "    results = evaluate_impostors_comprehensive(\n",
    "        gallery, impostor, thresholds, aggregation='mean', k=3\n",
    "    )\n",
    "    \n",
    "    csv_path = results_dir / model_name / 'impostor_metrics.csv'\n",
    "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    results['threshold_results'].to_csv(csv_path, index=False)\n",
    "    \n",
    "    plot_path = plots_dir / model_name / 'impostor_plot.png'\n",
    "    plot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plot_impostor_metrics(results, f\"{model_name} - Impostor Rejection\", plot_path)\n",
    "\n",
    "    df = results['threshold_results']\n",
    "    best_idx = df['rejection_rate'].idxmax()\n",
    "    print(f\"  Best Rejection Rate: {df.loc[best_idx, 'rejection_rate']:.4f} \"\n",
    "          f\"@ threshold {df.loc[best_idx, 'threshold']:.2f}\")\n",
    "    print(f\"  FAR at best: {df.loc[best_idx, 'far']:.4f}\")\n",
    "    print(f\"  Total impostors: {df.loc[best_idx, 'n_impostors']}\")\n",
    "    print(f\"  Mean impostor score: {results['mean_impostor_score']:.4f}\")\n",
    "    print(f\"  Std impostor score: {results['std_impostor_score']:.4f}\")\n",
    "    print(f\"  95% CI: [{results['impostor_ci'][0]:.4f}, {results['impostor_ci'][1]:.4f}]\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "496cc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmented_evaluation(model_name: str, embeddings: Dict,\n",
    "                             results_dir: Path, plots_dir: Path,\n",
    "                             gallery_type: str):\n",
    "    \"\"\"Run segmented evaluation (FIXED - added d-prime output)\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SEGMENTED EVALUATION: {model_name} ({gallery_type})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    gallery_key = f'gallery_{gallery_type}_augmented'\n",
    "    gallery = embeddings[gallery_key]\n",
    "    probe = embeddings['probe_positive_segmented']\n",
    "    \n",
    "    if gallery is None or probe is None:\n",
    "        print(\"Missing embeddings!\")\n",
    "        return None\n",
    "    \n",
    "    thresholds = np.arange(0.2, 0.91, 0.05)\n",
    "    \n",
    "    print(\"\\nEvaluating with MEAN aggregation...\")\n",
    "    segment_results = evaluate_segmented_comprehensive(\n",
    "        gallery, probe, thresholds, aggregation='mean', k=3\n",
    "    )\n",
    "    \n",
    "    for segment_name, results in segment_results.items():\n",
    "    # Save CSV results\n",
    "        csv_path = results_dir / model_name / f'segmented_{gallery_type}_{segment_name}_metrics.csv'\n",
    "        csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        results['threshold_results'].to_csv(csv_path, index=False)\n",
    "\n",
    "        # Generate core metrics plot\n",
    "        plot_path = plots_dir / model_name / f'segmented_{gallery_type}_{segment_name}_core.png'\n",
    "        plot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plot_core_metrics(\n",
    "            results, \n",
    "            f\"{model_name} - {segment_name} ({gallery_type})\", \n",
    "            plot_path\n",
    "        )\n",
    "        \n",
    "        # Generate paper figures for this segment\n",
    "        paper_figs_dir = plots_dir / model_name / 'paper_figures' / f'segmented_{gallery_type}_{segment_name}'\n",
    "        plot_paper_figures(\n",
    "            results,\n",
    "            model_name=f\"{model_name}_{segment_name}\",\n",
    "            save_dir=paper_figs_dir\n",
    "        )\n",
    "        \n",
    "        # Print summary\n",
    "        df = results['threshold_results']\n",
    "        best_idx = df['rank1_accuracy'].idxmax()\n",
    "        eer_idx = (df['far'] - df['frr']).abs().idxmin()\n",
    "        eer = (df.loc[eer_idx, 'far'] + df.loc[eer_idx, 'frr']) / 2\n",
    "        \n",
    "        print(f\"\\n  {segment_name}:\")\n",
    "        print(f\"    Rank-1: {df.loc[best_idx, 'rank1_accuracy']:.4f} \"\n",
    "            f\"@ threshold {df.loc[best_idx, 'threshold']:.2f}\")\n",
    "        print(f\"    Rank-5: {df.loc[best_idx, 'rank5_accuracy']:.4f}\")\n",
    "        print(f\"    Rank-10: {df.loc[best_idx, 'rank10_accuracy']:.4f}\")\n",
    "        print(f\"    ROC-AUC: {results['roc_auc']:.4f}\")\n",
    "        print(f\"    EER: {eer:.4f}\")\n",
    "    \n",
    "    return segment_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c50f4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rank1_per_identity_all_models(all_model_results: Dict[str, Dict],\n",
    "                                       save_path: Path,\n",
    "                                       sort_by_average: bool = True):\n",
    "    \"\"\"\n",
    "    Plot Rank-1 per identity for all models in a single chart.\n",
    "    Expects each model to contain:\n",
    "        results[\"basic_probe\"][\"per_identity\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Gather all identities across all models ---\n",
    "    identities = set()\n",
    "    for model_name, results in all_model_results.items():\n",
    "        per_id = results[\"basic_probe\"].get(\"per_identity\", {})\n",
    "        identities.update(per_id.keys())\n",
    "    identities = list(identities)\n",
    "\n",
    "    # --- Build table: identity -> {model: rank1} ---\n",
    "    rank1_table = {idn: {} for idn in identities}\n",
    "    model_names = list(all_model_results.keys())\n",
    "\n",
    "    for model_name, results in all_model_results.items():\n",
    "        per_id = results[\"basic_probe\"].get(\"per_identity\", {})\n",
    "        for idn in identities:\n",
    "            rank1_table[idn][model_name] = per_id.get(idn, {}).get(\"rank1\", 0)\n",
    "\n",
    "    # --- Sort identities (optional but recommended) ---\n",
    "    if sort_by_average:\n",
    "        identities.sort(\n",
    "            key=lambda idn: np.mean(list(rank1_table[idn].values())),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "    # --- Plot ---\n",
    "    x = np.arange(len(identities))\n",
    "    fig, ax = plt.subplots(figsize=(22, 8))\n",
    "\n",
    "    for model_name in model_names:\n",
    "        y = [rank1_table[idn][model_name] for idn in identities]\n",
    "        ax.plot(x, y, marker='o', linewidth=2, label=model_name)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(identities, rotation=90)\n",
    "    ax.set_ylabel(\"Rank-1 Accuracy\")\n",
    "    ax.set_title(\"Rank-1 Accuracy per Identity Across Models\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(save_path, dpi=300)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30e0b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_evaluation_pipeline(all_embeddings: Dict, output_base_dir: Path):\n",
    "    \"\"\"\n",
    "    Complete evaluation pipeline with all analysis and comparisons\n",
    "    \n",
    "    Args:\n",
    "        all_embeddings: Dict with structure {model_name: embeddings_dict}\n",
    "        output_base_dir: Base directory for all outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPLETE FACE RECOGNITION EVALUATION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results_dir = output_base_dir / 'evaluation_results'\n",
    "    plots_dir = output_base_dir / 'plots'\n",
    "    comparison_dir = output_base_dir / 'comparisons'\n",
    "\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "    comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (comparison_dir / \"charts\").mkdir(parents=True, exist_ok=True)\n",
    "    (comparison_dir / \"reports\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    all_model_results = {}\n",
    "    \n",
    "    # Run individual model evaluations\n",
    "    for model_name, embeddings in all_embeddings.items():\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"# PROCESSING MODEL: {model_name}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        model_results = {}\n",
    "        \n",
    "        # 1. Basic probe evaluation\n",
    "        model_results['basic_probe'] = run_basic_probe_evaluation(\n",
    "            model_name, embeddings, results_dir, plots_dir\n",
    "        )\n",
    "        \n",
    "        # 2. Impostor evaluation\n",
    "        model_results['impostor'] = run_impostor_evaluation(\n",
    "            model_name, embeddings, results_dir, plots_dir\n",
    "        )\n",
    "        \n",
    "        # 3. Segmented evaluation - oneshot\n",
    "        model_results['segmented_oneshot'] = run_segmented_evaluation(\n",
    "            model_name, embeddings, results_dir, plots_dir, 'oneshot'\n",
    "        )\n",
    "        \n",
    "        # 4. Segmented evaluation - fewshot\n",
    "        model_results['segmented_fewshot'] = run_segmented_evaluation(\n",
    "            model_name, embeddings, results_dir, plots_dir, 'fewshot'\n",
    "        )\n",
    "        \n",
    "        all_model_results[model_name] = model_results\n",
    "    \n",
    "    # ========================================================================\n",
    "    # COMPARATIVE ANALYSIS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(\"# COMPARATIVE ANALYSIS\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    all_summaries = {}\n",
    "    \n",
    "    # 1. Generate comparison summary\n",
    "    print(\"\\n1. Generating comparison summary...\")\n",
    "    all_summaries['comparison_summary'] = generate_comparison_summary(all_model_results)\n",
    "    all_summaries['comparison_summary'].to_csv(comparison_dir / 'comparison_summary.csv', index=False)\n",
    "    print(f\"   Saved: {comparison_dir / 'comparison_summary.csv'}\")\n",
    "    \n",
    "    # 2. Gallery strategy analysis\n",
    "    print(\"\\n2. Analyzing gallery strategies...\")\n",
    "    all_summaries['gallery_strategy'] = analyze_gallery_strategies(all_model_results)\n",
    "    all_summaries['gallery_strategy'].to_csv(comparison_dir / 'gallery_strategy_analysis.csv', index=False)\n",
    "    print(f\"   Saved: {comparison_dir / 'gallery_strategy_analysis.csv'}\")\n",
    "    \n",
    "    # 3. Aggregation method analysis\n",
    "    print(\"\\n3. Analyzing aggregation methods...\")\n",
    "    all_summaries['aggregation_analysis'] = summarize_aggregation_performance(all_model_results)\n",
    "    all_summaries['aggregation_analysis'].to_csv(comparison_dir / 'aggregation_analysis.csv', index=False)\n",
    "    print(f\"   Saved: {comparison_dir / 'aggregation_analysis.csv'}\")\n",
    "    \n",
    "    # 4. Threshold recommendations\n",
    "    print(\"\\n4. Generating threshold recommendations...\")\n",
    "    all_summaries['threshold_recommendations'] = recommend_operating_thresholds(all_model_results)\n",
    "    all_summaries['threshold_recommendations'].to_csv(comparison_dir / 'threshold_recommendations.csv', index=False)\n",
    "    print(f\"   Saved: {comparison_dir / 'threshold_recommendations.csv'}\")\n",
    "    \n",
    "    # 5. Segmented comparison tables\n",
    "    print(\"\\n5. Creating segmented comparison tables...\")\n",
    "    all_summaries['segmented_oneshot'] = create_segmented_comparison_table(all_model_results, 'oneshot')\n",
    "    all_summaries['segmented_oneshot'].to_csv(comparison_dir / 'segmented_oneshot_comparison.csv')\n",
    "    print(f\"   Saved: {comparison_dir / 'segmented_oneshot_comparison.csv'}\")\n",
    "    \n",
    "    all_summaries['segmented_fewshot'] = create_segmented_comparison_table(all_model_results, 'fewshot')\n",
    "    all_summaries['segmented_fewshot'].to_csv(comparison_dir / 'segmented_fewshot_comparison.csv')\n",
    "    print(f\"   Saved: {comparison_dir / 'segmented_fewshot_comparison.csv'}\")\n",
    "    \n",
    "    # 6. Failure analysis\n",
    "    print(\"\\n6. Analyzing failure cases...\")\n",
    "    all_summaries['failure_analysis'] = analyze_failure_cases(all_model_results)\n",
    "    with open(comparison_dir / 'failure_analysis.json', 'w') as f:\n",
    "        json.dump(all_summaries['failure_analysis'], f, indent=2)\n",
    "    print(f\"   Saved: {comparison_dir / 'failure_analysis.json'}\")\n",
    "    \n",
    "    # 7. Statistical comparison\n",
    "    print(\"\\n7. Performing statistical comparisons...\")\n",
    "    all_summaries['statistical_comparison'] = compare_models_statistical(all_model_results)\n",
    "    all_summaries['statistical_comparison'].to_csv(comparison_dir / 'statistical_comparison.csv', index=False)\n",
    "    print(f\"   Saved: {comparison_dir / 'statistical_comparison.csv'}\")\n",
    "    \n",
    "    # 8. Executive summary\n",
    "    print(\"\\n8. Generating executive summary...\")\n",
    "    all_summaries['executive_summary'] = generate_executive_summary(\n",
    "        all_model_results, \n",
    "        all_summaries['comparison_summary']\n",
    "    )\n",
    "    print(all_summaries['executive_summary'])\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VISUALIZATIONS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(\"# GENERATING COMPARISON VISUALIZATIONS\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    # 1. Model comparison charts\n",
    "    print(\"\\n1. Creating model comparison charts...\")\n",
    "    plot_model_comparison_charts(\n",
    "        all_model_results,\n",
    "        all_summaries['comparison_summary'],\n",
    "        comparison_dir / 'charts'\n",
    "    )\n",
    "    print(\"\\n1b. Creating Rank-1 per-identity comparison chart...\")\n",
    "    plot_rank1_per_identity_all_models(\n",
    "        all_model_results,\n",
    "        comparison_dir / \"charts\" / \"rank1_per_identity_all_models.png\"\n",
    "    )\n",
    "    print(f\"   Saved: {comparison_dir / 'charts' / 'rank1_per_identity_all_models.png'}\")\n",
    "    \n",
    "    # 2. Segmented heatmaps\n",
    "    print(\"\\n2. Creating segmented performance heatmaps...\")\n",
    "    plot_segmented_heatmap(\n",
    "        all_summaries['segmented_oneshot'],\n",
    "        comparison_dir / 'charts' / 'segmented_oneshot_heatmap.png',\n",
    "        'Segmented Performance - Oneshot'\n",
    "    )\n",
    "    plot_segmented_heatmap(\n",
    "        all_summaries['segmented_fewshot'],\n",
    "        comparison_dir / 'charts' / 'segmented_fewshot_heatmap.png',\n",
    "        'Segmented Performance - Fewshot'\n",
    "    )\n",
    "\n",
    "    plot_sensitivity_analysis(\n",
    "        all_summaries['segmented_oneshot'],\n",
    "        comparison_dir / 'charts' / 'sensitivity_analysis_oneshot.png',\n",
    "        'Sensitivity Analysis - Oneshot'\n",
    "    )\n",
    "    plot_segmented_heatmap(\n",
    "        all_summaries['segmented_fewshot'],\n",
    "        comparison_dir / 'charts' / 'sensitivity_analysis_fewshot.png',\n",
    "        'Sensitivity Analysis - Fewshot'\n",
    "    )\n",
    "    \n",
    "    # 3. Gallery strategy visualization\n",
    "    print(\"\\n3. Creating gallery strategy visualizations...\")\n",
    "    plot_gallery_strategy_comparison(\n",
    "        all_summaries['gallery_strategy'],\n",
    "        comparison_dir / 'charts' / 'gallery_strategy_comparison.png'\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXPORT REPORTS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(\"# EXPORTING COMPREHENSIVE REPORTS\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    export_comprehensive_report(\n",
    "        all_model_results,\n",
    "        all_summaries,\n",
    "        comparison_dir / 'reports'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EVALUATION PIPELINE COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nAll results saved to: {output_base_dir}\")\n",
    "    print(f\"  - Individual results: {results_dir}\")\n",
    "    print(f\"  - Individual plots: {plots_dir}\")\n",
    "    print(f\"  - Comparisons: {comparison_dir}\")\n",
    "    print(f\"  - Reports: {comparison_dir / 'reports'}\")\n",
    "    \n",
    "    return all_model_results, all_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ae6ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl(path: Path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def load_all_embeddings(base_dir: Path):\n",
    "    \"\"\"\n",
    "    Auto-loads all embeddings following this structure:\n",
    "\n",
    "    base_dir /\n",
    "        adaface_ir_101 /\n",
    "            gallery_few-shot_augmented.pkl\n",
    "            gallery_few-shot_base.pkl\n",
    "            ...\n",
    "        arcface_ir_50 /\n",
    "            ...\n",
    "\n",
    "    Returns: dict formatted exactly for run_complete_evaluation_pipeline()\n",
    "    \"\"\"\n",
    "    mapping = {\n",
    "        \"gallery_one-shot_base.pkl\": \"gallery_oneshot_base\",\n",
    "        \"gallery_one-shot_augmented.pkl\": \"gallery_oneshot_augmented\",\n",
    "        \"gallery_few-shot_base.pkl\": \"gallery_fewshot_base\",\n",
    "        \"gallery_few-shot_augmented.pkl\": \"gallery_fewshot_augmented\",\n",
    "        \"probe_negative.pkl\": \"probe_negative\",\n",
    "        \"probe_positive_segmented.pkl\": \"probe_positive_segmented\",\n",
    "        \"probe_positive_unsegmented.pkl\": \"probe_positive_unsegmented\",\n",
    "    }\n",
    "\n",
    "    all_embeddings = {}\n",
    "\n",
    "    for model_dir in base_dir.iterdir():\n",
    "        if not model_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        model_name = model_dir.name\n",
    "        all_embeddings[model_name] = {}\n",
    "\n",
    "        for file in model_dir.iterdir():\n",
    "            if file.suffix != \".pkl\":\n",
    "                continue\n",
    "\n",
    "            key = mapping.get(file.name)\n",
    "            if key is None:\n",
    "                print(f\"Warning: Unrecognized file {file.name}, skippingâ€¦\")\n",
    "                continue\n",
    "\n",
    "            all_embeddings[model_name][key] = load_pkl(file)\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "470219b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = {}\n",
    "\n",
    "for model in models:\n",
    "    model_dir = embeddings_root / model\n",
    "    if not model_dir.exists():\n",
    "        print(f\"Warning: {model_dir} not found, skipping...\")\n",
    "        continue\n",
    "\n",
    "    all_embeddings[model] = {}\n",
    "\n",
    "    for file in model_dir.glob(\"*.pkl\"):\n",
    "        fname = file.name\n",
    "\n",
    "        # Map filename â†’ dictionary key\n",
    "        if \"one-shot_base\" in fname:\n",
    "            key = \"gallery_oneshot_base\"\n",
    "        elif \"one-shot_augmented\" in fname:\n",
    "            key = \"gallery_oneshot_augmented\"\n",
    "        elif \"few-shot_base\" in fname:\n",
    "            key = \"gallery_fewshot_base\"\n",
    "        elif \"few-shot_augmented\" in fname:\n",
    "            key = \"gallery_fewshot_augmented\"\n",
    "        else:\n",
    "            # probe_negative, probe_positive_segmented, etc.\n",
    "            key = fname.replace(\".pkl\", \"\")\n",
    "\n",
    "        with open(file, \"rb\") as f:\n",
    "            all_embeddings[model][key] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1befcc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPLETE FACE RECOGNITION EVALUATION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "################################################################################\n",
      "# PROCESSING MODEL: adaface_ir_50\n",
      "################################################################################\n",
      "\n",
      "======================================================================\n",
      "BASIC PROBE EVALUATION: adaface_ir_50\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "GALLERY: ONESHOT_BASE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Evaluating with MEAN aggregation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 108.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\basic_probe_oneshot_base_mean_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_oneshot_base_mean\\adaface_ir_50_oneshot_base_mean_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_oneshot_base_mean\\adaface_ir_50_oneshot_base_mean_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_oneshot_base_mean\\adaface_ir_50_oneshot_base_mean_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_oneshot_base_mean\\adaface_ir_50_oneshot_base_mean_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_oneshot_base_mean\n",
      "  Best Rank-1: 0.5215 @ threshold 0.20\n",
      "  Rank-5: 0.7515\n",
      "  Rank-10: 0.8681\n",
      "  ROC-AUC: 0.9141\n",
      "  EER: 0.2638\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "GALLERY: ONESHOT_AUGMENTED\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Evaluating with MEAN aggregation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 30.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\basic_probe_oneshot_augmented_mean_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_oneshot_augmented_mean\\adaface_ir_50_oneshot_augmented_mean_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_oneshot_augmented_mean\\adaface_ir_50_oneshot_augmented_mean_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_oneshot_augmented_mean\\adaface_ir_50_oneshot_augmented_mean_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_oneshot_augmented_mean\\adaface_ir_50_oneshot_augmented_mean_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_oneshot_augmented_mean\n",
      "  Best Rank-1: 0.5521 @ threshold 0.20\n",
      "  Rank-5: 0.7607\n",
      "  Rank-10: 0.8650\n",
      "  ROC-AUC: 0.9020\n",
      "  EER: 0.2531\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "GALLERY: FEWSHOT_BASE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Evaluating with MEAN aggregation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 52.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\basic_probe_fewshot_base_mean_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_fewshot_base_mean\\adaface_ir_50_fewshot_base_mean_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_fewshot_base_mean\\adaface_ir_50_fewshot_base_mean_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_fewshot_base_mean\\adaface_ir_50_fewshot_base_mean_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_fewshot_base_mean\\adaface_ir_50_fewshot_base_mean_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_fewshot_base_mean\n",
      "  Best Rank-1: 0.5890 @ threshold 0.20\n",
      "  Rank-5: 0.8129\n",
      "  Rank-10: 0.8926\n",
      "  ROC-AUC: 0.9267\n",
      "  EER: 0.2439\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "GALLERY: FEWSHOT_AUGMENTED\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Evaluating with MEAN aggregation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:02<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\basic_probe_fewshot_augmented_mean_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_fewshot_augmented_mean\\adaface_ir_50_fewshot_augmented_mean_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_fewshot_augmented_mean\\adaface_ir_50_fewshot_augmented_mean_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_fewshot_augmented_mean\\adaface_ir_50_fewshot_augmented_mean_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_fewshot_augmented_mean\\adaface_ir_50_fewshot_augmented_mean_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\basic_fewshot_augmented_mean\n",
      "  Best Rank-1: 0.5982 @ threshold 0.20\n",
      "  Rank-5: 0.8037\n",
      "  Rank-10: 0.8988\n",
      "  ROC-AUC: 0.9153\n",
      "  EER: 0.2408\n",
      "\n",
      "======================================================================\n",
      "IMPOSTOR EVALUATION: adaface_ir_50\n",
      "======================================================================\n",
      "\n",
      "Evaluating with MEAN aggregation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing impostors (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\impostor_plot.png\n",
      "  Best Rejection Rate: 1.0000 @ threshold 0.35\n",
      "  FAR at best: 0.0000\n",
      "  Total impostors: 253\n",
      "  Mean impostor score: 0.1100\n",
      "  Std impostor score: 0.0517\n",
      "  95% CI: [0.1042, 0.1166]\n",
      "\n",
      "======================================================================\n",
      "SEGMENTED EVALUATION: adaface_ir_50 (oneshot)\n",
      "======================================================================\n",
      "\n",
      "Evaluating with MEAN aggregation...\n",
      "Found 10 segments: ['baseline', 'blur_blurry', 'blur_sharp', 'face_large', 'face_medium', 'face_small', 'pose_easy', 'pose_medium', 'pose_hard', 'low_quality']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 214.61it/s]\n",
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 160.31it/s]\n",
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 47.89it/s]]\n",
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 193.61it/s]s]\n",
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 90.34it/s]]\n",
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 43.01it/s]]\n",
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 76.11it/s]]\n",
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 86.22it/s]]\n",
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 75.46it/s]]\n",
      "Processing probes (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 63.36it/s]]\n",
      "Processing segments (mean): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:04<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\segmented_oneshot_baseline_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_baseline\\adaface_ir_50_baseline_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_baseline\\adaface_ir_50_baseline_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_baseline\\adaface_ir_50_baseline_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_baseline\\adaface_ir_50_baseline_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_baseline\n",
      "\n",
      "  baseline:\n",
      "    Rank-1: 0.8276 @ threshold 0.20\n",
      "    Rank-5: 1.0000\n",
      "    Rank-10: 1.0000\n",
      "    ROC-AUC: 0.9083\n",
      "    EER: 0.1207\n",
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\segmented_oneshot_blur_blurry_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_blur_blurry\\adaface_ir_50_blur_blurry_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_blur_blurry\\adaface_ir_50_blur_blurry_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_blur_blurry\\adaface_ir_50_blur_blurry_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_blur_blurry\\adaface_ir_50_blur_blurry_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_blur_blurry\n",
      "\n",
      "  blur_blurry:\n",
      "    Rank-1: 0.2727 @ threshold 0.20\n",
      "    Rank-5: 0.5000\n",
      "    Rank-10: 0.6364\n",
      "    ROC-AUC: 0.9219\n",
      "    EER: 0.3750\n",
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\segmented_oneshot_blur_sharp_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_blur_sharp\\adaface_ir_50_blur_sharp_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_blur_sharp\\adaface_ir_50_blur_sharp_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_blur_sharp\\adaface_ir_50_blur_sharp_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_blur_sharp\\adaface_ir_50_blur_sharp_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_blur_sharp\n",
      "\n",
      "  blur_sharp:\n",
      "    Rank-1: 0.6439 @ threshold 0.20\n",
      "    Rank-5: 0.8439\n",
      "    Rank-10: 0.9268\n",
      "    ROC-AUC: 0.9140\n",
      "    EER: 0.2024\n",
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\segmented_oneshot_face_large_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_large\\adaface_ir_50_face_large_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_large\\adaface_ir_50_face_large_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_large\\adaface_ir_50_face_large_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_large\\adaface_ir_50_face_large_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_large\n",
      "\n",
      "  face_large:\n",
      "    Rank-1: 0.7143 @ threshold 0.20\n",
      "    Rank-5: 0.9286\n",
      "    Rank-10: 0.9286\n",
      "    ROC-AUC: 0.9000\n",
      "    EER: 0.1786\n",
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\segmented_oneshot_face_medium_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_medium\\adaface_ir_50_face_medium_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_medium\\adaface_ir_50_face_medium_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_medium\\adaface_ir_50_face_medium_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_medium\\adaface_ir_50_face_medium_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_medium\n",
      "\n",
      "  face_medium:\n",
      "    Rank-1: 0.6505 @ threshold 0.20\n",
      "    Rank-5: 0.8738\n",
      "    Rank-10: 0.9417\n",
      "    ROC-AUC: 0.9245\n",
      "    EER: 0.1893\n",
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\segmented_oneshot_face_small_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_small\\adaface_ir_50_face_small_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_small\\adaface_ir_50_face_small_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_small\\adaface_ir_50_face_small_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_small\\adaface_ir_50_face_small_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_face_small\n",
      "\n",
      "  face_small:\n",
      "    Rank-1: 0.4928 @ threshold 0.20\n",
      "    Rank-5: 0.6938\n",
      "    Rank-10: 0.8230\n",
      "    ROC-AUC: 0.8790\n",
      "    EER: 0.2895\n",
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\segmented_oneshot_pose_easy_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_easy\\adaface_ir_50_pose_easy_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_easy\\adaface_ir_50_pose_easy_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_easy\\adaface_ir_50_pose_easy_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_easy\\adaface_ir_50_pose_easy_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_easy\n",
      "\n",
      "  pose_easy:\n",
      "    Rank-1: 0.7241 @ threshold 0.20\n",
      "    Rank-5: 0.8621\n",
      "    Rank-10: 0.9483\n",
      "    ROC-AUC: 0.8973\n",
      "    EER: 0.1724\n",
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\segmented_oneshot_pose_medium_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_medium\\adaface_ir_50_pose_medium_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_medium\\adaface_ir_50_pose_medium_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_medium\\adaface_ir_50_pose_medium_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_medium\\adaface_ir_50_pose_medium_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_medium\n",
      "\n",
      "  pose_medium:\n",
      "    Rank-1: 0.5349 @ threshold 0.20\n",
      "    Rank-5: 0.7907\n",
      "    Rank-10: 0.9070\n",
      "    ROC-AUC: 0.8717\n",
      "    EER: 0.2674\n",
      "Core metrics plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\segmented_oneshot_pose_hard_core.png\n",
      "CMC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_hard\\adaface_ir_50_pose_hard_cmc_curve.png\n",
      "ROC curve saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_hard\\adaface_ir_50_pose_hard_roc_curve.png\n",
      "TAR@FAR plot saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_hard\\adaface_ir_50_pose_hard_tar_at_far.png\n",
      "Score distribution saved: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_hard\\adaface_ir_50_pose_hard_score_distribution.png\n",
      "\n",
      "All paper figures saved to: d:\\KEVIN\\0SLC\\RIG\\output\\v1\\plots\\adaface_ir_50\\paper_figures\\segmented_oneshot_pose_hard\n",
      "\n",
      "  pose_hard:\n",
      "    Rank-1: 0.4032 @ threshold 0.20\n",
      "    Rank-5: 0.6452\n",
      "    Rank-10: 0.7581\n",
      "    ROC-AUC: 0.9168\n",
      "    EER: 0.3185\n"
     ]
    }
   ],
   "source": [
    "all_results, all_summaries = run_complete_evaluation_pipeline(\n",
    "    all_embeddings,\n",
    "    output_root\n",
    ")\n",
    "\n",
    "print(all_summaries['executive_summary'])\n",
    "print(all_summaries['comparison_summary'])\n",
    "print(all_summaries['gallery_strategy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
