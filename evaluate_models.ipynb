{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parents[2]\n",
    "\n",
    "OUTPUT_ROOT = PROJECT_ROOT / \"output\" / \"v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root(start: Path, target_folder=\"RIG\"):\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if parent.name == target_folder:\n",
    "            return parent\n",
    "    raise RuntimeError(f\"Could not find project root '{target_folder}'\")\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = find_project_root(NOTEBOOK_DIR)\n",
    "OUTPUT_ROOT = PROJECT_ROOT / \"output\" / \"v0\"\n",
    "\n",
    "print(NOTEBOOK_DIR)\n",
    "print(PROJECT_ROOT)\n",
    "print(OUTPUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "output_root = Path(OUTPUT_ROOT)\n",
    "embeddings_root = output_root / 'embeddings'\n",
    "results_dir = output_root / 'evaluation_results'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "models = [\n",
    "  'adaface_ir_50',\n",
    "  'adaface_ir_101',\n",
    "  'arcface_ir_50',\n",
    "  'arcface_ir_101'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model_name: str) -> Dict:\n",
    "  model_dir = embeddings_root / model_name\n",
    "  \n",
    "  if not model_dir.exists():\n",
    "    raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n",
    "  \n",
    "  embeddings = {\n",
    "    'gallery_oneshot_base': None,\n",
    "    'gallery_oneshot_augmented': None,\n",
    "    'gallery_fewshot_base': None,\n",
    "    'gallery_fewshot_augmented': None,\n",
    "    'probe_positive_unsegmented': None,\n",
    "    'probe_positive_segmented': None,\n",
    "    'probe_negative': None\n",
    "  }\n",
    "\n",
    "  oneshot_base_path = model_dir / 'gallery_one-shot_base.pkl'\n",
    "  if oneshot_base_path.exists():\n",
    "    with open(oneshot_base_path, 'rb') as f:\n",
    "      embeddings['gallery_oneshot_base'] = pickle.load(f)\n",
    "  \n",
    "  oneshot_aug_path = model_dir / 'gallery_one-shot_augmented.pkl'\n",
    "  if oneshot_aug_path.exists():\n",
    "    with open(oneshot_aug_path, 'rb') as f:\n",
    "      embeddings['gallery_oneshot_augmented'] = pickle.load(f)\n",
    "\n",
    "  fewshot_base_path = model_dir / 'gallery_few-shot_base.pkl'\n",
    "  if fewshot_base_path.exists():\n",
    "    with open(fewshot_base_path, 'rb') as f:\n",
    "      embeddings['gallery_fewshot_base'] = pickle.load(f)\n",
    "  \n",
    "  fewshot_aug_path = model_dir / 'gallery_few-shot_augmented.pkl'\n",
    "  if fewshot_aug_path.exists():\n",
    "    with open(fewshot_aug_path, 'rb') as f:\n",
    "      embeddings['gallery_fewshot_augmented'] = pickle.load(f)\n",
    "\n",
    "  probe_pos_unseg_path = model_dir / 'probe_positive_unsegmented.pkl'\n",
    "  if probe_pos_unseg_path.exists():\n",
    "    with open(probe_pos_unseg_path, 'rb') as f:\n",
    "      embeddings['probe_positive_unsegmented'] = pickle.load(f)\n",
    "\n",
    "  probe_pos_seg_path = model_dir / 'probe_positive_segmented.pkl'\n",
    "  if probe_pos_seg_path.exists():\n",
    "    with open(probe_pos_seg_path, 'rb') as f:\n",
    "      embeddings['probe_positive_segmented'] = pickle.load(f)\n",
    "\n",
    "  probe_neg_path = model_dir / 'probe_negative.pkl'\n",
    "  if probe_neg_path.exists():\n",
    "    with open(probe_neg_path, 'rb') as f:\n",
    "      embeddings['probe_negative'] = pickle.load(f)\n",
    "  \n",
    "  return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8d37b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "    norm1 = np.linalg.norm(emb1)\n",
    "    norm2 = np.linalg.norm(emb2)\n",
    "    if abs(norm1 - 1.0) < 0.01 and abs(norm2 - 1.0) < 0.01:\n",
    "        return np.dot(emb1, emb2)\n",
    "    return np.dot(emb1, emb2) / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06947225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_probe(probe_embedding: np.ndarray, \n",
    "                   gallery_embeddings: Dict[str, Dict],\n",
    "                   threshold: float) -> Tuple[str, float]:\n",
    "  max_similarity = -1\n",
    "  predicted_name = None\n",
    "  \n",
    "  for name, data in gallery_embeddings.items():\n",
    "    gallery_embs = data['embeddings']\n",
    "\n",
    "    for gallery_emb in gallery_embs:\n",
    "      similarity = cosine_similarity(probe_embedding, gallery_emb)\n",
    "      if similarity > max_similarity:\n",
    "        max_similarity = similarity\n",
    "        predicted_name = name\n",
    "\n",
    "  if max_similarity < threshold:\n",
    "    return None, max_similarity\n",
    "  \n",
    "  return predicted_name, max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedce8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_probe_mean(probe_embedding: np.ndarray, \n",
    "                        gallery_embeddings: Dict[str, Dict],\n",
    "                        threshold: float) -> Tuple[str, float]:\n",
    "    max_similarity = -1\n",
    "    predicted_name = None\n",
    "    \n",
    "    for name, data in gallery_embeddings.items():\n",
    "        gallery_embs = data['embeddings']\n",
    "\n",
    "        similarities = [cosine_similarity(probe_embedding, gallery_emb) \n",
    "                       for gallery_emb in gallery_embs]\n",
    "        mean_similarity = np.mean(similarities)\n",
    "        \n",
    "        if mean_similarity > max_similarity:\n",
    "            max_similarity = mean_similarity\n",
    "            predicted_name = name\n",
    "    \n",
    "    if max_similarity < threshold:\n",
    "        return None, max_similarity\n",
    "    \n",
    "    return predicted_name, max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_probe_topk(probe_embedding: np.ndarray, \n",
    "                        gallery_embeddings: Dict[str, Dict],\n",
    "                        threshold: float,\n",
    "                        k: int = 3) -> Tuple[str, float]:\n",
    "    max_similarity = -1\n",
    "    predicted_name = None\n",
    "    \n",
    "    for name, data in gallery_embeddings.items():\n",
    "        gallery_embs = data['embeddings']\n",
    "        \n",
    "        similarities = [cosine_similarity(probe_embedding, gallery_emb) \n",
    "                       for gallery_emb in gallery_embs]\n",
    "        topk_similarity = np.mean(sorted(similarities, reverse=True)[:k])\n",
    "        \n",
    "        if topk_similarity > max_similarity:\n",
    "            max_similarity = topk_similarity\n",
    "            predicted_name = name\n",
    "    \n",
    "    if max_similarity < threshold:\n",
    "        return None, max_similarity\n",
    "    \n",
    "    return predicted_name, max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_impostor_metrics(results_df: pd.DataFrame, title: str, save_path: Path):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(results_df['threshold'], results_df['rejection_rate'], 'g-', linewidth=2, label='Rejection Rate (TNR)')\n",
    "    ax.set_xlabel('Threshold')\n",
    "    ax.set_ylabel('Rate')\n",
    "    ax.set_title('Impostor Rejection Rate')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(results_df['threshold'], results_df['false_acceptance_rate'], 'r-', linewidth=2, label='False Acceptance Rate')\n",
    "    ax.set_xlabel('Threshold')\n",
    "    ax.set_ylabel('Rate')\n",
    "    ax.set_title('False Acceptance Rate (Impostors)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(results_df['threshold'], results_df['rejection_rate'], 'g-', linewidth=2, label='Rejection Rate')\n",
    "    ax.plot(results_df['threshold'], results_df['false_acceptance_rate'], 'r-', linewidth=2, label='FAR')\n",
    "    ax.set_xlabel('Threshold')\n",
    "    ax.set_ylabel('Rate')\n",
    "    ax.set_title('Rejection vs False Acceptance Trade-off')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(results_df['threshold'], results_df['avg_impostor_similarity'], 'b-', linewidth=2, label='Avg All')\n",
    "    ax.plot(results_df['threshold'], results_df['avg_accepted_similarity'], 'r--', linewidth=2, label='Avg Accepted (FP)')\n",
    "    ax.plot(results_df['threshold'], results_df['avg_rejected_similarity'], 'g--', linewidth=2, label='Avg Rejected (TN)')\n",
    "    ax.set_xlabel('Threshold')\n",
    "    ax.set_ylabel('Similarity')\n",
    "    ax.set_title('Impostor Similarity Distributions')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(results_df: pd.DataFrame, model_name: str, save_path: Path = None):\n",
    "  fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "  \n",
    "  axes[0, 0].plot(results_df['threshold'], results_df['rank1_accuracy'], 'b-', linewidth=2)\n",
    "  axes[0, 0].set_xlabel('Threshold')\n",
    "  axes[0, 0].set_ylabel('Rank-1 Accuracy')\n",
    "  axes[0, 0].set_title(f'{model_name}: Rank-1 Accuracy vs Threshold')\n",
    "  axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "  axes[0, 1].plot(results_df['threshold'], results_df['false_acceptance_rate'], \n",
    "                  'r-', linewidth=2, label='FAR')\n",
    "  axes[0, 1].plot(results_df['threshold'], results_df['false_rejection_rate'], \n",
    "                  'g-', linewidth=2, label='FRR')\n",
    "  axes[0, 1].set_xlabel('Threshold')\n",
    "  axes[0, 1].set_ylabel('Rate')\n",
    "  axes[0, 1].set_title(f'{model_name}: FAR and FRR vs Threshold')\n",
    "  axes[0, 1].legend()\n",
    "  axes[0, 1].grid(True, alpha=0.3)\n",
    "  \n",
    "  axes[1, 0].plot(results_df['threshold'], results_df['precision'], \n",
    "                  'b-', linewidth=2, label='Precision')\n",
    "  axes[1, 0].plot(results_df['threshold'], results_df['recall'], \n",
    "                  'orange', linewidth=2, label='Recall')\n",
    "  axes[1, 0].plot(results_df['threshold'], results_df['f1_score'], \n",
    "                  'purple', linewidth=2, label='F1-Score')\n",
    "  axes[1, 0].set_xlabel('Threshold')\n",
    "  axes[1, 0].set_ylabel('Score')\n",
    "  axes[1, 0].set_title(f'{model_name}: Precision, Recall, F1 vs Threshold')\n",
    "  axes[1, 0].legend()\n",
    "  axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "  axes[1, 1].plot(results_df['threshold'], results_df['avg_correct_similarity'], \n",
    "                  'g-', linewidth=2, label='Correct Matches')\n",
    "  axes[1, 1].plot(results_df['threshold'], results_df['avg_incorrect_similarity'], \n",
    "                  'r-', linewidth=2, label='Incorrect Matches')\n",
    "  axes[1, 1].axhline(y=results_df['avg_similarity'].iloc[0], \n",
    "                      color='black', linestyle='--', alpha=0.5, label='Overall Avg')\n",
    "  axes[1, 1].set_xlabel('Threshold')\n",
    "  axes[1, 1].set_ylabel('Cosine Similarity')\n",
    "  axes[1, 1].set_title(f'{model_name}: Average Similarities')\n",
    "  axes[1, 1].legend()\n",
    "  axes[1, 1].grid(True, alpha=0.3)\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  \n",
    "  if save_path:\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d9fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_identification(gallery_embeddings: Dict[str, Dict],\n",
    "                          probe_embeddings: Dict[str, Dict],\n",
    "                          thresholds: List[float],\n",
    "                          aggregation: str = 'max',\n",
    "                          topk: int = 3) -> pd.DataFrame:\n",
    "    if aggregation == 'mean':\n",
    "        identify_func = identify_probe_mean\n",
    "    elif aggregation == 'topk':\n",
    "        identify_func = lambda probe_emb, gallery, thresh: identify_probe_topk(\n",
    "            probe_emb, gallery, thresh, k=topk\n",
    "        )\n",
    "    else:\n",
    "        identify_func = identify_probe\n",
    "    \n",
    "    results = []\n",
    "    probe_data = probe_embeddings.get(\"all\", probe_embeddings)\n",
    "    \n",
    "    for threshold in tqdm(thresholds, desc=f\"Evaluating thresholds ({aggregation})\"):\n",
    "        true_positives = 0\n",
    "        false_positives = 0 \n",
    "        false_negatives = 0 \n",
    "        true_negatives = 0 \n",
    "        \n",
    "        total_probes = 0\n",
    "        similarities = []\n",
    "        correct_similarities = []\n",
    "        incorrect_similarities = []\n",
    "        \n",
    "        for true_name, data in probe_data.items():\n",
    "            probe_embs = data['embeddings']\n",
    "            \n",
    "            for probe_emb in probe_embs:\n",
    "                total_probes += 1\n",
    "                predicted_name, similarity = identify_func(\n",
    "                    probe_emb, gallery_embeddings, threshold\n",
    "                )\n",
    "                \n",
    "                similarities.append(similarity)\n",
    "                \n",
    "                if predicted_name is None:\n",
    "                    false_negatives += 1\n",
    "                elif predicted_name == true_name:\n",
    "                    true_positives += 1\n",
    "                    correct_similarities.append(similarity)\n",
    "                else:\n",
    "                    false_positives += 1\n",
    "                    incorrect_similarities.append(similarity)\n",
    "        \n",
    "        rank1_accuracy = true_positives / total_probes if total_probes > 0 else 0\n",
    "        \n",
    "        identification_rate = (true_positives) / total_probes if total_probes > 0 else 0\n",
    "\n",
    "        false_acceptance_rate = false_positives / total_probes if total_probes > 0 else 0\n",
    "\n",
    "        false_rejection_rate = false_negatives / total_probes if total_probes > 0 else 0\n",
    "\n",
    "        identified = true_positives + false_positives\n",
    "        precision = true_positives / identified if identified > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'rank1_accuracy': rank1_accuracy,\n",
    "            'identification_rate': identification_rate,\n",
    "            'false_acceptance_rate': false_acceptance_rate,\n",
    "            'false_rejection_rate': false_rejection_rate,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'true_positives': true_positives,\n",
    "            'false_positives': false_positives,\n",
    "            'false_negatives': false_negatives,\n",
    "            'total_probes': total_probes,\n",
    "            'avg_similarity': np.mean(similarities),\n",
    "            'avg_correct_similarity': np.mean(correct_similarities) if correct_similarities else 0,\n",
    "            'avg_incorrect_similarity': np.mean(incorrect_similarities) if incorrect_similarities else 0,\n",
    "            'aggregation': aggregation\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_impostors(gallery_embeddings: Dict[str, Dict],\n",
    "                       impostor_embeddings: Dict[str, Dict],\n",
    "                       thresholds: List[float],\n",
    "                       aggregation: str = 'max',\n",
    "                       topk: int = 3) -> pd.DataFrame:\n",
    "    if aggregation == 'mean':\n",
    "        identify_func = identify_probe_mean\n",
    "    elif aggregation == 'topk':\n",
    "        identify_func = lambda probe_emb, gallery, thresh: identify_probe_topk(\n",
    "            probe_emb, gallery, thresh, k=topk\n",
    "        )\n",
    "    else:\n",
    "        identify_func = identify_probe\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for threshold in tqdm(thresholds, desc=f\"Evaluating impostors ({aggregation})\"):\n",
    "        true_negatives = 0\n",
    "        false_positives = 0\n",
    "        \n",
    "        total_impostors = 0\n",
    "        impostor_similarities = []\n",
    "        accepted_impostor_similarities = []\n",
    "        rejected_impostor_similarities = []\n",
    "        \n",
    "        # Iterate over dataset types (e.g., 'real', 'lfw')\n",
    "        for dataset_name, data in impostor_embeddings.items():\n",
    "            impostor_embs = data['embeddings']\n",
    "            impostor_files = data['filenames']\n",
    "            \n",
    "            # Each embedding is from a different impostor\n",
    "            for idx, impostor_emb in enumerate(impostor_embs):\n",
    "                total_impostors += 1\n",
    "                predicted_name, similarity = identify_func(\n",
    "                    impostor_emb, gallery_embeddings, threshold\n",
    "                )\n",
    "                \n",
    "                impostor_similarities.append(similarity)\n",
    "\n",
    "                if predicted_name is None:\n",
    "                    # Correctly rejected (true negative)\n",
    "                    true_negatives += 1\n",
    "                    rejected_impostor_similarities.append(similarity)\n",
    "                else:\n",
    "                    # Incorrectly accepted (false positive)\n",
    "                    false_positives += 1\n",
    "                    accepted_impostor_similarities.append(similarity)\n",
    "        \n",
    "        rejection_rate = true_negatives / total_impostors if total_impostors > 0 else 0\n",
    "        false_acceptance_rate = false_positives / total_impostors if total_impostors > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'rejection_rate': rejection_rate,\n",
    "            'false_acceptance_rate': false_acceptance_rate,\n",
    "            'true_negatives': true_negatives,\n",
    "            'false_positives': false_positives,\n",
    "            'total_impostors': total_impostors,\n",
    "            'avg_impostor_similarity': np.mean(impostor_similarities) if impostor_similarities else 0,\n",
    "            'avg_accepted_similarity': np.mean(accepted_impostor_similarities) if accepted_impostor_similarities else 0,\n",
    "            'avg_rejected_similarity': np.mean(rejected_impostor_similarities) if rejected_impostor_similarities else 0,\n",
    "            'aggregation': aggregation\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmented_probes(gallery_embeddings: Dict[str, Dict],\n",
    "                              probe_embeddings: Dict[str, Dict],\n",
    "                              thresholds: List[float],\n",
    "                              aggregation: str = 'max',\n",
    "                              topk: int = 3) -> Dict[str, pd.DataFrame]:\n",
    "    if aggregation == 'mean':\n",
    "        identify_func = identify_probe_mean\n",
    "    elif aggregation == 'topk':\n",
    "        identify_func = lambda probe_emb, gallery, thresh: identify_probe_topk(\n",
    "            probe_emb, gallery, thresh, k=topk\n",
    "        )\n",
    "    else:\n",
    "        identify_func = identify_probe\n",
    "    \n",
    "    segment_results = {}\n",
    "    segments = [k for k in probe_embeddings.keys() if k != 'all']\n",
    "    \n",
    "    if not segments:\n",
    "        print(\"Warning: No segments found in probe embeddings\")\n",
    "        return segment_results\n",
    "    \n",
    "    print(f\"Found {len(segments)} segments: {segments}\")\n",
    "    \n",
    "    for segment_name in segments:\n",
    "        segment_data = probe_embeddings[segment_name]\n",
    "        results = []\n",
    "        \n",
    "        for threshold in tqdm(thresholds, desc=f\"Evaluating {segment_name} ({aggregation})\", leave=False):\n",
    "            true_positives = 0\n",
    "            false_positives = 0\n",
    "            false_negatives = 0\n",
    "            \n",
    "            total_probes = 0\n",
    "            similarities = []\n",
    "            correct_similarities = []\n",
    "            incorrect_similarities = []\n",
    "            \n",
    "            for true_name, data in segment_data.items():\n",
    "                probe_embs = data['embeddings']\n",
    "                \n",
    "                for probe_emb in probe_embs:\n",
    "                    total_probes += 1\n",
    "                    predicted_name, similarity = identify_func(\n",
    "                        probe_emb, gallery_embeddings, threshold\n",
    "                    )\n",
    "                    \n",
    "                    similarities.append(similarity)\n",
    "                    \n",
    "                    if predicted_name is None:\n",
    "                        false_negatives += 1\n",
    "                    elif predicted_name == true_name:\n",
    "                        true_positives += 1\n",
    "                        correct_similarities.append(similarity)\n",
    "                    else:\n",
    "                        false_positives += 1\n",
    "                        incorrect_similarities.append(similarity)\n",
    "            \n",
    "            rank1_accuracy = true_positives / total_probes if total_probes > 0 else 0\n",
    "            identification_rate = true_positives / total_probes if total_probes > 0 else 0\n",
    "            false_acceptance_rate = false_positives / total_probes if total_probes > 0 else 0\n",
    "            false_rejection_rate = false_negatives / total_probes if total_probes > 0 else 0\n",
    "            \n",
    "            identified = true_positives + false_positives\n",
    "            precision = true_positives / identified if identified > 0 else 0\n",
    "            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'threshold': threshold,\n",
    "                'segment': segment_name,\n",
    "                'rank1_accuracy': rank1_accuracy,\n",
    "                'identification_rate': identification_rate,\n",
    "                'false_acceptance_rate': false_acceptance_rate,\n",
    "                'false_rejection_rate': false_rejection_rate,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1_score,\n",
    "                'true_positives': true_positives,\n",
    "                'false_positives': false_positives,\n",
    "                'false_negatives': false_negatives,\n",
    "                'total_probes': total_probes,\n",
    "                'avg_similarity': np.mean(similarities),\n",
    "                'avg_correct_similarity': np.mean(correct_similarities) if correct_similarities else 0,\n",
    "                'avg_incorrect_similarity': np.mean(incorrect_similarities) if incorrect_similarities else 0,\n",
    "                'aggregation': aggregation\n",
    "            })\n",
    "        \n",
    "        segment_results[segment_name] = pd.DataFrame(results)\n",
    "    \n",
    "    return segment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_impostor_evaluation(model_name: str, embeddings: Dict,\n",
    "                           results_dir: Path, gallery_type: str = 'base',\n",
    "                           impostor_type: str = 'real',\n",
    "                           thresholds: np.ndarray = None,\n",
    "                           aggregations: List[str] = ['max']):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.2, 0.91, 0.05)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {model_name} - Impostor ({impostor_type.upper()}, {gallery_type.upper()} gallery)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get the appropriate gallery\n",
    "    if gallery_type == 'oneshot_base':\n",
    "        gallery = embeddings.get('gallery_oneshot_base')\n",
    "    elif gallery_type == 'oneshot_aug':\n",
    "        gallery = embeddings.get('gallery_oneshot_augmented')\n",
    "    elif gallery_type == 'fewshot_base':\n",
    "        gallery = embeddings.get('gallery_fewshot_base')\n",
    "    elif gallery_type == 'fewshot_aug':\n",
    "        gallery = embeddings.get('gallery_fewshot_augmented')\n",
    "    else:\n",
    "        gallery = embeddings.get(f'gallery_{gallery_type}')\n",
    "\n",
    "    impostor_embeddings_full = embeddings.get('probe_negative')\n",
    "    \n",
    "    if gallery is None or impostor_embeddings_full is None:\n",
    "        print(f\"Missing embeddings for impostor evaluation\")\n",
    "        return None\n",
    "\n",
    "    # Filter impostor embeddings based on type\n",
    "    if impostor_type == 'real':\n",
    "        # Use only 'real' dataset\n",
    "        if 'real' not in impostor_embeddings_full:\n",
    "            print(f\"No 'real' impostors found in probe_negative\")\n",
    "            return None\n",
    "        impostor_embeddings = {'real': impostor_embeddings_full['real']}\n",
    "        print(f\"Using {len(impostor_embeddings_full['real']['embeddings'])} REAL impostors\")\n",
    "        \n",
    "    elif impostor_type == 'lfw':\n",
    "        # Use only 'lfw' dataset\n",
    "        if 'lfw' not in impostor_embeddings_full:\n",
    "            print(f\"No 'lfw' impostors found in probe_negative\")\n",
    "            return None\n",
    "        impostor_embeddings = {'lfw': impostor_embeddings_full['lfw']}\n",
    "        print(f\"Using {len(impostor_embeddings_full['lfw']['embeddings'])} LFW impostors\")\n",
    "        \n",
    "    elif impostor_type == 'combined':\n",
    "        # Use both datasets\n",
    "        impostor_embeddings = {}\n",
    "        total_count = 0\n",
    "        if 'real' in impostor_embeddings_full:\n",
    "            impostor_embeddings['real'] = impostor_embeddings_full['real']\n",
    "            total_count += len(impostor_embeddings_full['real']['embeddings'])\n",
    "        if 'lfw' in impostor_embeddings_full:\n",
    "            impostor_embeddings['lfw'] = impostor_embeddings_full['lfw']\n",
    "            total_count += len(impostor_embeddings_full['lfw']['embeddings'])\n",
    "        print(f\"Using {total_count} combined impostors (real + lfw)\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Unknown impostor_type: {impostor_type}. Use 'real', 'lfw', or 'combined'\")\n",
    "        return None\n",
    "    \n",
    "    model_results_dir = results_dir / model_name\n",
    "    model_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for agg in aggregations:\n",
    "        print(f\"\\n--- Testing aggregation: {agg.upper()} ---\")\n",
    "        results_df = evaluate_impostors(gallery, impostor_embeddings, thresholds, aggregation=agg)\n",
    "        all_results[agg] = results_df\n",
    "\n",
    "        csv_path = model_results_dir / f'impostor_{impostor_type}_{gallery_type}_{agg}_metrics.csv'\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "        print(f\"Results saved to: {csv_path}\")\n",
    "\n",
    "        plot_path = model_results_dir / f'impostor_{impostor_type}_{gallery_type}_{agg}_plot.png'\n",
    "        plot_impostor_metrics(results_df, \n",
    "                             f\"{model_name} - Impostor ({impostor_type}, {gallery_type}) - {agg.upper()}\", \n",
    "                             plot_path)\n",
    "\n",
    "        best_rejection_idx = results_df['rejection_rate'].idxmax()\n",
    "        best_rejection = results_df.iloc[best_rejection_idx]\n",
    "        print(f\"Best Rejection Rate ({agg}): {best_rejection['rejection_rate']:.4f} at threshold {best_rejection['threshold']:.2f}\")\n",
    "        print(f\"  FAR at this threshold: {best_rejection['false_acceptance_rate']:.4f}\")\n",
    "        print(f\"  Total impostors tested: {int(best_rejection['total_impostors'])}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_segmented_evaluation(model_name: str, embeddings: Dict,\n",
    "                             results_dir: Path, gallery_type: str = 'base',\n",
    "                             thresholds: np.ndarray = None,\n",
    "                             aggregations: List[str] = ['max']):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.2, 0.91, 0.05)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {model_name} - Segmented Probes ({gallery_type.upper()} gallery)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if gallery_type == 'oneshot_base':\n",
    "        gallery = embeddings.get('gallery_oneshot_base')\n",
    "    elif gallery_type == 'oneshot_augmented':\n",
    "        gallery = embeddings.get('gallery_oneshot_augmented')\n",
    "    elif gallery_type == 'fewshot_base':\n",
    "        gallery = embeddings.get('gallery_fewshot_base')\n",
    "    elif gallery_type == 'fewshot_augmented':\n",
    "        gallery = embeddings.get('gallery_fewshot_augmented')\n",
    "    else:\n",
    "        gallery = embeddings.get(f'gallery_{gallery_type}')\n",
    "    \n",
    "    probe = embeddings.get('probe_positive_segmented')\n",
    "    \n",
    "    if gallery is None or probe is None:\n",
    "        print(f\"Missing embeddings for segmented evaluation\")\n",
    "        return None\n",
    "    \n",
    "    model_results_dir = results_dir / model_name\n",
    "    model_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for agg in aggregations:\n",
    "        print(f\"\\n--- Testing aggregation: {agg.upper()} ---\")\n",
    "        segment_results = evaluate_segmented_probes(gallery, probe, thresholds, aggregation=agg)\n",
    "        all_results[agg] = segment_results\n",
    "\n",
    "        for segment_name, results_df in segment_results.items():\n",
    "            csv_path = model_results_dir / f'segmented_{segment_name}_{gallery_type}_{agg}_metrics.csv'\n",
    "            results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "            plot_path = model_results_dir / f'segmented_{segment_name}_{gallery_type}_{agg}_plot.png'\n",
    "            plot_metrics(results_df, \n",
    "                        f\"{model_name} - {segment_name} ({gallery_type}) - {agg.upper()}\", \n",
    "                        plot_path)\n",
    "            \n",
    "            # Print summary\n",
    "            best_rank1_idx = results_df['rank1_accuracy'].idxmax()\n",
    "            best_rank1 = results_df.iloc[best_rank1_idx]\n",
    "            print(f\"\\n{segment_name}:\")\n",
    "            print(f\"  Best Rank-1 ({agg}): {best_rank1['rank1_accuracy']:.4f} at threshold {best_rank1['threshold']:.2f}\")\n",
    "        \n",
    "        # Comparison across segments\n",
    "        if len(segment_results) > 1:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"SEGMENT COMPARISON ({agg.upper()})\")\n",
    "            print(f\"{'='*70}\")\n",
    "            for segment_name, results_df in segment_results.items():\n",
    "                best_rank1 = results_df['rank1_accuracy'].max()\n",
    "                best_f1 = results_df['f1_score'].max()\n",
    "                print(f\"{segment_name:20s} → Rank-1: {best_rank1:.4f} | F1: {best_f1:.4f}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_oneshot_evaluation(model_name: str, embeddings: Dict, \n",
    "                          results_dir: Path, gallery_type: str = 'base',\n",
    "                          thresholds: np.ndarray = None,\n",
    "                          aggregations: List[str] = ['max']):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.2, 0.91, 0.05)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {model_name} - One-Shot Gallery ({gallery_type.upper()})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    gallery_key = f'gallery_oneshot_{gallery_type}'\n",
    "    gallery = embeddings.get(gallery_key)\n",
    "    probe = embeddings['probe_positive_unsegmented']\n",
    "    \n",
    "    if gallery is None or probe is None:\n",
    "        print(f\"Missing embeddings for {model_name} - {gallery_type}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Gallery identities: {len(gallery)}\")\n",
    "    print(f\"Probe identities: {len(probe.get('all', probe))}\")\n",
    "    \n",
    "    model_results_dir = results_dir / model_name\n",
    "    model_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_results = {}\n",
    "\n",
    "    for agg in aggregations:\n",
    "        print(f\"\\n--- Testing aggregation: {agg.upper()} ---\")\n",
    "        results_df = evaluate_identification(gallery, probe, thresholds, aggregation=agg)\n",
    "        all_results[agg] = results_df\n",
    "\n",
    "        csv_path = model_results_dir / f'oneshot_{gallery_type}_{agg}_metrics.csv'\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "        print(f\"Results saved to: {csv_path}\")\n",
    "\n",
    "        plot_path = model_results_dir / f'oneshot_{gallery_type}_{agg}_metrics_plot.png'\n",
    "        plot_metrics(results_df, f\"{model_name} - One-Shot ({gallery_type.capitalize()}) - {agg.upper()}\", plot_path)\n",
    "\n",
    "        best_rank1_idx = results_df['rank1_accuracy'].idxmax()\n",
    "        best_rank1 = results_df.iloc[best_rank1_idx]\n",
    "        print(f\"Best Rank-1 Accuracy ({agg}): {best_rank1['rank1_accuracy']:.4f} at threshold {best_rank1['threshold']:.2f}\")\n",
    "\n",
    "    if len(aggregations) > 1:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"AGGREGATION COMPARISON - {gallery_type.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        for agg, results_df in all_results.items():\n",
    "            best_rank1 = results_df['rank1_accuracy'].max()\n",
    "            best_f1 = results_df['f1_score'].max()\n",
    "            print(f\"{agg.upper():8s} → Best Rank-1: {best_rank1:.4f} | Best F1: {best_f1:.4f}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def run_fewshot_evaluation(model_name: str, embeddings: Dict, \n",
    "                          results_dir: Path, gallery_type: str = 'base',\n",
    "                          thresholds: np.ndarray = None,\n",
    "                          aggregations: List[str] = ['max', 'mean', 'topk']):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.2, 0.91, 0.05)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {model_name} - Few-Shot Gallery ({gallery_type.upper()})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    gallery_key = f'gallery_fewshot_{gallery_type}'\n",
    "    gallery = embeddings.get(gallery_key)\n",
    "    probe = embeddings['probe_positive_unsegmented']\n",
    "    \n",
    "    if gallery is None or probe is None:\n",
    "        print(f\"Missing embeddings for {model_name} - {gallery_type}\")\n",
    "        return None\n",
    "\n",
    "    min_required = 2 if gallery_type == 'base' else 16\n",
    "    filtered_gallery = {}\n",
    "    invalid_identities = []\n",
    "    \n",
    "    for identity, data in gallery.items():\n",
    "        num_embeddings = len(data['embeddings'])\n",
    "        if num_embeddings < min_required:\n",
    "            invalid_identities.append((identity, num_embeddings))\n",
    "        else:\n",
    "            filtered_gallery[identity] = data\n",
    "    \n",
    "    if invalid_identities:\n",
    "        print(f\"\\nWARNING: Filtered out {len(invalid_identities)} identities with insufficient embeddings:\")\n",
    "        print(f\"   Required: >= {min_required} embeddings per identity\")\n",
    "        for identity, count in invalid_identities[:5]:\n",
    "            print(f\"   - {identity}: {count} embeddings\")\n",
    "        if len(invalid_identities) > 5:\n",
    "            print(f\"   ... and {len(invalid_identities) - 5} more\")\n",
    "        print(f\"\\n✓ Continuing with {len(filtered_gallery)} valid identities\")\n",
    "    \n",
    "    print(f\"Gallery identities: {len(gallery)}\")\n",
    "    print(f\"Probe identities: {len(probe.get('all', probe))}\")\n",
    "    \n",
    "    embedding_counts = [len(data['embeddings']) for data in gallery.values()]\n",
    "    print(f\"Embeddings per identity - Min: {min(embedding_counts)}, \"\n",
    "          f\"Max: {max(embedding_counts)}, Mean: {np.mean(embedding_counts):.1f}\")\n",
    "    \n",
    "    model_results_dir = results_dir / model_name\n",
    "    model_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    all_results = {}\n",
    "\n",
    "    for agg in aggregations:\n",
    "        print(f\"\\n--- Testing aggregation: {agg.upper()} ---\")\n",
    "        results_df = evaluate_identification(gallery, probe, thresholds, aggregation=agg)\n",
    "        all_results[agg] = results_df\n",
    "\n",
    "        csv_path = model_results_dir / f'fewshot_{gallery_type}_{agg}_metrics.csv'\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "        print(f\"Results saved to: {csv_path}\")\n",
    "\n",
    "        plot_path = model_results_dir / f'fewshot_{gallery_type}_{agg}_metrics_plot.png'\n",
    "        plot_metrics(results_df, f\"{model_name} - Few-Shot ({gallery_type.capitalize()}) - {agg.upper()}\", plot_path)\n",
    "\n",
    "        best_rank1_idx = results_df['rank1_accuracy'].idxmax()\n",
    "        best_rank1 = results_df.iloc[best_rank1_idx]\n",
    "        print(f\"Best Rank-1 Accuracy ({agg}): {best_rank1['rank1_accuracy']:.4f} at threshold {best_rank1['threshold']:.2f}\")\n",
    "\n",
    "    if len(aggregations) > 1:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"AGGREGATION COMPARISON - {gallery_type.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        for agg, results_df in all_results.items():\n",
    "            best_rank1 = results_df['rank1_accuracy'].max()\n",
    "            best_f1 = results_df['f1_score'].max()\n",
    "            print(f\"{agg.upper():8s} → Best Rank-1: {best_rank1:.4f} | Best F1: {best_f1:.4f}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\n{'#'*70}\")\n",
    "# print(f\"# OVERALL BEST MODEL COMPARISON\")\n",
    "# print(f\"{'#'*70}\")\n",
    "\n",
    "# all_results = {\n",
    "#     'oneshot_base': {},\n",
    "#     'oneshot_aug': {},\n",
    "#     'fewshot_base': {},\n",
    "#     'fewshot_aug': {}\n",
    "# }\n",
    "\n",
    "# for model_name in models:\n",
    "#     embeddings = load_embeddings(model_name)\n",
    "\n",
    "#     results_oneshot_base = run_oneshot_evaluation(\n",
    "#         model_name, \n",
    "#         embeddings, \n",
    "#         results_dir, \n",
    "#         gallery_type='base',\n",
    "#         aggregations=['max']\n",
    "#     )\n",
    "#     results_oneshot_aug = run_oneshot_evaluation(\n",
    "#         model_name, \n",
    "#         embeddings, \n",
    "#         results_dir, \n",
    "#         gallery_type='augmented',\n",
    "#         aggregations=['max', 'mean', 'topk']\n",
    "#     )\n",
    "#     all_results['oneshot_base'][model_name] = results_oneshot_base\n",
    "#     all_results['oneshot_aug'][model_name] = results_oneshot_aug\n",
    "\n",
    "#     if results_oneshot_base is not None and results_oneshot_aug is not None:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(f\"ONE-SHOT COMPARISON: BASE vs AUGMENTED - {model_name}\")\n",
    "#         print(f\"{'='*70}\")\n",
    "\n",
    "#         best_base = results_oneshot_base['max']['rank1_accuracy'].max()\n",
    "\n",
    "#         print(f\"Base (max):       Rank-1 = {best_base:.4f}\")\n",
    "#         print(f\"\\nAugmented:\")\n",
    "#         for agg in ['max', 'mean', 'topk']:\n",
    "#             best_aug = results_oneshot_aug[agg]['rank1_accuracy'].max()\n",
    "#             improvement = best_aug - best_base\n",
    "#             pct_improvement = (best_aug/best_base - 1)*100 if best_base > 0 else 0\n",
    "#             print(f\"  {agg:6s}:      Rank-1 = {best_aug:.4f} | \"\n",
    "#                   f\"Δ = {improvement:+.4f} ({pct_improvement:+.2f}%)\")\n",
    "\n",
    "#     results_fewshot_base = run_fewshot_evaluation(\n",
    "#         model_name, \n",
    "#         embeddings, \n",
    "#         results_dir, \n",
    "#         gallery_type='base',\n",
    "#         aggregations=['max', 'mean', 'topk']\n",
    "#     )\n",
    "#     results_fewshot_aug = run_fewshot_evaluation(\n",
    "#         model_name, \n",
    "#         embeddings, \n",
    "#         results_dir, \n",
    "#         gallery_type='augmented',\n",
    "#         aggregations=['max', 'mean', 'topk']\n",
    "#     )\n",
    "\n",
    "#     all_results['fewshot_base'][model_name] = results_fewshot_base\n",
    "#     all_results['fewshot_aug'][model_name] = results_fewshot_aug\n",
    "\n",
    "#     if results_fewshot_base is not None and results_fewshot_aug is not None:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(f\"FEW-SHOT COMPARISON: BASE vs AUGMENTED - {model_name}\")\n",
    "#         print(f\"{'='*70}\")\n",
    "\n",
    "#         for agg in ['max', 'mean', 'topk']:\n",
    "#             best_base = results_fewshot_base[agg]['rank1_accuracy'].max()\n",
    "#             best_aug = results_fewshot_aug[agg]['rank1_accuracy'].max()\n",
    "#             improvement = best_aug - best_base\n",
    "#             pct_improvement = (best_aug/best_base - 1)*100 if best_base > 0 else 0\n",
    "            \n",
    "#             print(f\"\\n{agg.upper()} Aggregation:\")\n",
    "#             print(f\"  Base:       Rank-1 = {best_base:.4f}\")\n",
    "#             print(f\"  Augmented:  Rank-1 = {best_aug:.4f}\")\n",
    "#             print(f\"  Improvement: {improvement:+.4f} ({pct_improvement:+.2f}%)\")\n",
    "\n",
    "#         print(f\"\\n{'─'*70}\")\n",
    "#         print(\"OVERALL BEST (any aggregation):\")\n",
    "#         best_base_overall = max(results_fewshot_base[agg]['rank1_accuracy'].max() \n",
    "#                                 for agg in ['max', 'mean', 'topk'])\n",
    "#         best_aug_overall = max(results_fewshot_aug[agg]['rank1_accuracy'].max() \n",
    "#                                for agg in ['max', 'mean', 'topk'])\n",
    "\n",
    "#         best_base_agg = max(['max', 'mean', 'topk'], \n",
    "#                            key=lambda a: results_fewshot_base[a]['rank1_accuracy'].max())\n",
    "#         best_aug_agg = max(['max', 'mean', 'topk'], \n",
    "#                           key=lambda a: results_fewshot_aug[a]['rank1_accuracy'].max())\n",
    "        \n",
    "#         print(f\"  Base best:       {best_base_overall:.4f} ({best_base_agg})\")\n",
    "#         print(f\"  Augmented best:  {best_aug_overall:.4f} ({best_aug_agg})\")\n",
    "#         improvement = best_aug_overall - best_base_overall\n",
    "#         pct_improvement = (best_aug_overall/best_base_overall - 1)*100 if best_base_overall > 0 else 0\n",
    "#         print(f\"  Improvement:     {improvement:+.4f} ({pct_improvement:+.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b29c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'#'*70}\")\n",
    "print(f\"# COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(f\"{'#'*70}\")\n",
    "\n",
    "all_results = {\n",
    "    # Identification results\n",
    "    'oneshot_base': {},\n",
    "    'oneshot_aug': {},\n",
    "    'fewshot_base': {},\n",
    "    'fewshot_aug': {},\n",
    "    \n",
    "    # Impostor results - separated by type\n",
    "    'impostor_oneshot_base_real': {},\n",
    "    'impostor_oneshot_base_lfw': {},\n",
    "    'impostor_oneshot_base_combined': {},\n",
    "    'impostor_oneshot_aug_real': {},\n",
    "    'impostor_oneshot_aug_lfw': {},\n",
    "    'impostor_oneshot_aug_combined': {},\n",
    "    'impostor_fewshot_base_real': {},\n",
    "    'impostor_fewshot_base_lfw': {},\n",
    "    'impostor_fewshot_base_combined': {},\n",
    "    'impostor_fewshot_aug_real': {},\n",
    "    'impostor_fewshot_aug_lfw': {},\n",
    "    'impostor_fewshot_aug_combined': {},\n",
    "    \n",
    "    # Segmented results\n",
    "    'segmented_oneshot_base': {},\n",
    "    'segmented_oneshot_aug': {},\n",
    "    'segmented_fewshot_base': {},\n",
    "    'segmented_fewshot_aug': {}\n",
    "}\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING MODEL: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    embeddings = load_embeddings(model_name)\n",
    "\n",
    "    # ========================================================================\n",
    "    # 1. IDENTIFICATION EVALUATIONS (Genuine Probes)\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"1. IDENTIFICATION EVALUATIONS\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # One-shot evaluations\n",
    "    results_oneshot_base = run_oneshot_evaluation(\n",
    "        model_name, \n",
    "        embeddings, \n",
    "        results_dir, \n",
    "        gallery_type='base',\n",
    "        aggregations=['max']\n",
    "    )\n",
    "    results_oneshot_aug = run_oneshot_evaluation(\n",
    "        model_name, \n",
    "        embeddings, \n",
    "        results_dir, \n",
    "        gallery_type='augmented',\n",
    "        aggregations=['max', 'mean', 'topk']\n",
    "    )\n",
    "    all_results['oneshot_base'][model_name] = results_oneshot_base\n",
    "    all_results['oneshot_aug'][model_name] = results_oneshot_aug\n",
    "\n",
    "    if results_oneshot_base is not None and results_oneshot_aug is not None:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ONE-SHOT IDENTIFICATION: BASE vs AUGMENTED - {model_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        best_base = results_oneshot_base['max']['rank1_accuracy'].max()\n",
    "\n",
    "        print(f\"Base (max):       Rank-1 = {best_base:.4f}\")\n",
    "        print(f\"\\nAugmented:\")\n",
    "        for agg in ['max', 'mean', 'topk']:\n",
    "            best_aug = results_oneshot_aug[agg]['rank1_accuracy'].max()\n",
    "            improvement = best_aug - best_base\n",
    "            pct_improvement = (best_aug/best_base - 1)*100 if best_base > 0 else 0\n",
    "            print(f\"  {agg:6s}:      Rank-1 = {best_aug:.4f} | \"\n",
    "                  f\"Δ = {improvement:+.4f} ({pct_improvement:+.2f}%)\")\n",
    "\n",
    "    # Few-shot evaluations\n",
    "    results_fewshot_base = run_fewshot_evaluation(\n",
    "        model_name, \n",
    "        embeddings, \n",
    "        results_dir, \n",
    "        gallery_type='base',\n",
    "        aggregations=['max', 'mean', 'topk']\n",
    "    )\n",
    "    results_fewshot_aug = run_fewshot_evaluation(\n",
    "        model_name, \n",
    "        embeddings, \n",
    "        results_dir, \n",
    "        gallery_type='augmented',\n",
    "        aggregations=['max', 'mean', 'topk']\n",
    "    )\n",
    "\n",
    "    all_results['fewshot_base'][model_name] = results_fewshot_base\n",
    "    all_results['fewshot_aug'][model_name] = results_fewshot_aug\n",
    "\n",
    "    if results_fewshot_base is not None and results_fewshot_aug is not None:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FEW-SHOT IDENTIFICATION: BASE vs AUGMENTED - {model_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        for agg in ['max', 'mean', 'topk']:\n",
    "            best_base = results_fewshot_base[agg]['rank1_accuracy'].max()\n",
    "            best_aug = results_fewshot_aug[agg]['rank1_accuracy'].max()\n",
    "            improvement = best_aug - best_base\n",
    "            pct_improvement = (best_aug/best_base - 1)*100 if best_base > 0 else 0\n",
    "            \n",
    "            print(f\"\\n{agg.upper()} Aggregation:\")\n",
    "            print(f\"  Base:       Rank-1 = {best_base:.4f}\")\n",
    "            print(f\"  Augmented:  Rank-1 = {best_aug:.4f}\")\n",
    "            print(f\"  Improvement: {improvement:+.4f} ({pct_improvement:+.2f}%)\")\n",
    "\n",
    "        print(f\"\\n{'─'*70}\")\n",
    "        print(\"OVERALL BEST (any aggregation):\")\n",
    "        best_base_overall = max(results_fewshot_base[agg]['rank1_accuracy'].max() \n",
    "                                for agg in ['max', 'mean', 'topk'])\n",
    "        best_aug_overall = max(results_fewshot_aug[agg]['rank1_accuracy'].max() \n",
    "                               for agg in ['max', 'mean', 'topk'])\n",
    "\n",
    "        best_base_agg = max(['max', 'mean', 'topk'], \n",
    "                           key=lambda a: results_fewshot_base[a]['rank1_accuracy'].max())\n",
    "        best_aug_agg = max(['max', 'mean', 'topk'], \n",
    "                          key=lambda a: results_fewshot_aug[a]['rank1_accuracy'].max())\n",
    "        \n",
    "        print(f\"  Base best:       {best_base_overall:.4f} ({best_base_agg})\")\n",
    "        print(f\"  Augmented best:  {best_aug_overall:.4f} ({best_aug_agg})\")\n",
    "        improvement = best_aug_overall - best_base_overall\n",
    "        pct_improvement = (best_aug_overall/best_base_overall - 1)*100 if best_base_overall > 0 else 0\n",
    "        print(f\"  Improvement:     {improvement:+.4f} ({pct_improvement:+.2f}%)\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 2. IMPOSTOR EVALUATIONS (SEPARATED BY TYPE)\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"2. IMPOSTOR REJECTION EVALUATIONS\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # One-shot impostor evaluations\n",
    "    for gallery_type, gallery_key in [('oneshot_base', 'base'), ('oneshot_aug', 'augmented')]:\n",
    "        agg_methods = ['max'] if gallery_key == 'base' else ['max', 'mean', 'topk']\n",
    "        \n",
    "        # Evaluate each impostor type separately\n",
    "        print(f\"\\nEvaluating {gallery_type} against impostors...\")\n",
    "        \n",
    "        # Real impostors\n",
    "        results_impostor_real = run_impostor_evaluation(\n",
    "            model_name, embeddings, results_dir,\n",
    "            gallery_type=gallery_type,\n",
    "            impostor_type='real',\n",
    "            aggregations=agg_methods\n",
    "        )\n",
    "        all_results[f'impostor_{gallery_type}_real'][model_name] = results_impostor_real\n",
    "        \n",
    "        # LFW impostors\n",
    "        results_impostor_lfw = run_impostor_evaluation(\n",
    "            model_name, embeddings, results_dir,\n",
    "            gallery_type=gallery_type,\n",
    "            impostor_type='lfw',\n",
    "            aggregations=agg_methods\n",
    "        )\n",
    "        all_results[f'impostor_{gallery_type}_lfw'][model_name] = results_impostor_lfw\n",
    "        \n",
    "        # Combined impostors\n",
    "        results_impostor_combined = run_impostor_evaluation(\n",
    "            model_name, embeddings, results_dir,\n",
    "            gallery_type=gallery_type,\n",
    "            impostor_type='combined',\n",
    "            aggregations=agg_methods\n",
    "        )\n",
    "        all_results[f'impostor_{gallery_type}_combined'][model_name] = results_impostor_combined\n",
    "    \n",
    "    # Few-shot impostor evaluations\n",
    "    for gallery_type in ['fewshot_base', 'fewshot_aug']:\n",
    "        print(f\"\\nEvaluating {gallery_type} against impostors...\")\n",
    "        \n",
    "        # Real impostors\n",
    "        results_impostor_real = run_impostor_evaluation(\n",
    "            model_name, embeddings, results_dir,\n",
    "            gallery_type=gallery_type,\n",
    "            impostor_type='real',\n",
    "            aggregations=['max', 'mean', 'topk']\n",
    "        )\n",
    "        all_results[f'impostor_{gallery_type}_real'][model_name] = results_impostor_real\n",
    "        \n",
    "        # LFW impostors\n",
    "        results_impostor_lfw = run_impostor_evaluation(\n",
    "            model_name, embeddings, results_dir,\n",
    "            gallery_type=gallery_type,\n",
    "            impostor_type='lfw',\n",
    "            aggregations=['max', 'mean', 'topk']\n",
    "        )\n",
    "        all_results[f'impostor_{gallery_type}_lfw'][model_name] = results_impostor_lfw\n",
    "        \n",
    "        # Combined impostors\n",
    "        results_impostor_combined = run_impostor_evaluation(\n",
    "            model_name, embeddings, results_dir,\n",
    "            gallery_type=gallery_type,\n",
    "            impostor_type='combined',\n",
    "            aggregations=['max', 'mean', 'topk']\n",
    "        )\n",
    "        all_results[f'impostor_{gallery_type}_combined'][model_name] = results_impostor_combined\n",
    "    \n",
    "    # Display impostor results summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"IMPOSTOR REJECTION SUMMARY - {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    impostor_summary = []\n",
    "    for gallery_config in ['oneshot_base', 'oneshot_aug', 'fewshot_base', 'fewshot_aug']:\n",
    "        for imp_type in ['real', 'lfw', 'combined']:\n",
    "            key = f'impostor_{gallery_config}_{imp_type}'\n",
    "            if all_results[key].get(model_name):\n",
    "                results = all_results[key][model_name]\n",
    "                if results:\n",
    "                    best_agg = max(results.keys(), \n",
    "                                  key=lambda a: results[a]['rejection_rate'].max())\n",
    "                    best_rejection = results[best_agg]['rejection_rate'].max()\n",
    "                    best_far = results[best_agg].loc[\n",
    "                        results[best_agg]['rejection_rate'].idxmax(), \n",
    "                        'false_acceptance_rate'\n",
    "                    ]\n",
    "                    total_impostors = results[best_agg]['total_impostors'].iloc[0]\n",
    "                    \n",
    "                    impostor_summary.append({\n",
    "                        'gallery': gallery_config,\n",
    "                        'impostor': imp_type,\n",
    "                        'agg': best_agg,\n",
    "                        'rejection': best_rejection,\n",
    "                        'far': best_far,\n",
    "                        'n_impostors': total_impostors\n",
    "                    })\n",
    "    \n",
    "    if impostor_summary:\n",
    "        print(f\"\\n{'Gallery':<20} {'Impostor':<10} {'Agg':<8} {'N':<6} {'Rejection':<12} {'FAR':<10}\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        for item in impostor_summary:\n",
    "            print(f\"{item['gallery']:<20} {item['impostor']:<10} {item['agg']:<8} \"\n",
    "                  f\"{item['n_impostors']:<6} {item['rejection']:<12.4f} {item['far']:<10.4f}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # 3. SEGMENTED PROBE EVALUATIONS\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(\"3. SEGMENTED PROBE EVALUATIONS\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # One-shot segmented evaluations\n",
    "    for gallery_type, gallery_key in [('oneshot_base', 'base'), ('oneshot_aug', 'augmented')]:\n",
    "        agg_methods = ['max'] if gallery_key == 'base' else ['max', 'mean', 'topk']\n",
    "        \n",
    "        results_key = f'segmented_{gallery_type}'\n",
    "        \n",
    "        results_segmented = run_segmented_evaluation(\n",
    "            model_name, embeddings, results_dir,\n",
    "            gallery_type=gallery_type,\n",
    "            aggregations=agg_methods\n",
    "        )\n",
    "        \n",
    "        all_results[results_key][model_name] = results_segmented\n",
    "    \n",
    "    # Few-shot segmented evaluations\n",
    "    for gallery_type in ['fewshot_base', 'fewshot_aug']:\n",
    "        results_key = f'segmented_{gallery_type}'\n",
    "        \n",
    "        results_segmented = run_segmented_evaluation(\n",
    "            model_name, embeddings, results_dir,\n",
    "            gallery_type=gallery_type,\n",
    "            aggregations=['max', 'mean', 'topk']\n",
    "        )\n",
    "        \n",
    "        all_results[results_key][model_name] = results_segmented\n",
    "    \n",
    "    # Display segmented results summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SEGMENTED PERFORMANCE SUMMARY - {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for gallery_config in ['oneshot_base', 'oneshot_aug', 'fewshot_base', 'fewshot_aug']:\n",
    "        key = f'segmented_{gallery_config}'\n",
    "        if all_results[key].get(model_name):\n",
    "            results = all_results[key][model_name]\n",
    "            if results:\n",
    "                print(f\"\\n{gallery_config.upper()}:\")\n",
    "                \n",
    "                # Collect segment performance across aggregations\n",
    "                segment_performance = {}\n",
    "                for agg, seg_dict in results.items():\n",
    "                    if isinstance(seg_dict, dict):\n",
    "                        for seg_name, seg_df in seg_dict.items():\n",
    "                            if seg_name not in segment_performance:\n",
    "                                segment_performance[seg_name] = {}\n",
    "                            best_rank1 = seg_df['rank1_accuracy'].max()\n",
    "                            segment_performance[seg_name][agg] = best_rank1\n",
    "                \n",
    "                # Display table\n",
    "                if segment_performance:\n",
    "                    # Get all aggregation methods used\n",
    "                    all_aggs = set()\n",
    "                    for seg_perf in segment_performance.values():\n",
    "                        all_aggs.update(seg_perf.keys())\n",
    "                    all_aggs = sorted(all_aggs)\n",
    "                    \n",
    "                    # Header\n",
    "                    header = f\"  {'Segment':<25}\"\n",
    "                    for agg in all_aggs:\n",
    "                        header += f\" {agg.upper():<10}\"\n",
    "                    header += \" BEST\"\n",
    "                    print(header)\n",
    "                    print(f\"  {'-'*70}\")\n",
    "                    \n",
    "                    # Rows\n",
    "                    for seg_name in sorted(segment_performance.keys()):\n",
    "                        seg_perf = segment_performance[seg_name]\n",
    "                        row = f\"  {seg_name:<25}\"\n",
    "                        \n",
    "                        values = []\n",
    "                        for agg in all_aggs:\n",
    "                            val = seg_perf.get(agg, 0)\n",
    "                            row += f\" {val:<10.4f}\"\n",
    "                            values.append(val)\n",
    "                        \n",
    "                        best_val = max(values) if values else 0\n",
    "                        row += f\" {best_val:.4f}\"\n",
    "                        print(row)\n",
    "                    \n",
    "                    # Overall statistics\n",
    "                    all_values = [v for seg_perf in segment_performance.values() \n",
    "                                 for v in seg_perf.values()]\n",
    "                    if all_values:\n",
    "                        print(f\"\\n  Statistics:\")\n",
    "                        print(f\"    Mean:   {np.mean(all_values):.4f}\")\n",
    "                        print(f\"    Std:    {np.std(all_values):.4f}\")\n",
    "                        print(f\"    Min:    {min(all_values):.4f}\")\n",
    "                        print(f\"    Max:    {max(all_values):.4f}\")\n",
    "                        print(f\"    Range:  {max(all_values) - min(all_values):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL COMPREHENSIVE COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n\\n{'#'*80}\")\n",
    "print(f\"# FINAL COMPREHENSIVE COMPARISON ACROSS ALL MODELS\")\n",
    "print(f\"{'#'*80}\\n\")\n",
    "\n",
    "# Compare identification performance\n",
    "print(f\"{'='*80}\")\n",
    "print(\"1. IDENTIFICATION PERFORMANCE RANKING\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "identification_ranking = []\n",
    "for model_name in models:\n",
    "    for scenario in ['oneshot_base', 'oneshot_aug', 'fewshot_base', 'fewshot_aug']:\n",
    "        if all_results[scenario].get(model_name):\n",
    "            results = all_results[scenario][model_name]\n",
    "            if results:\n",
    "                for agg, df in results.items():\n",
    "                    best_rank1 = df['rank1_accuracy'].max()\n",
    "                    best_f1 = df['f1_score'].max()\n",
    "                    identification_ranking.append({\n",
    "                        'model': model_name,\n",
    "                        'scenario': scenario,\n",
    "                        'aggregation': agg,\n",
    "                        'rank1': best_rank1,\n",
    "                        'f1': best_f1\n",
    "                    })\n",
    "\n",
    "identification_ranking = sorted(identification_ranking, \n",
    "                               key=lambda x: x['rank1'], \n",
    "                               reverse=True)\n",
    "\n",
    "print(f\"{'Rank':<6} {'Model':<20} {'Scenario':<18} {'Agg':<8} {'Rank-1':<10} {'F1':<10}\")\n",
    "print(f\"{'-'*80}\")\n",
    "for i, item in enumerate(identification_ranking[:20], 1):\n",
    "    print(f\"{i:<6} {item['model']:<20} {item['scenario']:<18} \"\n",
    "          f\"{item['aggregation']:<8} {item['rank1']:<10.4f} {item['f1']:<10.4f}\")\n",
    "\n",
    "# Compare impostor rejection - separated by type\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"2. IMPOSTOR REJECTION PERFORMANCE RANKING\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for impostor_type in ['real', 'lfw', 'combined']:\n",
    "    print(f\"\\n{impostor_type.upper()} IMPOSTORS:\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    impostor_ranking = []\n",
    "    for model_name in models:\n",
    "        for gallery in ['oneshot_base', 'oneshot_aug', 'fewshot_base', 'fewshot_aug']:\n",
    "            key = f'impostor_{gallery}_{impostor_type}'\n",
    "            if all_results[key].get(model_name):\n",
    "                results = all_results[key][model_name]\n",
    "                if results:\n",
    "                    for agg, df in results.items():\n",
    "                        best_rejection = df['rejection_rate'].max()\n",
    "                        best_far = df.loc[df['rejection_rate'].idxmax(), \n",
    "                                         'false_acceptance_rate']\n",
    "                        total_impostors = df['total_impostors'].iloc[0]\n",
    "                        impostor_ranking.append({\n",
    "                            'model': model_name,\n",
    "                            'gallery': gallery,\n",
    "                            'agg': agg,\n",
    "                            'rejection': best_rejection,\n",
    "                            'far': best_far,\n",
    "                            'n': total_impostors\n",
    "                        })\n",
    "\n",
    "    impostor_ranking = sorted(impostor_ranking, \n",
    "                             key=lambda x: x['rejection'], \n",
    "                             reverse=True)\n",
    "\n",
    "    print(f\"{'Rank':<6} {'Model':<15} {'Gallery':<15} {'Agg':<8} \"\n",
    "          f\"{'N':<6} {'Rejection':<12} {'FAR':<10}\")\n",
    "    print(f\"{'-'*90}\")\n",
    "    for i, item in enumerate(impostor_ranking[:15], 1):\n",
    "        print(f\"{i:<6} {item['model']:<15} {item['gallery']:<15} \"\n",
    "              f\"{item['agg']:<8} {item['n']:<6} {item['rejection']:<12.4f} {item['far']:<10.4f}\")\n",
    "\n",
    "# Segmented performance analysis\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"3. SEGMENTED PERFORMANCE ANALYSIS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Aggregate all segment performances\n",
    "all_segment_scores = {}\n",
    "for model_name in models:\n",
    "    for gallery_config in ['oneshot_base', 'oneshot_aug', 'fewshot_base', 'fewshot_aug']:\n",
    "        key = f'segmented_{gallery_config}'\n",
    "        if all_results[key].get(model_name):\n",
    "            results = all_results[key][model_name]\n",
    "            if results:\n",
    "                for agg, seg_dict in results.items():\n",
    "                    if isinstance(seg_dict, dict):\n",
    "                        for seg_name, seg_df in seg_dict.items():\n",
    "                            if seg_name not in all_segment_scores:\n",
    "                                all_segment_scores[seg_name] = []\n",
    "                            best_rank1 = seg_df['rank1_accuracy'].max()\n",
    "                            all_segment_scores[seg_name].append(best_rank1)\n",
    "\n",
    "# Calculate statistics per segment\n",
    "segment_stats = []\n",
    "for seg_name, scores in all_segment_scores.items():\n",
    "    segment_stats.append({\n",
    "        'segment': seg_name,\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores),\n",
    "        'min': min(scores),\n",
    "        'max': max(scores),\n",
    "        'count': len(scores)\n",
    "    })\n",
    "\n",
    "segment_stats = sorted(segment_stats, key=lambda x: x['mean'], reverse=True)\n",
    "\n",
    "print(f\"{'Segment':<25} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10} {'N':<6}\")\n",
    "print(f\"{'-'*80}\")\n",
    "for item in segment_stats:\n",
    "    print(f\"{item['segment']:<25} {item['mean']:<10.4f} {item['std']:<10.4f} \"\n",
    "          f\"{item['min']:<10.4f} {item['max']:<10.4f} {item['count']:<6}\")\n",
    "\n",
    "# Overall summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"4. OVERALL SUMMARY\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(\"Best Identification Performance:\")\n",
    "best_id = identification_ranking[0]\n",
    "print(f\"  Model: {best_id['model']}\")\n",
    "print(f\"  Config: {best_id['scenario']} ({best_id['aggregation']})\")\n",
    "print(f\"  Rank-1 Accuracy: {best_id['rank1']:.4f}\")\n",
    "\n",
    "print(\"\\nBest Impostor Rejection (by type):\")\n",
    "for impostor_type in ['real', 'lfw', 'combined']:\n",
    "    impostor_ranking = []\n",
    "    for model_name in models:\n",
    "        for gallery in ['oneshot_base', 'oneshot_aug', 'fewshot_base', 'fewshot_aug']:\n",
    "            key = f'impostor_{gallery}_{impostor_type}'\n",
    "            if all_results[key].get(model_name):\n",
    "                results = all_results[key][model_name]\n",
    "                if results:\n",
    "                    for agg, df in results.items():\n",
    "                        best_rejection = df['rejection_rate'].max()\n",
    "                        impostor_ranking.append({\n",
    "                            'model': model_name,\n",
    "                            'gallery': gallery,\n",
    "                            'agg': agg,\n",
    "                            'rejection': best_rejection,\n",
    "                            'type': impostor_type\n",
    "                        })\n",
    "    \n",
    "    if impostor_ranking:\n",
    "        best_imp = max(impostor_ranking, key=lambda x: x['rejection'])\n",
    "        print(f\"  {impostor_type.upper():10s}: {best_imp['model']:<15s} \"\n",
    "              f\"{best_imp['gallery']:<15s} ({best_imp['agg']:<5s}) - {best_imp['rejection']:.4f}\")\n",
    "\n",
    "print(\"\\nBest/Worst Segments:\")\n",
    "print(f\"  Best: {segment_stats[0]['segment']} (mean: {segment_stats[0]['mean']:.4f})\")\n",
    "print(f\"  Worst: {segment_stats[-1]['segment']} (mean: {segment_stats[-1]['mean']:.4f})\")\n",
    "print(f\"  Performance Gap: {segment_stats[0]['mean'] - segment_stats[-1]['mean']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(\"# EVALUATION COMPLETE\")\n",
    "print(f\"{'#'*80}\\n\")\n",
    "\n",
    "# Save all results\n",
    "results_pickle_path = results_dir / 'all_results_comprehensive.pkl'\n",
    "with open(results_pickle_path, 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n",
    "print(f\"All results saved to: {results_pickle_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61374fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_plot(comparison_df, results_dir):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    top_10 = comparison_df.head(10)\n",
    "    ax = axes[0, 0]\n",
    "    bars = ax.barh(range(len(top_10)), top_10['rank1_accuracy'])\n",
    "    ax.set_yticks(range(len(top_10)))\n",
    "    ax.set_yticklabels([f\"{row['model']}\\n{row['scenario']}\\n({row['aggregation']})\" \n",
    "                        for _, row in top_10.iterrows()], fontsize=8)\n",
    "    ax.set_xlabel('Rank-1 Accuracy')\n",
    "    ax.set_title('Top 10 Models by Rank-1 Accuracy')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    for i, (_, row) in enumerate(top_10.iterrows()):\n",
    "        if 'adaface' in row['model']:\n",
    "            bars[i].set_color('skyblue')\n",
    "        else:\n",
    "            bars[i].set_color('lightcoral')\n",
    "\n",
    "    top_10_f1 = comparison_df.sort_values('f1_score', ascending=False).head(10)\n",
    "    ax = axes[0, 1]\n",
    "    bars = ax.barh(range(len(top_10_f1)), top_10_f1['f1_score'])\n",
    "    ax.set_yticks(range(len(top_10_f1)))\n",
    "    ax.set_yticklabels([f\"{row['model']}\\n{row['scenario']}\\n({row['aggregation']})\" \n",
    "                        for _, row in top_10_f1.iterrows()], fontsize=8)\n",
    "    ax.set_xlabel('F1-Score')\n",
    "    ax.set_title('Top 10 Models by F1-Score')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, (_, row) in enumerate(top_10_f1.iterrows()):\n",
    "        if 'adaface' in row['model']:\n",
    "            bars[i].set_color('skyblue')\n",
    "        else:\n",
    "            bars[i].set_color('lightcoral')\n",
    "\n",
    "    top_10_eer = comparison_df.sort_values('eer', ascending=True).head(10)\n",
    "    ax = axes[1, 0]\n",
    "    bars = ax.barh(range(len(top_10_eer)), top_10_eer['eer'])\n",
    "    ax.set_yticks(range(len(top_10_eer)))\n",
    "    ax.set_yticklabels([f\"{row['model']}\\n{row['scenario']}\\n({row['aggregation']})\" \n",
    "                        for _, row in top_10_eer.iterrows()], fontsize=8)\n",
    "    ax.set_xlabel('Equal Error Rate (EER)')\n",
    "    ax.set_title('Top 10 Models by EER (Lower is Better)')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, (_, row) in enumerate(top_10_eer.iterrows()):\n",
    "        if 'adaface' in row['model']:\n",
    "            bars[i].set_color('skyblue')\n",
    "        else:\n",
    "            bars[i].set_color('lightcoral')\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    scenario_stats = comparison_df.groupby('scenario')['rank1_accuracy'].agg(['mean', 'max', 'std'])\n",
    "    x = range(len(scenario_stats))\n",
    "    ax.bar([i-0.2 for i in x], scenario_stats['mean'], width=0.4, label='Mean', alpha=0.7)\n",
    "    ax.bar([i+0.2 for i in x], scenario_stats['max'], width=0.4, label='Max', alpha=0.7)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(scenario_stats.index, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Rank-1 Accuracy')\n",
    "    ax.set_title('Performance by Scenario')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = results_dir / 'model_comparison_plot.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Comparison plot saved to: {plot_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd1d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(all_results):\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# FINAL MODEL RANKING\")\n",
    "    print(f\"{'#'*70}\")\n",
    "\n",
    "    configs = []\n",
    "    \n",
    "    for scenario in ['oneshot_base', 'oneshot_aug', 'fewshot_base', 'fewshot_aug']:\n",
    "        for model_name, results in all_results[scenario].items():\n",
    "            if results is None:\n",
    "                continue\n",
    "            \n",
    "            if scenario == 'oneshot_base':\n",
    "                aggregations = ['max']\n",
    "            else:\n",
    "                aggregations = ['max', 'mean', 'topk'] if isinstance(results, dict) else ['max']\n",
    "            \n",
    "            for agg in aggregations:\n",
    "                if isinstance(results, dict) and agg in results:\n",
    "                    df = results[agg]\n",
    "                elif not isinstance(results, dict):\n",
    "                    df = results\n",
    "                    agg = 'max'\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                best_idx = df['rank1_accuracy'].idxmax()\n",
    "                best_row = df.iloc[best_idx]\n",
    "\n",
    "                df_temp = df.copy()\n",
    "                df_temp['err_diff'] = abs(df_temp['false_acceptance_rate'] - \n",
    "                                         df_temp['false_rejection_rate'])\n",
    "                eer_idx = df_temp['err_diff'].idxmin()\n",
    "                eer_row = df.iloc[eer_idx]\n",
    "                \n",
    "                configs.append({\n",
    "                    'model': model_name,\n",
    "                    'scenario': scenario,\n",
    "                    'aggregation': agg,\n",
    "                    'best_threshold': best_row['threshold'],\n",
    "                    'rank1_accuracy': best_row['rank1_accuracy'],\n",
    "                    'precision': best_row['precision'],\n",
    "                    'recall': best_row['recall'],\n",
    "                    'f1_score': best_row['f1_score'],\n",
    "                    'far': best_row['false_acceptance_rate'],\n",
    "                    'frr': best_row['false_rejection_rate'],\n",
    "                    'eer_threshold': eer_row['threshold'],\n",
    "                    'eer': (eer_row['false_acceptance_rate'] + \n",
    "                           eer_row['false_rejection_rate']) / 2,\n",
    "                    'eer_accuracy': eer_row['rank1_accuracy']\n",
    "                })\n",
    "\n",
    "    comparison_df = pd.DataFrame(configs)\n",
    "\n",
    "    comparison_df = comparison_df.sort_values('rank1_accuracy', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RANKING BY RANK-1 ACCURACY\")\n",
    "    print(\"=\"*70)\n",
    "    print(comparison_df[['model', 'scenario', 'aggregation', 'rank1_accuracy', \n",
    "                         'best_threshold', 'f1_score']].head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RANKING BY F1-SCORE\")\n",
    "    print(\"=\"*70)\n",
    "    comparison_df_f1 = comparison_df.sort_values('f1_score', ascending=False)\n",
    "    print(comparison_df_f1[['model', 'scenario', 'aggregation', 'f1_score', \n",
    "                            'best_threshold', 'rank1_accuracy']].head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RANKING BY EER (Lower is better)\")\n",
    "    print(\"=\"*70)\n",
    "    comparison_df_eer = comparison_df.sort_values('eer', ascending=True)\n",
    "    print(comparison_df_eer[['model', 'scenario', 'aggregation', 'eer', \n",
    "                             'eer_threshold', 'eer_accuracy']].head(10).to_string(index=False))\n",
    "\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(\"# OVERALL BEST MODEL\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    best_config = comparison_df.iloc[0]\n",
    "    print(f\"\\nModel: {best_config['model']}\")\n",
    "    print(f\"Scenario: {best_config['scenario']}\")\n",
    "    print(f\"Aggregation: {best_config['aggregation']}\")\n",
    "    print(f\"\\nBest Performance (Threshold = {best_config['best_threshold']:.2f}):\")\n",
    "    print(f\"  Rank-1 Accuracy: {best_config['rank1_accuracy']:.4f}\")\n",
    "    print(f\"  Precision:       {best_config['precision']:.4f}\")\n",
    "    print(f\"  Recall:          {best_config['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:        {best_config['f1_score']:.4f}\")\n",
    "    print(f\"  FAR:             {best_config['far']:.4f}\")\n",
    "    print(f\"  FRR:             {best_config['frr']:.4f}\")\n",
    "    print(f\"\\nAt EER (Threshold = {best_config['eer_threshold']:.2f}):\")\n",
    "    print(f\"  EER:             {best_config['eer']:.4f}\")\n",
    "    print(f\"  Rank-1 Accuracy: {best_config['eer_accuracy']:.4f}\")\n",
    "\n",
    "    comparison_path = results_dir / 'model_comparison.csv'\n",
    "    comparison_df.to_csv(comparison_path, index=False)\n",
    "    print(f\"\\nFull comparison saved to: {comparison_path}\")\n",
    "\n",
    "    create_comparison_plot(comparison_df, results_dir)\n",
    "    \n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_comparison = find_best_model(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba13c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
